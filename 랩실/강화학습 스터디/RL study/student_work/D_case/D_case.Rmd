---
title: "D_case"
author: "Lee Sung Ho"
date: '2021 2 5 '
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(background = '718CBA')
library(reticulate)
py_install("pandas")
```

1. Object

This problem was created by applying the Stanford Mars problem. Existing problems were simple models, making it easy to figure out the optimal policy. Thus, we adjusted the reward and probability to change the problem so that the simulation can tell.

\vspace{10pt}


2. Problem

-NASA collaborated with Stanford to send a Mars rover. If the rover goes west of the landing site, the path is safe, but there is not much to investigate. It is worth twice as much research as the west to the east. However, on the way through the crater terrain, the machine is broken and, if severe, it will stop working with a 1% chance. In this case, how can they get the biggest benefit?



\newpage
## Environment
```{python, echo=TRUE}
import pandas as pd
import numpy as np
action = ['move_left','move_not', 'move_right']
state = [1,2,3,4,5,6,7] # s1 ~ s7
P_ML = pd.DataFrame(np.matrix([[0,0,0,0,0,0,0],
                             [1,0,0,0,0,0,0],
                             [0,1,0,0,0,0,0],
                             [0,0,1,0,0,0,0],
                             [0,0,0,1,0,0,0],
                             [0,0,0,0,1,0,0],
                              [0,0,0,0,0,1,0]
                              ]),index = state , columns = state)
                              
P_MR = pd.DataFrame(np.matrix([[0,1,0,0,0,0,0],
                             [0,0,1,0,0,0,0],
                             [0,0,0,1,0,0,0],
                             [0,0,0,0,1,0,0],
                            [0,0,0,0,0,1,0],
                              [0,0,0,0,0,0,1],
                             [0,0,0,0,0,0,0]
                              ]),index = state , columns = state)    
                              
P_MN = pd.DataFrame(np.matrix([[1,0,0,0,0,0,0],
                             [0,1,0,0,0,0,0],
                             [0,0,1,0,0,0,0],
                             [0,0,0,1,0,0,0],
                            [0,0,0,0,1,0,0],
                              [0,0,0,0,0,1,0],
                             [0,0,0,0,0,0,1]
                              ]),index = state , columns = state)                                
                            
pi_mars =pd.DataFrame(np.matrix([np.repeat(0.4,len(state)),np.repeat(0.2,len(state)),np.repeat(0.4,len(state))]),index = action,columns = state).T
 
pi_mars['move_left'][1] = 0
pi_mars['move_not'][1] = 0.6
pi_mars['move_right'][7] = 0
pi_mars['move_not'][7] = 0.6
print(pi_mars.T)

reward = np.array([5,0,0,0,0,-4,10])
print(reward)



```
\newpage
## Simulator 

```{python, echo=TRUE}
pi = pi_mars
np.random.seed(1234)
history = []
MC_N = 10000
for MC_i in range(MC_N):
  s_now = 4 # Start s4
  history_i = [4]
  count = 0
  
  while count < 10 :
    
    probability = np.random.uniform(0,1)
    
    if probability < pi.loc[s_now]['move_left']:
      a_now = 'move_left'
      P = P_ML
      s_next = s_now - 1 
  
    elif probability >= pi.loc[s_now]['move_left'] and probability < (pi.loc[s_now]['move_left'] + pi.loc[s_now]['move_not']):
    
      a_now = 'move_not'
      P = P_MN
      s_next = s_now
      
    else:
      a_now = 'move_right'
      P = P_MR
      s_next = s_now + 1 
    
    if s_now == 6 and probability <= 0.01:
      r_now = -20
      history_i.extend([a_now,r_now,s_next])      
      break
    
    r_now = reward[s_now-1]
    history_i.extend([a_now,r_now,s_next])
    s_now = s_next
    

      
    count+=1
    
  history.append(history_i)  
history[-5:]
```
\newpage
## Implementation 1 (vectorized)

```{python, echo=TRUE}
pol_eval=pd.DataFrame(np.matrix(np.zeros((len(state)*2))).reshape(len(state),2), index=state, columns=['count','sum'])
print(pol_eval.T)
for MC_i in range(MC_N):
  history_i = history[MC_i]
  
  for j in range(0,len(history_i),3):
    pol_eval.loc[history_i[j]]['count']+=1
    
    if j < len(history_i) :
      pol_eval.loc[history_i[j]]['sum']+=pd.Series(history_i)[range(j+2,len(history_i)-1,3)].sum()
            
    else:
      pol_eval.loc[history_i[j]]['sum']+=0
            
print(pol_eval.T)
pol_cal=pd.DataFrame(pol_eval['sum']/pol_eval['count'])
print(pol_cal.T)
  
```

\newpage
## Implementation 2 (vectorized)

```{python, echo=TRUE}
pol_eval=pd.DataFrame(np.matrix(np.zeros((len(state)*2))).reshape(len(state),2), index=state, columns=['count','est'])
print(pol_eval.T)
for MC_i in range(MC_N):
    history_i=history[MC_i]
    
    for j in range(0,len(history_i),3):
        # update count
        pol_eval.loc[history_i[j]]['count']+=1
        current_cnt=pol_eval.loc[history_i[j]]['count']
        
        # return is the new info
        if j < len(history_i):
            new_info=pd.Series(history_i)[range(j+2,len(history_i)-1,3)].sum()
            
        else:
            new_info = 0
        
        # update the last estimate with new info    
        alpha=1/current_cnt
        pol_eval.loc[history_i[j]]['est']+=alpha*(new_info-pol_eval.loc[history_i[j]]['est'])
        
        
print(pol_eval)
  
```


\newpage
## Implementation 3 
```{python,echo=T}
pol_eval=pd.DataFrame(np.matrix(np.zeros((len(state)*2))).reshape(len(state),2), index=state, columns=['count','est'])
print(pol_eval.T)
for episode_i in range(len(history)):
  history_i = history[episode_i]
  
  # update count
  for j in range(0,len(history_i),3):
    pol_eval.loc[history_i[j]]['count'] +=1
    current_cnt =pol_eval.loc[history_i[j]]['count']
    
    #build TD target
    if(j < len(history_i)-3):
      TD_tgt = float(history_i[j+2])+pol_eval.loc[history_i[j+3]]['est']
      
    else:
      TD_tgt = 0
      
    # TD-updating  
    alpha = 1/current_cnt
    
    pol_eval.loc[history_i[j]]['est'] += alpha*(TD_tgt - pol_eval.loc[history_i[j]]['est'])
  
pol_eval
```

\newpage

```{python, echo=TRUE}
q_s_a=pd.DataFrame(np.c_[np.repeat(0.0,len(state)), np.repeat(0.0,len(state)), np.repeat(0.0,len(state)) ], index=state, columns=['move_left','move_not', 'move_right'])

def pol_eval_MC(sample_path, q_s_a, alpha):
  
  for j in range(0,len(sample_path)-1,3):
      s = sample_path[j]
      a = sample_path[j+1]
      G = sum([sample_path[g] for g in range(j+2, len(sample_path)-1 , 3)])
      q_s_a.loc[s][a] = q_s_a.loc[s][a] + alpha*(G - q_s_a.loc[s][a])

  return q_s_a    
  
q_s_a = pol_eval_MC(sample_path = history[0] , q_s_a = q_s_a, alpha = 0.1)
print(q_s_a)

q_s_a = pol_eval_MC(sample_path = history[1] , q_s_a = q_s_a, alpha = 0.1)
print(q_s_a)

# for i in history:
#   q_s_a = pol_eval_MC(sample_path = i , q_s_a = q_s_a, alpha = 0.1)



```
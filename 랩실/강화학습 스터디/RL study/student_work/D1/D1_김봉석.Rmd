---
title: "Lecture D1. Markov Reword Process1"  
author: "Reinforcement Learning Study"  
date: "`r Sys.Date()`"  
output:   
  pdf_document:  
    latex_engine: xelatex
    highlight: haddock  
    keep_tex: true  
    includes:
      in_header: rmd-pdf-support/latex-topmatter.tex
    # pandoc_args: [
    #  "-V", "classoption=twocolumn"
    # ]
    toc: true   
    toc_depth: 2  
    # number_sections: true  
monofont: Consolas
smaller: yes
classoption: a4paper
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(background = '718CBA')  
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(reticulate)
#matplotlib <- import("matplotlib")
#matplotlib$use("Agg", force = TRUE)
```

## Recap

```{python, echo=TRUE}
import numpy as np

def soda_simul(this_state):
    u= np.random.uniform()
    
    if(this_state=="c"):
        if(u<=0.7):
            next_state="c"
        else:
            next_state="p"
            
    else:
        if(u<=0.5):
            next_state="c"
        else:
            next_state="p"
        
    return(next_state)


def cost_eval(path):
    cost_one_path=path.count('c')*1.5+path.count('p')*1
    return cost_one_path


```


## MC simulation for estimating state-value function

```{python, echo=TRUE}
# MC evaluation for state-value function

#with state s, time 0, reward r, time-horizon H


def MC_V_t(initial_state, num_episode, time_horizon):
    
    episode_i = 0

    cum_sum_G_i = 0

    while(episode_i<num_episode) :
        path=initial_state
        for n in range(time_horizon-1):
            this_state=path[-1]
            next_state=soda_simul(this_state)
            path+=next_state
            
        G_i=cost_eval(path) 
        cum_sum_G_i+=G_i
        episode_i+=1
    V_t=cum_sum_G_i/num_episode
    return V_t
    

print(MC_V_t('c',100000,10))
print(MC_V_t('s',100000,10))
```




\newpage

## iterative Solution Excercise

For general t,

\begin{eqnarray*}
V_t(s) &=& \mathbb E[G_t|S_t=t]  \\
  \\
&=& \mathbb E[r_t+r_{t+1}+r_{t+2} \dots+r_k |S_t=s] \\
  \\
&=& \mathbb E[r_t |S_t]+\mathbb E[r_{t+1}+r_{t+2} \dots+r_k |S_t=s] \\
  \\
&=&  R(s)+\mathbb E[r_{t+1}+r_{t+2} \dots+r_k |S_t=s] \\
  \\
&=&  R(s)+\mathbb E[G_{t+1} |S_t=s,S_{t+1}=s^\prime] \\
  \\
&=&  R(s)+\mathbb E[G_{t+1} |S_{t+1}=s^\prime]  (\because Markov property)\\
  \\
&=&  R(s)+\sum_{s\in s^\prime}^{} P_{ss^\prime}V_{t+1}(s^\prime) \\
\end{eqnarray*}


\newpage

## Backward induction for estimating state-value function

```{python, echo=TRUE}
import numpy as np

P=np.array([[0.7,0.3],[0.5,0.5]])

R=np.array([1.5,1])[:,None] # [:,None] retrun Column vector

H=10

v_t1=np.array([0,0])[:,None]

```


```{python, echo=TRUE}
print('P :\n',P)

print('R :\n',R)

print('v_t1 :\n',v_t1)
```


```{python, echo=TRUE}
t=H-1

while(t>=0):
    v_t = R+np.dot(P,v_t1)
    t = t-1
    v_t1 = v_t
v_t

```

```{r message=FALSE, warning=FALSE, paged.print=TRUE}
"Done, Lecture D1. Markov Reword Process1 "
```
---
title: "D1_DTMC Python"
author: "Jaemin Park"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex
    highlight: haddock
    keep_tex: yes
    includes:
      in_header: rmd-pdf-support/latex-topmatter.tex
    toc: yes
    toc_depth: 2
  word_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: '2'
    df_print: paged
smaller: yes
classoption: a4paper
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(background = '718CBA')  
```

\newpage

\newpage
```{python, echo=FALSE}
import numpy as np


def soda_simul(this_state):
    u = np.random.rand()
    if(this_state == "c"):
        if(u<=0.7):
            next_state = "c"
        else:
            next_state = "p"
    else:
        if(u<=0.5):
            next_state = "c"
        else:
            next_state = "p"
    return next_state

for i in range(5):
    this_stage = "c"
    result = []
    for j in range(9):
        result.append(this_stage)
        next_state = soda_simul(this_stage)
        this_stage = next_state
```
# Exercises
## p.10 MC Simulator
R code
```{r, eval = FALSE}
MC_N <‐ 10000
spending_records <‐ rep(0, MC_N)
for (i in 1:MC_N) {
  path <‐ "c" # coke today (day‐0)
  for (t in 1:9) {
    this_state <‐ str_sub(path, ‐1, ‐1)
    next_state <‐ soda_simul(this_state)
    path <‐ paste0(path, next_state)
  }
  spending_records[i] <‐ cost_eval(path)
}
cost_eval <‐ function(path) {
  cost_one_path <‐
    str_count(path, pattern = "c")*1.5 +
    str_count(path, pattern = "p")*1
  return(cost_one_path)
}
```
Python code
```{python}
def cost_eval(path):
    cost_one_path = path.count("c")*1.5 + path.count("p")*1
    return cost_one_path
MC_N = 100
record = np.array([])
for i in range(MC_N):
    this_stage = "c"
    result = []
    for j in range(9):
        result.append(this_stage)
        next_state = soda_simul(this_stage)
        this_stage = next_state
    record = np.append(record, np.array([cost_eval(''.join(result))]))
```

\newpage


## p.11 MC Simulator
R code
```{R, eval = FALSE}
episode_i <- 0
cum_sum_G_i <- 0
while episode_i < num_episode
  #Generate an stochastic path starting from state s and time 0.
  #Calculate return G_i <‐ sum of rewards from time 0 to time H‐1.
  cum_sum_G_i <- cum_sum_G_i + G_i
  episode_i <- episode_i + 1
State‐value‐fn V_t(s) -‐ cum_sum_G_i/num_episode
return V_t(s)
```

Python code
```{python C1_2}
#MC evalutaion for state-value function
#with state s, time 0, reward r, time horizon H
episode = 100
episode_i = 0
cum_sum_G_i = 0
while episode_i < episode:
  this_stage = "c"
  result = []
  for j in range(9):
      result.append(this_stage)
      next_state = soda_simul(this_stage)
      this_stage = next_state
  
  G_i = cost_eval(result)
  cum_sum_G_i = cum_sum_G_i + G_i
  episode_i +=1
V_t = cum_sum_G_i / episode
V_t
```
\newpage

## p.18 Exercise

For general t,(exercise)
$$
\begin{aligned}
  V_t(S) &= \mathbb{E}[G_t\ |\ S_t = s]\\
  &=\mathbb{E}[\sum_{i=t}^{9}r_t \ |\ S_t=s]\\
  &=\mathbb{E}[r_t+r_{t+1}+..+r_9\ |\ S_t=s]\\
  &=\mathbb{E}[r_t\ |\ S_t=s]+\mathbb{E}[r_{t+1}+..+r_9\ |\ S_t]\\
  &=R(s)+ \sum_{{s'}\in S} \ P_{ss'}\ \mathbb{E}[G_{t+1}\ |\ S_t=s, S_{t+1}=s']\\
  &=R(s)+ \sum_{{s'}\in S} \ P_{ss'}\ \mathbb{E}[G_{t+1}\ |\ S_{t+1}=s'](\because Markov\ property)\\
  &=R(s)+ \sum_{s' \in S} \ P_{ss'}V_{t+1}(s')
\end{aligned}
$$


\newpage
## p.20 Iterative solution
```{python}
import numpy as np

P = np.matrix([[0.7,0.3],[0.5,0.5]])
R = np.matrix([[1.5],[1]])
H=10
v_t1 = np.matrix([[0],[0]])

t=H-1
while(t>=0):
    v_t = R + P*v_t1
    t-=1
    v_t1 = v_t

print(v_t)
```


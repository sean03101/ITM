{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "elec  = pd.read_csv(\"US_County_Level_Presidential_Results_12-16.csv\")\n",
    "county  = pd.read_csv(\"county_facts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=elec[elec['county_name']!='Alaska'].merge(county,left_on='FIPS',right_on='fips',how='left')\n",
    "data_ak=elec[elec['county_name']=='Alaska'].drop_duplicates(['votes_dem_2016','votes_gop_2016'])\n",
    "data_ak['FIPS']=2000\n",
    "data_ak=data_ak.merge(county,left_on='FIPS',right_on='fips',how='left')\n",
    "data=pd.concat((data,data_ak),axis=0).sort_values('fips')\n",
    "data['target_2016']=(data['votes_dem_2016']>data['votes_gop_2016'])*1\n",
    "data['target_2012']=(data['votes_dem_2012']>data['votes_gop_2012'])*1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3113\n",
      "Index(['Unnamed: 0', 'combined_fips', 'votes_dem_2016', 'votes_gop_2016',\n",
      "       'total_votes_2016', 'per_dem_2016', 'per_gop_2016', 'diff_2016',\n",
      "       'per_point_diff_2016', 'state_abbr', 'county_name', 'FIPS',\n",
      "       'total_votes_2012', 'votes_dem_2012', 'votes_gop_2012', 'county_fips',\n",
      "       'state_fips', 'per_dem_2012', 'per_gop_2012', 'diff_2012',\n",
      "       'per_point_diff_2012', 'fips', 'area_name', 'state_abbreviation',\n",
      "       'PST045214', 'PST040210', 'PST120214', 'POP010210', 'AGE135214',\n",
      "       'AGE295214', 'AGE775214', 'SEX255214', 'RHI125214', 'RHI225214',\n",
      "       'RHI325214', 'RHI425214', 'RHI525214', 'RHI625214', 'RHI725214',\n",
      "       'RHI825214', 'POP715213', 'POP645213', 'POP815213', 'EDU635213',\n",
      "       'EDU685213', 'VET605213', 'LFE305213', 'HSG010214', 'HSG445213',\n",
      "       'HSG096213', 'HSG495213', 'HSD410213', 'HSD310213', 'INC910213',\n",
      "       'INC110213', 'PVY020213', 'BZA010213', 'BZA110213', 'BZA115213',\n",
      "       'NES010213', 'SBO001207', 'SBO315207', 'SBO115207', 'SBO215207',\n",
      "       'SBO515207', 'SBO415207', 'SBO015207', 'MAN450207', 'WTN220207',\n",
      "       'RTN130207', 'RTN131207', 'AFN120207', 'BPS030214', 'LND110210',\n",
      "       'POP060210', 'target_2016', 'target_2012'],\n",
      "      dtype='object')\n",
      "Unnamed: 0          0\n",
      "combined_fips       0\n",
      "votes_dem_2016      0\n",
      "votes_gop_2016      0\n",
      "total_votes_2016    0\n",
      "                   ..\n",
      "BPS030214           0\n",
      "LND110210           0\n",
      "POP060210           0\n",
      "target_2016         0\n",
      "target_2012         0\n",
      "Length: 77, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(data.columns)\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['AGE295214', 'AGE775214', 'SEX255214', 'RHI125214', 'RHI225214',\n",
    "       'RHI325214', 'RHI425214', 'RHI525214', 'RHI625214', 'RHI725214',\n",
    "       'RHI825214', 'POP715213', 'POP645213', 'POP815213', 'EDU635213',\n",
    "       'EDU685213', 'VET605213', 'LFE305213', 'HSG010214', 'HSG445213',\n",
    "       'HSG096213', 'HSG495213', 'HSD410213', 'HSD310213', 'INC910213',\n",
    "       'INC110213', 'PVY020213', 'BZA010213', 'BZA110213', 'BZA115213',\n",
    "       'NES010213', 'SBO001207', 'SBO315207', 'SBO115207', 'SBO215207',\n",
    "       'SBO515207', 'SBO415207', 'SBO015207', 'MAN450207', 'WTN220207',\n",
    "       'RTN130207', 'RTN131207', 'AFN120207', 'BPS030214', 'LND110210',\n",
    "       'POP060210', 'per_gop_2016']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LND110210       0.016211\n",
       "BZA115213       0.017635\n",
       "AGE295214       0.034754\n",
       "LFE305213       0.053984\n",
       "EDU635213       0.090323\n",
       "SBO515207       0.097759\n",
       "RHI325214       0.113527\n",
       "SBO115207       0.134685\n",
       "RHI525214       0.141385\n",
       "PVY020213       0.142331\n",
       "SEX255214       0.169669\n",
       "HSD310213       0.170743\n",
       "RHI725214       0.188371\n",
       "POP715213       0.198195\n",
       "RHI625214       0.200045\n",
       "INC110213       0.208196\n",
       "RTN131207       0.234911\n",
       "INC910213       0.236907\n",
       "WTN220207       0.251530\n",
       "POP060210       0.262629\n",
       "MAN450207       0.267236\n",
       "BPS030214       0.286972\n",
       "SBO415207       0.306420\n",
       "AFN120207       0.306532\n",
       "NES010213       0.317739\n",
       "AGE775214       0.323267\n",
       "POP815213       0.326551\n",
       "SBO001207       0.329608\n",
       "BZA010213       0.349001\n",
       "RTN130207       0.349794\n",
       "BZA110213       0.351493\n",
       "SBO015207       0.357109\n",
       "HSD410213       0.363317\n",
       "HSG010214       0.365450\n",
       "VET605213       0.371663\n",
       "POP645213       0.395248\n",
       "SBO215207       0.404285\n",
       "SBO315207       0.419201\n",
       "RHI225214       0.425049\n",
       "RHI425214       0.446764\n",
       "HSG445213       0.468329\n",
       "HSG495213       0.486002\n",
       "EDU685213       0.487345\n",
       "RHI125214       0.529763\n",
       "RHI825214       0.536188\n",
       "HSG096213       0.604606\n",
       "per_gop_2016    1.000000\n",
       "Name: per_gop_2016, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df.corr(method = 'pearson')\n",
    "cor2y = np.abs(corr.loc['per_gop_2016'])\n",
    "cor22y = cor2y.sort_values()\n",
    "\n",
    "cor22y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SBO415207', 'AFN120207', 'NES010213', 'AGE775214', 'POP815213',\n",
       "       'SBO001207', 'BZA010213', 'RTN130207', 'BZA110213', 'SBO015207',\n",
       "       'HSD410213', 'HSG010214', 'VET605213', 'POP645213', 'SBO215207',\n",
       "       'SBO315207', 'RHI225214', 'RHI425214', 'HSG445213', 'HSG495213',\n",
       "       'EDU685213', 'RHI125214', 'RHI825214', 'HSG096213', 'per_gop_2016'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables = cor22y[cor22y > 0.3]\n",
    "variables = variables.index\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['SBO415207', 'AFN120207', 'NES010213', 'AGE775214', 'POP815213',\n",
    "       'SBO001207', 'BZA010213', 'RTN130207', 'BZA110213', 'SBO015207',\n",
    "       'HSD410213', 'HSG010214', 'VET605213', 'POP645213', 'SBO215207',\n",
    "       'SBO315207', 'RHI225214', 'RHI425214', 'HSG445213', 'HSG495213',\n",
    "       'EDU685213', 'RHI125214', 'RHI825214', 'HSG096213', 'target_2016']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sean\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "X = data[['SBO415207', 'AFN120207', 'NES010213', 'AGE775214', 'POP815213',\n",
    "       'SBO001207', 'BZA010213', 'RTN130207', 'BZA110213', 'SBO015207',\n",
    "       'HSD410213', 'HSG010214', 'VET605213', 'POP645213', 'SBO215207',\n",
    "       'SBO315207', 'RHI225214', 'RHI425214', 'HSG445213', 'HSG495213',\n",
    "       'EDU685213', 'RHI125214', 'RHI825214', 'HSG096213']]\n",
    "\n",
    "X['const'] = 1\n",
    "\n",
    "y = data['per_gop_2016']\n",
    "\n",
    "feature_columns = ['SBO415207', 'AFN120207', 'NES010213', 'AGE775214', 'POP815213',\n",
    "       'SBO001207', 'BZA010213', 'RTN130207', 'BZA110213', 'SBO015207',\n",
    "       'HSD410213', 'HSG010214', 'VET605213', 'POP645213', 'SBO215207',\n",
    "       'SBO315207', 'RHI225214', 'RHI425214', 'HSG445213', 'HSG495213',\n",
    "       'EDU685213', 'RHI125214', 'RHI825214', 'HSG096213']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002224F835548>, 'AIC': -5832.90848773861}\n",
      "Processed 24 models on 1 predictors in 0.03124380111694336 seconds.\n",
      "Processed 276 models on 2 predictors in 0.59688401222229 seconds.\n",
      "Processed 2024 models on 3 predictors in 5.6608052253723145 seconds.\n",
      "Processed  24 models on 1 predictors in 0.04686617851257324\n",
      "Selected predictors: ['HSG096213', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746EC9C8>\n",
      "forward\n",
      "Processed 1 models on 0 predictors in 0.0\n",
      "Selected predictors : ['const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222736A82C8>\n",
      "Processed  23 models on 2 predictors in 0.04686450958251953\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002227A7554C8>\n",
      "forward\n",
      "Processed 2 models on 1 predictors in 0.0\n",
      "Selected predictors : ['HSG096213', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222726CA2C8>\n",
      "Processed  22 models on 3 predictors in 0.16570806503295898\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746E6788>\n",
      "forward\n",
      "Processed 3 models on 2 predictors in 0.015625\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746D9D88>\n",
      "Processed  21 models on 4 predictors in 0.10933232307434082\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222726C5C08>\n",
      "forward\n",
      "Processed 4 models on 3 predictors in 0.015636682510375977\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222736A8D08>\n",
      "Processed  20 models on 5 predictors in 0.17575788497924805\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002227A755288>\n",
      "forward\n",
      "Processed 5 models on 4 predictors in 0.031242847442626953\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222726C5A48>\n",
      "Processed  19 models on 6 predictors in 0.10934925079345703\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746E6988>\n",
      "forward\n",
      "Processed 6 models on 5 predictors in 0.11171317100524902\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222736A8E88>\n",
      "Processed  18 models on 7 predictors in 0.09372091293334961\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002227A755BC8>\n",
      "forward\n",
      "Processed 7 models on 6 predictors in 0.031249046325683594\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746E6E48>\n",
      "Processed  17 models on 8 predictors in 0.07676815986633301\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222736A8E48>\n",
      "forward\n",
      "Processed 8 models on 7 predictors in 0.056061506271362305\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222726CAC88>\n",
      "Processed  16 models on 9 predictors in 0.06248760223388672\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746D9088>\n",
      "forward\n",
      "Processed 9 models on 8 predictors in 0.031240463256835938\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'AGE775214', 'POP815213', 'POP645213', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222726CA548>\n",
      "Processed  15 models on 10 predictors in 0.05330491065979004\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746D9BC8>\n",
      "forward\n",
      "Processed 10 models on 9 predictors in 0.036637306213378906\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746E6E08>\n",
      "Processed  14 models on 11 predictors in 0.04685854911804199\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222736A8448>\n",
      "forward\n",
      "Processed 11 models on 10 predictors in 0.031241178512573242\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746EC548>\n",
      "Processed  13 models on 12 predictors in 0.05758333206176758\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746D9108>\n",
      "forward\n",
      "Processed 12 models on 11 predictors in 0.0536346435546875\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002227A755C48>\n",
      "Processed  12 models on 13 predictors in 0.04686331748962402\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746E6308>\n",
      "forward\n",
      "Processed 13 models on 12 predictors in 0.046888113021850586\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746E6888>\n",
      "Processed  11 models on 14 predictors in 0.07139039039611816\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746D9548>\n",
      "forward\n",
      "Processed 14 models on 13 predictors in 0.0624847412109375\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746D9B88>\n",
      "Processed  10 models on 15 predictors in 0.04686093330383301\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'AFN120207', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002227A77A508>\n",
      "forward\n",
      "Processed 15 models on 14 predictors in 0.0624847412109375\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222726C5288>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  9 models on 16 predictors in 0.06267786026000977\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'AFN120207', 'VET605213', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746D9648>\n",
      "forward\n",
      "Processed 16 models on 15 predictors in 0.10102415084838867\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'AFN120207', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746E6148>\n",
      "Processed  8 models on 17 predictors in 0.031243085861206055\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'AFN120207', 'VET605213', 'NES010213', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222716A1848>\n",
      "forward\n",
      "Processed 17 models on 16 predictors in 0.10618066787719727\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'VET605213', 'NES010213', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002227A77AE08>\n",
      "Processed  7 models on 18 predictors in 0.031244754791259766\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'AFN120207', 'VET605213', 'NES010213', 'HSD410213', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746E6048>\n",
      "forward\n",
      "Processed 18 models on 17 predictors in 0.1044931411743164\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'AFN120207', 'NES010213', 'HSD410213', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002227169A788>\n",
      "backward\n",
      "Processed  7 models on 18 predictors in 0.04686737060546875\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'AFN120207', 'NES010213', 'HSD410213', 'HSG010214', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746D9948>\n",
      "forward\n",
      "Processed 18 models on 17 predictors in 0.09376835823059082\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'NES010213', 'HSD410213', 'HSG010214', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222726C50C8>\n",
      "backward\n",
      "Processed  7 models on 18 predictors in 0.04685163497924805\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'NES010213', 'HSD410213', 'HSG010214', 'BZA010213', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222726CA548>\n",
      "forward\n",
      "Processed 18 models on 17 predictors in 0.09540033340454102\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI225214', 'NES010213', 'HSD410213', 'HSG010214', 'BZA010213', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002227A755B08>\n",
      "Processed  6 models on 19 predictors in 0.03124833106994629\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'NES010213', 'HSD410213', 'HSG010214', 'BZA010213', 'RTN130207', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222716A1188>\n",
      "forward\n",
      "Processed 19 models on 18 predictors in 0.10688638687133789\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'NES010213', 'HSD410213', 'HSG010214', 'BZA010213', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222726C5248>\n",
      "Processed  5 models on 20 predictors in 0.022711515426635742\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'NES010213', 'HSD410213', 'HSG010214', 'BZA010213', 'RTN130207', 'RHI825214', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x0000022271693508>\n",
      "forward\n",
      "Processed 20 models on 19 predictors in 0.09373354911804199\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'NES010213', 'HSD410213', 'HSG010214', 'BZA010213', 'RTN130207', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x0000022271693BC8>\n",
      "backward\n",
      "Processed  5 models on 20 predictors in 0.04688525199890137\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'NES010213', 'HSD410213', 'HSG010214', 'BZA010213', 'RTN130207', 'RHI825214', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222716A16C8>\n",
      "forward\n",
      "Processed 20 models on 19 predictors in 0.12605690956115723\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'NES010213', 'HSD410213', 'HSG010214', 'BZA010213', 'RTN130207', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x00000222746E6D88>\n",
      "backward\n",
      "Processed  5 models on 20 predictors in 0.03124403953552246\n",
      "Selected predictors: ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'NES010213', 'HSD410213', 'HSG010214', 'BZA010213', 'RTN130207', 'RHI825214', 'const'] AIC :  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002227169AF08>\n",
      "forward\n",
      "Processed 20 models on 19 predictors in 0.09373331069946289\n",
      "Selected predictors : ['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', 'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', 'NES010213', 'HSD410213', 'HSG010214', 'BZA010213', 'RTN130207', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002227169A688>\n",
      "backward\n",
      "Total elapsed time :  3.461423635482788 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#변수 선택법\n",
    "def processSubset(x,y, feature_set):\n",
    "    model = sm.OLS(y,x[list(feature_set)])  #modeling\n",
    "    regr = model.fit()  #모델학습\n",
    "    AIC = regr.aic    #모델의 AIC\n",
    "    return {\"model\" : regr , \"AIC\" : AIC}\n",
    " \n",
    "print(processSubset(X,y,feature_set=feature_columns))\n",
    " \n",
    "#모든 조합을 다 조합해서 좋은 모델을 반환시키는 알고리즘\n",
    "import time\n",
    "import itertools\n",
    " \n",
    "def getBest(x,y,k):\n",
    "    tic = time.time()  #시작시간\n",
    "    results = []       #결과저장공간\n",
    "    for combo in itertools.combinations(x.columns.difference(['const']),k): \n",
    "        combo=(list(combo)+['const'])\n",
    "        #각 변수조합을 고려한 경우의 수\n",
    "        results.append(processSubset(x,y,feature_set=combo))#모델링된 것들을 저장\n",
    "    models=pd.DataFrame(results) #데이터 프레임으로 변환\n",
    "    #가장 낮은 AIC를 가지는 모델 선택 및 저장\n",
    "    bestModel = models.loc[models['AIC'].argmin()] #index\n",
    "    toc = time.time() #종료시간\n",
    "    print(\"Processed\",models.shape[0],\"models on\",k,\"predictors in\",(toc-tic),\n",
    "          \"seconds.\")\n",
    "    return  bestModel\n",
    " \n",
    "#print(getBest(x=trainX,y=trainY,k=2))\n",
    " \n",
    "#변수 선택에 따른 학습시간과 저장 K 반복\n",
    " \n",
    " \n",
    "models = pd.DataFrame(columns=[\"AIC\",\"model\"])\n",
    "tic = time.time()\n",
    "for i in range(1,4):\n",
    "    models.loc[i] = getBest(X,y,i)\n",
    " \n",
    "#toc = time.time()\n",
    "#print(\"Total elapsed time : \",(toc-tic),\"seconds\")\n",
    " \n",
    "#print(models)\n",
    " \n",
    "#전진 선택법(Step=1)\n",
    " \n",
    "def forward(x,y,predictors):\n",
    "    remainingPredictors = [p for p in x.columns.difference(['const'])\n",
    "                           if p not in predictors]\n",
    "    tic=time.time()\n",
    "    results=[]\n",
    "    for p in remainingPredictors:\n",
    "        results.append(processSubset(x=x,y=y,feature_set=predictors+[p]+['const']))\n",
    "    #데이터프레임으로 변환\n",
    "    models = pd.DataFrame(results)\n",
    " \n",
    "    #AIC가 가장 낮은 것을 선택\n",
    "    bestModel = models.loc[models['AIC'].argmin()] #index\n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0],\"models on\", len(predictors)+1,\n",
    "          \"predictors in\",(toc-tic))\n",
    "    print(\"Selected predictors:\",bestModel['model'].model.exog_names,\n",
    "          'AIC : ',bestModel[0])\n",
    "    return bestModel\n",
    "\"\"\" \n",
    "#전진선택법 모델\n",
    "def forward_model(x,y):\n",
    "    fModels = pd.DataFrame(columns=[\"AIC\",\"model\"])\n",
    "    tic = time.time()\n",
    "    #미리 정의된 데이터 변수\n",
    "    predictors = []\n",
    "    #변수1~10개 : 0~9 -> 1~10\n",
    "    for i in range(1,len(x.columns.difference(['const']))+1):\n",
    "        forwardResult= forward(x,y,predictors)\n",
    "        if i > 1:\n",
    "            if forwardResult['AIC'] > fmodelBefore:\n",
    "                break\n",
    "        fModels.loc[i] = forwardResult\n",
    "        predictors = fModels.loc[i][\"model\"].model.exog_names\n",
    "        fmodelBefore = fModels.loc[i][\"AIC\"]\n",
    "        predictors = [k for k in predictors if k != 'const']\n",
    "    toc = time.time()\n",
    "    print(\"Total elapesed time : \", (toc - tic), \"seconds.\")\n",
    " \n",
    "    return (fModels['model'][len(fModels['model'])])\n",
    " \n",
    "forwordBestModel=forward_model(X,y)\n",
    " \n",
    "print(forwordBestModel.summary())\n",
    "\"\"\"\n",
    "\n",
    " \n",
    " \n",
    "#후진제거법\n",
    " \n",
    "def backward(x,y,predictors):\n",
    "    tic = time.time()\n",
    "    results=[]\n",
    "    #데이터 변수들이 미리정의된 predictors 조합확인\n",
    "    for combo in itertools.combinations(predictors, len(predictors)-1):\n",
    "        results.append(processSubset(x,y,list(combo)+['const']))\n",
    "    models = pd.DataFrame(results)\n",
    "    #가장 낮은 AIC를 가진 모델을 선택\n",
    "    bestModel = models.loc[models['AIC'].argmin()]\n",
    "    toc = time.time()\n",
    "    print(\"Processed\",models.shape[0],\"models on\",len(predictors)-1,\n",
    "          \"predictors in\",(toc - tic))\n",
    "    print(\"Selected predictors :\",bestModel['model'].model.exog_names,\n",
    "          ' AIC:',bestModel[0])\n",
    "    return bestModel\n",
    "\"\"\"\n",
    "def backword_model(x,y):\n",
    "    BModels = pd.DataFrame(columns=[\"AIC\",\"model\"])\n",
    "    tic = time.time()\n",
    "    #미리 정의된 데이터 변수\n",
    "    predictors = x.columns.difference(['const'])\n",
    "    BmodelBefore = processSubset(x,y,predictors)['AIC']\n",
    "    while(len(predictors)>1):\n",
    "        backwardResult=backward(X,y,predictors)\n",
    "        if backwardResult['AIC'] > BmodelBefore:\n",
    "            break\n",
    "        BModels.loc[len(predictors)-1] = backwardResult\n",
    "        predictors = BModels.loc[len(predictors)-1][\"model\"].model.exog_names\n",
    "        BmodelBefore = backwardResult[\"AIC\"]\n",
    "        predictors = [ k for k in predictors if k != 'const']\n",
    " \n",
    "    toc = time.time()\n",
    "    print(\"Total elapsed time :\",(toc - tic), \"seconds.\")\n",
    "    return (BModels[\"model\"].dropna().iloc[0])\n",
    " \n",
    "backwardBestModel = backword_model(X,y)\n",
    "\"\"\"\n",
    "def Stepwise_model(x,y):\n",
    "    stepModels = pd.DataFrame(columns=[\"AIC\",\"model\"])\n",
    "    tic = time.time()\n",
    "    predictors = []\n",
    "    SmodelBefore = processSubset(x,y,predictors+['const'])['AIC']\n",
    "    \n",
    "    for i in range(1, len(x.columns.difference(['const']))+1):\n",
    "        forwardResult = forward(x,y,predictors)\n",
    "        print(\"forward\")\n",
    "        stepModels.loc[i] = forwardResult\n",
    "        predictors = stepModels.loc[i][\"model\"].model.exog_names\n",
    "        predictors = [k for k in predictors if k != 'const']\n",
    "        backwordResult = backward(x,y,predictors)\n",
    "        if backwordResult['AIC'] < forwardResult['AIC']:\n",
    "            stepModels.loc[i] = backwordResult\n",
    "            predictors=stepModels.loc[i][\"model\"].model.exog_names\n",
    "            smodelBefore=stepModels.loc[i][\"AIC\"]\n",
    "            predictors=[k for k in predictors if k != 'const']\n",
    "            print('backward')\n",
    "        if stepModels.loc[i][\"AIC\"] > SmodelBefore:\n",
    "            break\n",
    "        else:\n",
    "            smodelBefore = stepModels.loc[i][\"AIC\"]\n",
    "    toc=time.time()\n",
    "    print(\"Total elapsed time : \", (toc - tic), \"seconds\")\n",
    "    return (stepModels['model'][len(stepModels['model'])])\n",
    " \n",
    "stepwiseBestModel = Stepwise_model(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>per_gop_2016</td>   <th>  R-squared:         </th> <td>   0.687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   357.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 14 Oct 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:20:29</td>     <th>  Log-Likelihood:    </th> <td>  3164.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  3113</td>      <th>  AIC:               </th> <td>  -6290.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  3093</td>      <th>  BIC:               </th> <td>  -6169.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    19</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HSG096213</th> <td>   -0.0067</td> <td>    0.000</td> <td>  -19.394</td> <td> 0.000</td> <td>   -0.007</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RHI125214</th> <td>    0.0044</td> <td>    0.000</td> <td>   18.147</td> <td> 0.000</td> <td>    0.004</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HSG495213</th> <td>-5.242e-07</td> <td> 3.44e-08</td> <td>  -15.252</td> <td> 0.000</td> <td>-5.92e-07</td> <td>-4.57e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SBO415207</th> <td>   -0.0036</td> <td>    0.000</td> <td>   -9.832</td> <td> 0.000</td> <td>   -0.004</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EDU685213</th> <td>   -0.0027</td> <td>    0.000</td> <td>   -8.907</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SBO215207</th> <td>    0.0082</td> <td>    0.001</td> <td>    5.709</td> <td> 0.000</td> <td>    0.005</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE775214</th> <td>   -0.0035</td> <td>    0.000</td> <td>   -7.876</td> <td> 0.000</td> <td>   -0.004</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>POP815213</th> <td>   -0.0043</td> <td>    0.000</td> <td>  -13.156</td> <td> 0.000</td> <td>   -0.005</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>POP645213</th> <td>    0.0074</td> <td>    0.001</td> <td>   10.434</td> <td> 0.000</td> <td>    0.006</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SBO315207</th> <td>   -0.0010</td> <td>    0.000</td> <td>   -3.103</td> <td> 0.002</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HSG445213</th> <td>   -0.0015</td> <td>    0.000</td> <td>   -4.551</td> <td> 0.000</td> <td>   -0.002</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SBO015207</th> <td>   -0.0005</td> <td>    0.000</td> <td>   -3.484</td> <td> 0.001</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RHI425214</th> <td>   -0.0033</td> <td>    0.002</td> <td>   -2.007</td> <td> 0.045</td> <td>   -0.007</td> <td>-7.51e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RHI225214</th> <td>   -0.0007</td> <td>    0.000</td> <td>   -2.609</td> <td> 0.009</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NES010213</th> <td> 7.886e-07</td> <td> 3.22e-07</td> <td>    2.448</td> <td> 0.014</td> <td> 1.57e-07</td> <td> 1.42e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HSD410213</th> <td>-1.536e-06</td> <td> 2.93e-07</td> <td>   -5.233</td> <td> 0.000</td> <td>-2.11e-06</td> <td> -9.6e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HSG010214</th> <td> 9.952e-07</td> <td> 2.48e-07</td> <td>    4.021</td> <td> 0.000</td> <td>  5.1e-07</td> <td> 1.48e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BZA010213</th> <td> 5.641e-06</td> <td> 1.63e-06</td> <td>    3.457</td> <td> 0.001</td> <td> 2.44e-06</td> <td> 8.84e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RTN130207</th> <td>-4.872e-09</td> <td> 2.57e-09</td> <td>   -1.895</td> <td> 0.058</td> <td>-9.91e-09</td> <td> 1.69e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>     <td>    0.6579</td> <td>    0.030</td> <td>   22.200</td> <td> 0.000</td> <td>    0.600</td> <td>    0.716</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>64.215</td> <th>  Durbin-Watson:     </th> <td>   1.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  74.504</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.299</td> <th>  Prob(JB):          </th> <td>6.63e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.464</td> <th>  Cond. No.          </th> <td>8.18e+07</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 8.18e+07. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:           per_gop_2016   R-squared:                       0.687\n",
       "Model:                            OLS   Adj. R-squared:                  0.685\n",
       "Method:                 Least Squares   F-statistic:                     357.2\n",
       "Date:                Wed, 14 Oct 2020   Prob (F-statistic):               0.00\n",
       "Time:                        19:20:29   Log-Likelihood:                 3164.9\n",
       "No. Observations:                3113   AIC:                            -6290.\n",
       "Df Residuals:                    3093   BIC:                            -6169.\n",
       "Df Model:                          19                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "HSG096213     -0.0067      0.000    -19.394      0.000      -0.007      -0.006\n",
       "RHI125214      0.0044      0.000     18.147      0.000       0.004       0.005\n",
       "HSG495213  -5.242e-07   3.44e-08    -15.252      0.000   -5.92e-07   -4.57e-07\n",
       "SBO415207     -0.0036      0.000     -9.832      0.000      -0.004      -0.003\n",
       "EDU685213     -0.0027      0.000     -8.907      0.000      -0.003      -0.002\n",
       "SBO215207      0.0082      0.001      5.709      0.000       0.005       0.011\n",
       "AGE775214     -0.0035      0.000     -7.876      0.000      -0.004      -0.003\n",
       "POP815213     -0.0043      0.000    -13.156      0.000      -0.005      -0.004\n",
       "POP645213      0.0074      0.001     10.434      0.000       0.006       0.009\n",
       "SBO315207     -0.0010      0.000     -3.103      0.002      -0.002      -0.000\n",
       "HSG445213     -0.0015      0.000     -4.551      0.000      -0.002      -0.001\n",
       "SBO015207     -0.0005      0.000     -3.484      0.001      -0.001      -0.000\n",
       "RHI425214     -0.0033      0.002     -2.007      0.045      -0.007   -7.51e-05\n",
       "RHI225214     -0.0007      0.000     -2.609      0.009      -0.001      -0.000\n",
       "NES010213   7.886e-07   3.22e-07      2.448      0.014    1.57e-07    1.42e-06\n",
       "HSD410213  -1.536e-06   2.93e-07     -5.233      0.000   -2.11e-06    -9.6e-07\n",
       "HSG010214   9.952e-07   2.48e-07      4.021      0.000     5.1e-07    1.48e-06\n",
       "BZA010213   5.641e-06   1.63e-06      3.457      0.001    2.44e-06    8.84e-06\n",
       "RTN130207  -4.872e-09   2.57e-09     -1.895      0.058   -9.91e-09    1.69e-10\n",
       "const          0.6579      0.030     22.200      0.000       0.600       0.716\n",
       "==============================================================================\n",
       "Omnibus:                       64.215   Durbin-Watson:                   1.369\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               74.504\n",
       "Skew:                          -0.299   Prob(JB):                     6.63e-17\n",
       "Kurtosis:                       3.464   Cond. No.                     8.18e+07\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 8.18e+07. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stepwiseBestModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#변수 선택법\\ndef processSubset(x,y, feature_set):\\n    model = LogisticRegressionCV(cv=5, random_state=0)  #modeling\\n    regr = model.fit(x[list(feature_set)],y) #모델학습\\n    Score = regr.score(x,y)    #모델의 스코어\\n    return {\"model\" : regr , \"Score\" : Score}\\n    \\n \\nprint(processSubset(X,y,feature_set=feature_columns))\\n \\n#모든 조합을 다 조합해서 좋은 모델을 반환시키는 알고리즘\\nimport time\\nimport itertools\\n \\ndef getBest(x,y,k):\\n    tic = time.time()  #시작시간\\n    results = []       #결과저장공간\\n    for combo in itertools.combinations(x.columns.difference([\\'const\\']),k): \\n        combo=(list(combo))\\n        #각 변수조합을 고려한 경우의 수\\n        results.append(processSubset(x,y,feature_set=combo))#모델링된 것들을 저장\\n    models=pd.DataFrame(results) #데이터 프레임으로 변환\\n    #가장 높은 Score를 가지는 모델 선택 및 저장\\n    bestModel = models.loc[models[\\'Score\\'].argmax()] #index\\n    toc = time.time() #종료시간\\n    print(\"Processed\",models.shape[0],\"models on\",k,\"predictors in\",(toc-tic),\\n          \"seconds.\")\\n    return  bestModel\\n \\n#print(getBest(x=trainX,y=trainY,k=2))\\n \\n#변수 선택에 따른 학습시간과 저장 K 반복\\n \\n \\nmodels = pd.DataFrame(columns=[\"Score\",\"model\"])\\ntic = time.time()\\nfor i in range(1,4):\\n    models.loc[i] = getBest(X,y,i)\\n \\n#toc = time.time()\\n#print(\"Total elapsed time : \",(toc-tic),\"seconds\")\\n \\n#print(models)\\n \\n#전진 선택법(Step=1)\\n \\ndef forward(x,y,predictors):\\n    remainingPredictors = [p for p in x.columns.difference([\\'const\\'])\\n                           if p not in predictors]\\n    tic=time.time()\\n    results=[]\\n    for p in remainingPredictors:\\n        results.append(processSubset(x=x,y=y,feature_set=predictors+[p]+[\\'const\\']))\\n    #데이터프레임으로 변환\\n    models = pd.DataFrame(results)\\n \\n    #가장 높은 Score를 가지는 모델 선택 및 저장\\n    bestModel = models.loc[models[\\'Score\\'].argmax()] #index\\n    toc = time.time()\\n    print(\"Processed \", models.shape[0],\"models on\", len(predictors)+1,\\n          \"predictors in\",(toc-tic))\\n    print(\"Selected predictors:\",bestModel[\\'model\\'].model.exog_names,\\n          \\'Score : \\',bestModel[0])\\n    return bestModel\\n\"\"\" \\n#전진선택법 모델\\ndef forward_model(x,y):\\n    fModels = pd.DataFrame(columns=[\"AIC\",\"model\"])\\n    tic = time.time()\\n    #미리 정의된 데이터 변수\\n    predictors = []\\n    #변수1~10개 : 0~9 -> 1~10\\n    for i in range(1,len(x.columns.difference([\\'const\\']))+1):\\n        forwardResult= forward(x,y,predictors)\\n        if i > 1:\\n            if forwardResult[\\'AIC\\'] > fmodelBefore:\\n                break\\n        fModels.loc[i] = forwardResult\\n        predictors = fModels.loc[i][\"model\"].model.exog_names\\n        fmodelBefore = fModels.loc[i][\"AIC\"]\\n        predictors = [k for k in predictors if k != \\'const\\']\\n    toc = time.time()\\n    print(\"Total elapesed time : \", (toc - tic), \"seconds.\")\\n \\n    return (fModels[\\'model\\'][len(fModels[\\'model\\'])])\\n \\nforwordBestModel=forward_model(X,y)\\n \\nprint(forwordBestModel.summary())\\n\"\"\"\\n\\n \\n \\n#후진제거법\\n \\ndef backward(x,y,predictors):\\n    tic = time.time()\\n    results=[]\\n    #데이터 변수들이 미리정의된 predictors 조합확인\\n    for combo in itertools.combinations(predictors, len(predictors)-1):\\n        results.append(processSubset(x,y,list(combo)+[\\'const\\']))\\n    models = pd.DataFrame(results)\\n    #가장 높은 Score를 가지는 모델 선택 및 저장\\n    bestModel = models.loc[models[\\'Score\\'].argmax()] #index\\n    toc = time.time()\\n    print(\"Processed\",models.shape[0],\"models on\",len(predictors)-1,\\n          \"predictors in\",(toc - tic))\\n    print(\"Selected predictors :\",bestModel[\\'model\\'].model.exog_names,\\n          \\' Score:\\',bestModel[0])\\n    return bestModel\\n\"\"\"\\ndef backword_model(x,y):\\n    BModels = pd.DataFrame(columns=[\"AIC\",\"model\"])\\n    tic = time.time()\\n    #미리 정의된 데이터 변수\\n    predictors = x.columns.difference([\\'const\\'])\\n    BmodelBefore = processSubset(x,y,predictors)[\\'AIC\\']\\n    while(len(predictors)>1):\\n        backwardResult=backward(X,y,predictors)\\n        if backwardResult[\\'AIC\\'] > BmodelBefore:\\n            break\\n        BModels.loc[len(predictors)-1] = backwardResult\\n        predictors = BModels.loc[len(predictors)-1][\"model\"].model.exog_names\\n        BmodelBefore = backwardResult[\"AIC\"]\\n        predictors = [ k for k in predictors if k != \\'const\\']\\n \\n    toc = time.time()\\n    print(\"Total elapsed time :\",(toc - tic), \"seconds.\")\\n    return (BModels[\"model\"].dropna().iloc[0])\\n \\nbackwardBestModel = backword_model(X,y)\\n\"\"\"\\ndef Stepwise_model(x,y):\\n    stepModels = pd.DataFrame(columns=[\"Score\",\"model\"])\\n    tic = time.time()\\n    predictors = []\\n    SmodelBefore = processSubset(x,y,predictors+[\\'const\\'])[\\'Score\\']\\n    #변수 1~10개 : 0~9 -> 1~10\\n    for i in range(1, len(x.columns.difference([\\'const\\']))+1):\\n        forwardResult = forward(x,y,predictors)\\n        print(\"forward\")\\n        stepModels.loc[i] = forwardResult\\n        predictors = stepModels.loc[i][\"model\"].model.exog_names\\n        predictors = [k for k in predictors if k != \\'const\\']\\n        backwordResult = backward(x,y,predictors)\\n        if backwordResult[\\'Score\\'] > forwardResult[\\'Score\\']:\\n            stepModels.loc[i] = backwordResult\\n            predictors=stepModels.loc[i][\"model\"].model.exog_names\\n            smodelBefore=stepModels.loc[i][\"Score\"]\\n            predictors=[k for k in predictors if k != \\'const\\']\\n            print(\\'backward\\')\\n        if stepModels.loc[i][\"Score\"] < SmodelBefore:\\n            break\\n        else:\\n            smodelBefore = stepModels.loc[i][\"Score\"]\\n    toc=time.time()\\n    print(\"Total elapsed time : \", (toc - tic), \"seconds\")\\n    return (stepModels[\\'model\\'][len(stepModels[\\'model\\'])])\\n \\nstepwiseBestModel = Stepwise_model(X,y)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#변수 선택법\n",
    "def processSubset(x,y, feature_set):\n",
    "    model = LogisticRegressionCV(cv=5, random_state=0)  #modeling\n",
    "    regr = model.fit(x[list(feature_set)],y) #모델학습\n",
    "    Score = regr.score(x,y)    #모델의 스코어\n",
    "    return {\"model\" : regr , \"Score\" : Score}\n",
    "    \n",
    " \n",
    "print(processSubset(X,y,feature_set=feature_columns))\n",
    " \n",
    "#모든 조합을 다 조합해서 좋은 모델을 반환시키는 알고리즘\n",
    "import time\n",
    "import itertools\n",
    " \n",
    "def getBest(x,y,k):\n",
    "    tic = time.time()  #시작시간\n",
    "    results = []       #결과저장공간\n",
    "    for combo in itertools.combinations(x.columns.difference(['const']),k): \n",
    "        combo=(list(combo))\n",
    "        #각 변수조합을 고려한 경우의 수\n",
    "        results.append(processSubset(x,y,feature_set=combo))#모델링된 것들을 저장\n",
    "    models=pd.DataFrame(results) #데이터 프레임으로 변환\n",
    "    #가장 높은 Score를 가지는 모델 선택 및 저장\n",
    "    bestModel = models.loc[models['Score'].argmax()] #index\n",
    "    toc = time.time() #종료시간\n",
    "    print(\"Processed\",models.shape[0],\"models on\",k,\"predictors in\",(toc-tic),\n",
    "          \"seconds.\")\n",
    "    return  bestModel\n",
    " \n",
    "#print(getBest(x=trainX,y=trainY,k=2))\n",
    " \n",
    "#변수 선택에 따른 학습시간과 저장 K 반복\n",
    " \n",
    " \n",
    "models = pd.DataFrame(columns=[\"Score\",\"model\"])\n",
    "tic = time.time()\n",
    "for i in range(1,4):\n",
    "    models.loc[i] = getBest(X,y,i)\n",
    " \n",
    "#toc = time.time()\n",
    "#print(\"Total elapsed time : \",(toc-tic),\"seconds\")\n",
    " \n",
    "#print(models)\n",
    " \n",
    "#전진 선택법(Step=1)\n",
    " \n",
    "def forward(x,y,predictors):\n",
    "    remainingPredictors = [p for p in x.columns.difference(['const'])\n",
    "                           if p not in predictors]\n",
    "    tic=time.time()\n",
    "    results=[]\n",
    "    for p in remainingPredictors:\n",
    "        results.append(processSubset(x=x,y=y,feature_set=predictors+[p]+['const']))\n",
    "    #데이터프레임으로 변환\n",
    "    models = pd.DataFrame(results)\n",
    " \n",
    "    #가장 높은 Score를 가지는 모델 선택 및 저장\n",
    "    bestModel = models.loc[models['Score'].argmax()] #index\n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0],\"models on\", len(predictors)+1,\n",
    "          \"predictors in\",(toc-tic))\n",
    "    print(\"Selected predictors:\",bestModel['model'].model.exog_names,\n",
    "          'Score : ',bestModel[0])\n",
    "    return bestModel\n",
    "\"\"\" \n",
    "#전진선택법 모델\n",
    "def forward_model(x,y):\n",
    "    fModels = pd.DataFrame(columns=[\"AIC\",\"model\"])\n",
    "    tic = time.time()\n",
    "    #미리 정의된 데이터 변수\n",
    "    predictors = []\n",
    "    #변수1~10개 : 0~9 -> 1~10\n",
    "    for i in range(1,len(x.columns.difference(['const']))+1):\n",
    "        forwardResult= forward(x,y,predictors)\n",
    "        if i > 1:\n",
    "            if forwardResult['AIC'] > fmodelBefore:\n",
    "                break\n",
    "        fModels.loc[i] = forwardResult\n",
    "        predictors = fModels.loc[i][\"model\"].model.exog_names\n",
    "        fmodelBefore = fModels.loc[i][\"AIC\"]\n",
    "        predictors = [k for k in predictors if k != 'const']\n",
    "    toc = time.time()\n",
    "    print(\"Total elapesed time : \", (toc - tic), \"seconds.\")\n",
    " \n",
    "    return (fModels['model'][len(fModels['model'])])\n",
    " \n",
    "forwordBestModel=forward_model(X,y)\n",
    " \n",
    "print(forwordBestModel.summary())\n",
    "\"\"\"\n",
    "\n",
    " \n",
    " \n",
    "#후진제거법\n",
    " \n",
    "def backward(x,y,predictors):\n",
    "    tic = time.time()\n",
    "    results=[]\n",
    "    #데이터 변수들이 미리정의된 predictors 조합확인\n",
    "    for combo in itertools.combinations(predictors, len(predictors)-1):\n",
    "        results.append(processSubset(x,y,list(combo)+['const']))\n",
    "    models = pd.DataFrame(results)\n",
    "    #가장 높은 Score를 가지는 모델 선택 및 저장\n",
    "    bestModel = models.loc[models['Score'].argmax()] #index\n",
    "    toc = time.time()\n",
    "    print(\"Processed\",models.shape[0],\"models on\",len(predictors)-1,\n",
    "          \"predictors in\",(toc - tic))\n",
    "    print(\"Selected predictors :\",bestModel['model'].model.exog_names,\n",
    "          ' Score:',bestModel[0])\n",
    "    return bestModel\n",
    "\"\"\"\n",
    "def backword_model(x,y):\n",
    "    BModels = pd.DataFrame(columns=[\"AIC\",\"model\"])\n",
    "    tic = time.time()\n",
    "    #미리 정의된 데이터 변수\n",
    "    predictors = x.columns.difference(['const'])\n",
    "    BmodelBefore = processSubset(x,y,predictors)['AIC']\n",
    "    while(len(predictors)>1):\n",
    "        backwardResult=backward(X,y,predictors)\n",
    "        if backwardResult['AIC'] > BmodelBefore:\n",
    "            break\n",
    "        BModels.loc[len(predictors)-1] = backwardResult\n",
    "        predictors = BModels.loc[len(predictors)-1][\"model\"].model.exog_names\n",
    "        BmodelBefore = backwardResult[\"AIC\"]\n",
    "        predictors = [ k for k in predictors if k != 'const']\n",
    " \n",
    "    toc = time.time()\n",
    "    print(\"Total elapsed time :\",(toc - tic), \"seconds.\")\n",
    "    return (BModels[\"model\"].dropna().iloc[0])\n",
    " \n",
    "backwardBestModel = backword_model(X,y)\n",
    "\"\"\"\n",
    "def Stepwise_model(x,y):\n",
    "    stepModels = pd.DataFrame(columns=[\"Score\",\"model\"])\n",
    "    tic = time.time()\n",
    "    predictors = []\n",
    "    SmodelBefore = processSubset(x,y,predictors+['const'])['Score']\n",
    "    #변수 1~10개 : 0~9 -> 1~10\n",
    "    for i in range(1, len(x.columns.difference(['const']))+1):\n",
    "        forwardResult = forward(x,y,predictors)\n",
    "        print(\"forward\")\n",
    "        stepModels.loc[i] = forwardResult\n",
    "        predictors = stepModels.loc[i][\"model\"].model.exog_names\n",
    "        predictors = [k for k in predictors if k != 'const']\n",
    "        backwordResult = backward(x,y,predictors)\n",
    "        if backwordResult['Score'] > forwardResult['Score']:\n",
    "            stepModels.loc[i] = backwordResult\n",
    "            predictors=stepModels.loc[i][\"model\"].model.exog_names\n",
    "            smodelBefore=stepModels.loc[i][\"Score\"]\n",
    "            predictors=[k for k in predictors if k != 'const']\n",
    "            print('backward')\n",
    "        if stepModels.loc[i][\"Score\"] < SmodelBefore:\n",
    "            break\n",
    "        else:\n",
    "            smodelBefore = stepModels.loc[i][\"Score\"]\n",
    "    toc=time.time()\n",
    "    print(\"Total elapsed time : \", (toc - tic), \"seconds\")\n",
    "    return (stepModels['model'][len(stepModels['model'])])\n",
    " \n",
    "stepwiseBestModel = Stepwise_model(X,y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stepwiseBestModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "Final_X = data[['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', \n",
    "                'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', \n",
    "                'NES010213', 'HSD410213', 'HSG010214', 'BZA010213', 'RTN130207']]\n",
    "y = data['target_2016']\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(Final_X, y, test_size=0.2)\n",
    "\n",
    "clf1 = LogisticRegression(C=1, max_iter=1000).fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic model's Accuracy score is : 0.9020866773675762\n",
      "Logistic model's Presision score is : 0.8181818181818182\n",
      "Logistic model's Recall score is : 0.46875\n",
      "Logistic model's F1 score is : 0.5960264900662251\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = clf1.predict(x_test)\n",
    "\n",
    "print(\"Logistic model's Accuracy score is :\",accuracy_score(y_test, y_pred2))\n",
    "print(\"Logistic model's Presision score is :\",precision_score(y_test, y_pred2))\n",
    "print(\"Logistic model's Recall score is :\",recall_score(y_test, y_pred2))\n",
    "print(\"Logistic model's F1 score is :\",f1_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sean\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sean\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sean\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sean\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sean\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sean\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sean\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sean\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sean\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sean\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sean\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sean\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sean\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sean\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "Final_X = data[['HSG096213', 'RHI125214', 'HSG495213', 'SBO415207', 'EDU685213', 'SBO215207', 'AGE775214', \n",
    "                'POP815213', 'POP645213', 'SBO315207', 'HSG445213', 'SBO015207', 'RHI425214', 'RHI225214', \n",
    "                'NES010213', 'HSD410213', 'HSG010214', 'BZA010213', 'RTN130207']]\n",
    "y = data['target_2016']\n",
    "\n",
    "clf2 = LogisticRegressionCV(cv=5, random_state=0).fit(Final_X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticCV model's Accuracy score is : 0.8904593639575972\n",
      "LogisticCV model's Presision score is : 0.8356164383561644\n",
      "LogisticCV model's Recall score is : 0.375\n",
      "LogisticCV model's F1 score is : 0.5176803394625177\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf2.predict(Final_X)\n",
    "\n",
    "\n",
    "print(\"LogisticCV model's Accuracy score is :\",accuracy_score(y, y_pred))\n",
    "print(\"LogisticCV model's Presision score is :\",precision_score(y, y_pred))\n",
    "print(\"LogisticCV model's Recall score is :\",recall_score(y, y_pred))\n",
    "print(\"LogisticCV model's F1 score is :\",f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2625\n",
       "1     488\n",
       "Name: target_2016, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target_2016'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2427\n",
       "1     686\n",
       "Name: target_2012, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target_2012'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3113, 77)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

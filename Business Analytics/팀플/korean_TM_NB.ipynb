{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "korean_TM_NB.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPowqVQNOvS4rMC6cC5FlP3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwr0218/analysis_reviews_restaurant_with_Korean_English/blob/main/korean_TM_NB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYpSmlN9maGg"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt06YjbRlrJD"
      },
      "source": [
        "!pip install lda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6TvSS-mlv24"
      },
      "source": [
        "\n",
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9Vj3PxJlUgI"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import collections\n",
        "import itertools\n",
        "import lda\n",
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import math\n",
        "import operator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from pandas import read_table\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from konlpy  import tag \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQdOtMHwmAiQ"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/data_mango_merged.csv', encoding = 'utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Xgy3HgmWYi"
      },
      "source": [
        "data\n",
        "\n",
        "data['evaluation'] = data.apply(lambda x : 1 if x['evaluation'] == '맛있다' else x['evaluation'],axis = 1)\n",
        "\n",
        "\n",
        "data['evaluation'] = data.apply(lambda x : 0 if x['evaluation'] == '별로' else x['evaluation'],axis = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uNhS8rwm1rw"
      },
      "source": [
        "total_review = data['review']\n",
        "positive_review =  data[data['evaluation']==1]['review']\n",
        "\n",
        "\n",
        "negative_review = data[data['evaluation']==0]['review']\n",
        "\n",
        "\n",
        "tot_review = total_review.values.tolist()\n",
        "pos_review = positive_review.values.tolist()\n",
        "neg_review = negative_review.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP6NHNB_nab4"
      },
      "source": [
        "pos_word = ['같다', '맛있다', '나오다', '없다', '메뉴', '자다', '가다', '들다', '정도', '느낌', '방문', '생각', '싶다', '그렇다', '인데', '주문', '되어다', '좋아하다', '만들다', '느끼다', '조금', '식감', '오다', '재료', '살짝', '하나', '소스', '까지', '에는', '들어가다', '주다', '많다', '나다', '음식', '시키다', '정말', '처럼', '역시', '부드럽다', '느껴지다']\n",
        "\n",
        "neg_word = ['가다', '같다', '맛있다', '없다', '들다', '인데', '까지', '정말', '아쉽다', '느낌', '주문', '알다', '가격', '이렇다', '괜찮다', '메뉴', '방문', '많다', '레스토랑', '자다', '이라고는', '아내', '방법', '별로', '그렇다', '주다', '껍질', '소스', '서비스', '예전', '이집', '에는', '식당', '예약', '게다가', '식감', '기억', '요리', '고기', '지금']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZe-q4hf7qzg"
      },
      "source": [
        "data['evaluation'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSMBpJXC2zjn"
      },
      "source": [
        "class SentenceTokenizer(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        self.kkma = tag.Kkma()\n",
        "        self.okt = tag.Okt()\n",
        "        self.twitter = tag.Twitter()\n",
        "        self.stopwords = ['중인' ,'만큼', '마찬가지', '꼬집었', \"연합뉴스\", \"데일리\", \"동아일보\", \"중앙일보\", \"조선일보\", \"기자\",\"아\", \"휴\", \"아이구\", \"아이쿠\", \"아이고\", \"어\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\", \"을\", \"를\", \"에\", \"의\", \"가\",]\n",
        "    \n",
        "    \n",
        "    def text2sentences(self, text):\n",
        "        sentences = self.kkma.sentences(text)\n",
        "        for idx in range(0, len(sentences)):\n",
        "            if len(sentences[idx]) <= 10:\n",
        "                sentences[idx-1] += (' ' + sentences[idx])\n",
        "                sentences[idx] = ''\n",
        "        return sentences\n",
        "    \n",
        "    def get_nouns(self, sentences):\n",
        "        nouns = []\n",
        "        for sentence in sentences:\n",
        "            if sentence is not '':\n",
        "                nouns.append(' '.join([noun for noun in self.twitter.nouns(str(sentence))\n",
        "                                           if noun not in self.stopwords and len(noun) > 1]))\n",
        "        return nouns\n",
        "    \n",
        "    \n",
        "    def get_adj(self, sentences):\n",
        "        adj = []\n",
        "        count = dict()\n",
        "        not_use_lst = ['좋다','보다','하다','먹다','있다','되다','이다','아니다','이다','하고','으로','에서','않다',\n",
        "                       '에게','에서']\n",
        "        for sentence in sentences:\n",
        "            if sentence is not '':\n",
        "                find = [a[0] for a in self.okt.pos(str(sentence),norm = True ,stem = True)\n",
        "                                           if (a[1] in ['Adjective','Verb','Noun','Josa'] and a[0] not in not_use_lst) ]\n",
        "                adj.append(\" \".join(find))\n",
        "                for i in find:\n",
        "                    try:\n",
        "                        count[i] = count[i] + 1\n",
        "                    except KeyError:\n",
        "                        count[i] = 1        \n",
        "        return adj , count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAk8a5h9nkVT"
      },
      "source": [
        "tokenizer = SentenceTokenizer()\n",
        "okt = tag.Okt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#okt.pos(str(sentence),norm = True , stem = True)\n",
        "\n",
        "total_tokenized , total_count = tokenizer.get_adj(total_review)\n",
        "\n",
        "positive_tokenized ,positive_count = tokenizer.get_adj(positive_review)\n",
        "\n",
        "negative_tokenized, negative_count = tokenizer.get_adj(negative_review)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_HrIk0H7CM4"
      },
      "source": [
        "positive_tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDEO798xD9dZ"
      },
      "source": [
        "\n",
        "service_good_feature = {'좋':0,'친절':0,'괜찮':0,'최고':0,'빠르':0,'짱':0,'훌륭':0,'추천':0,'감사':0,'구수':0,'최상':0,'대박':0,'훈훈':0,'특별':0,'개이득':0,'최고':0,'만족':0,'세련':0,'최고':0,'감동':0,\n",
        "                        '스윗':0,'센스':0 ,'청결하다':0, '훌륭':0, '자부심':0\n",
        "                        }\n",
        "\n",
        "service_bad_feature = {'아쉽':0,'최악':0,'나쁘다':0,'느리':0,'빡치다':0,'비추':0,'별로':0,'그냥':0,'낙제':0,'엉망':0,'실망':0,'불친절':0,'문제':0,'컴플레인':0,'거지':0,'그닥':0,'그다지':0,'구려':0,'불편':0,'엉성':0,'헬':0,'개판':0,\n",
        "                       '의외의':0,'불통':0,'':0,'불친절하다':0,'똑바로':0,'싸가지':0,'지나치다':0,'분하다':0,'민망하다':0,'잃다':0}\n",
        "# 서비스 좋음 / 안좋음 \n",
        "#맛\n",
        "taste_good_feature = {'맛있다':0 ,'고급스럽다':0, '정말':0,'고소':0,'부드럽':0,'신선':0,'촉촉':0,'싱싱':0,'정갈':0,'존맛':0,'그리움':0,'만족':0,\n",
        "                      '풍부':0,'바삭':0,'신선하다':0 , '담백':0, '즐기다':0,'추천' : 0, '건강하다':0, '싱싱':0,'달콤':0\n",
        "\n",
        "                      }\n",
        "\n",
        "taste_bad_feature = {'맛없다':0,'불호':0,'노맛':0,'심각':0,'없다':0,'평범':0,'찝찝하다':0\n",
        "                     ,'싱겁':0,'느끼다하다':0,'짜다':0,'상해':0,'상하다':0,'느끼다':0,'짜다':0,'딱딱하다':0,'차갑다':0,'퍽퍽해':0, '퍽퍽':0,'모르다':0,'아쉽다':0,\n",
        "                     '버리다':0,\n",
        "                     '무난':0,'들쭉날쭉':0 , '싫어':0 }\n",
        "#느낌 입감 \n",
        "cost_good_feature = {'괜찮':0,'착하다':0,'저렴':0,'적당':0,'싸다':0,'좋다':0,'합리적':0,'훌륭':0,'최고':0,'만족':0,'마음':0,'든든':0,'알맞다':0,'무난':0,'괜춘':0,'최상':0,'낮':0\n",
        "                     ,'많':0,'적당':0,'푸짐':0,'괜찮다':0,'넉넉':0,'충분':0    \n",
        "                     }\n",
        "\n",
        "cost_bad_feature ={'비싸':0,'있다':0,'나쁘':0,'사악':0,'비효율':0,'높다':0,'부담':0,'아쉽':0,'별로':0,'그닥':0,'그다지':0,'쎄':0,'높':0,'거품':0,'눈물':0,\n",
        "                   '부족':0, '별로':0,'적다':0,'작다':0,'아쉽':0,'적다':0,'다소':0,'별로':0  \n",
        "                   }\n",
        "# 특징 \n",
        "atmosphere_good_feature = {'괜찮':0,'조용':0,'깔끔':0,'적당':0,'깡패':0,'굉장':0,'아담':0,'완벽':0,'아기자기':0,'고급':0,'최고':0,'세련':0,'만족':0,'아늑':0,'훌륭':0 , '한적하다':0 ,'한적':0,\n",
        "                           '좁다':0\n",
        "                            ,'예쁘':0,'이쁘':0,'짱':0,'심쿵':0,'따뜻':0,'깨끗':0,'독특':0,'매력':0,'모던':0,'취향저격':0,'맘':0,'마음':0,'클래식':0,'아름':0,'인상적':0,'귀엽':0,'포근':0\n",
        "                           ,'여유':0 \n",
        "\n",
        "                           \n",
        "                           \n",
        "                           } \n",
        "                           \n",
        "atmosphere_bad_feature = {'어수선하다':0,'부조화':0,'나쁘다':0,'바쁘다':0,'어수선하다':0,'이상하다':0,'촌스럽다':0,'별로':0,'부담스럽다':0,'시끄럽':0,'복잡':0 ,'어둡다':0,'불편':0,'과대':0,'애매':0,\n",
        "                          '불쾌' : 0  \n",
        "                          }\n",
        "#분위기\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsNUwnDt3jyi"
      },
      "source": [
        "def keyword_count(review,dic):\n",
        "  \n",
        "  for i in dic.keys():\n",
        "\n",
        "\n",
        "   \n",
        "    \n",
        "      \n",
        "    if i in review:\n",
        "\n",
        "\n",
        "      dic[i] +=1\n",
        "  return dic \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKVrqEn-97vK"
      },
      "source": [
        "keyword_count(total_tokenized[0],service_good_feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGBi1HMzq76L"
      },
      "source": [
        "preprocessed_data_service = pd.DataFrame()\n",
        "preprocessed_data_taste = pd.DataFrame()\n",
        "preprocessed_data_cost = pd.DataFrame()\n",
        "preprocessed_data_atmosphere = pd.DataFrame()\n",
        "\n",
        "for token in total_tokenized:\n",
        "  \n",
        "  service_good_feature = {'좋':0,'친절':0,'괜찮':0,'최고':0,'빠르':0,'짱':0,'훌륭':0,'추천':0,'감사':0,'구수':0,'최상':0,'대박':0,'훈훈':0,'특별':0,'개이득':0,'최고':0,'만족':0,'세련':0,'최고':0,'감동':0,\n",
        "                        '스윗':0,'센스':0 ,'청결하다':0, '훌륭':0, '자부심':0, \n",
        "                        }\n",
        "\n",
        "  service_bad_feature = {'아쉽':0,'최악':0,'나쁘다':0,'느리':0,'빡치다':0,'비추':0,'별로':0,'그냥':0,'낙제':0,'엉망':0,'실망':0,'불친절':0,'문제':0,'컴플레인':0,'거지':0,'그닥':0,'그다지':0,'구려':0,'불편':0,'엉성':0,'헬':0,'개판':0,\n",
        "                        '의외의':0,'불통':0,'':0,'불친절하다':0,'똑바로':0,'싸가지':0,'지나치다':0,'분하다':0,'민망하다':0,'잃다':0}\n",
        "  # 서비스 좋음 / 안좋음 \n",
        "\n",
        "  #맛\n",
        "  taste_good_feature = {'맛있다':0 ,'고급스럽다':0, '정말':0,'고소':0,'부드럽':0,'신선':0,'촉촉':0,'싱싱':0,'정갈':0,'존맛':0,'그리움':0,'만족':0,\n",
        "                        '풍부':0,'바삭':0,'신선하다':0 , '담백':0, '즐기다':0,'추천' : 0, '건강하다':0, '싱싱':0,'달콤':0\n",
        "\n",
        "                        }\n",
        "\n",
        "  taste_bad_feature = {'맛없다':0,'불호':0,'노맛':0,'심각':0,'없다':0,'평범':0,'찝찝하다':0\n",
        "                      ,'싱겁':0,'느끼다하다':0,'짜다':0,'상해':0,'상하다':0,'느끼다':0,'짜다':0,'딱딱하다':0,'차갑다':0,'퍽퍽해':0, '퍽퍽':0,'모르다':0,'아쉽다':0,\n",
        "                      '버리다':0,\n",
        "                      '무난':0,'들쭉날쭉':0 , '싫어':0 }\n",
        "  #느낌 입감 \n",
        "  cost_good_feature = {'괜찮':0,'착하다':0,'저렴':0,'적당':0,'싸다':0,'좋다':0,'합리적':0,'훌륭':0,'최고':0,'만족':0,'마음':0,'든든':0,'알맞다':0,'무난':0,'괜춘':0,'최상':0,'낮':0\n",
        "                      ,'많':0,'적당':0,'푸짐':0,'괜찮다':0,'넉넉':0,'충분':0                    \n",
        "                      }\n",
        "\n",
        "  cost_bad_feature ={'비싸':0,'있다':0,'나쁘':0,'사악':0,'비효율':0,'높다':0,'부담':0,'아쉽':0,'별로':0,'그닥':0,'그다지':0,'쎄':0,'높':0,'거품':0,'눈물':0,\n",
        "                    '부족':0, '별로':0,'적다':0,'작다':0,'아쉽':0,'적다':0,'다소':0,'별로':0 \n",
        "                    }\n",
        "  #\n",
        "  \n",
        "  atmosphere_good_feature = {'괜찮':0,'조용':0,'깔끔':0,'적당':0,'깡패':0,'굉장':0,'아담':0,'완벽':0,'아기자기':0,'고급':0,'최고':0,'세련':0,'만족':0,'아늑':0,'훌륭':0 , '한적하다':0 ,'한적':0,\n",
        "                            '좁다':0\n",
        "                              ,'예쁘':0,'이쁘':0,'짱':0,'심쿵':0,'따뜻':0,'깨끗':0,'독특':0,'매력':0,'모던':0,'취향저격':0,'맘':0,'마음':0,'클래식':0,'아름':0,'인상적':0,'귀엽':0,'포근':0\n",
        "                            ,'여유':0 \n",
        "\n",
        "                            \n",
        "                            \n",
        "                            } \n",
        "                            \n",
        "  atmosphere_bad_feature = {'어수선하다':0,'부조화':0,'나쁘다':0,'바쁘다':0,'어수선하다':0,'이상하다':0,'촌스럽다':0,'별로':0,'부담스럽다':0,'시끄럽':0,'복잡':0 ,'어둡다':0,'불편':0,'과대':0,'애매':0,\n",
        "                            '불쾌' : 0  \n",
        "                            }\n",
        "  #분위기\n",
        "  service_good_feature = keyword_count(token,service_good_feature)\n",
        "  service_bad_feature = keyword_count(token,service_bad_feature)\n",
        "  service_good_feature_df = pd.DataFrame(service_good_feature,index = [0])\n",
        "  service_bad_feature_df = pd.DataFrame(service_bad_feature,index = [0])\n",
        "  \n",
        "  preprocessed_data_service_concated = pd.concat([service_good_feature_df,service_bad_feature_df],axis = 1)\n",
        "\n",
        "\n",
        "  taste_good_feature = keyword_count(token,taste_good_feature)\n",
        "  taste_bad_feature = keyword_count(token,taste_bad_feature)\n",
        "  taste_good_feature_df = pd.DataFrame(taste_good_feature,index = [0])\n",
        "  taste_bad_feature_df = pd.DataFrame(taste_bad_feature,index = [0])\n",
        "  \n",
        "  \n",
        "  preprocessed_data_taste_concated = pd.concat([taste_good_feature_df,taste_bad_feature_df],axis = 1)\n",
        "\n",
        "  \n",
        "  \n",
        "  cost_good_feature = keyword_count(token,cost_good_feature )\n",
        "  cost_bad_feature = keyword_count(token,cost_bad_feature )\n",
        "  cost_good_feature_df = pd.DataFrame(cost_good_feature,index = [0])\n",
        "  cost_bad_feature_df = pd.DataFrame(cost_bad_feature,index = [0])\n",
        "  \n",
        "  preprocessed_data_cost_concated = pd.concat([cost_good_feature_df,cost_bad_feature_df],axis = 1)\n",
        "\n",
        "  \n",
        "  atmosphere_good_feature = keyword_count(token,atmosphere_good_feature)\n",
        "  atmosphere_bad_feature = keyword_count(token,atmosphere_bad_feature)\n",
        "  atmosphere_good_feature_df = pd.DataFrame(atmosphere_good_feature,index = [0])\n",
        "  atmosphere_bad_feature_df = pd.DataFrame(atmosphere_bad_feature,index = [0])\n",
        "\n",
        "  preprocessed_data_atmosphere_concated = pd.concat([atmosphere_good_feature_df,atmosphere_bad_feature_df],axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "  preprocessed_data_service = pd.concat([preprocessed_data_service,preprocessed_data_service_concated],ignore_index=True )\n",
        "  preprocessed_data_taste = pd.concat([preprocessed_data_taste,preprocessed_data_taste_concated],ignore_index=True )\n",
        "  preprocessed_data_cost = pd.concat([preprocessed_data_cost,preprocessed_data_cost_concated],ignore_index=True )\n",
        "  preprocessed_data_atmosphere = pd.concat([preprocessed_data_atmosphere,preprocessed_data_atmosphere_concated],ignore_index=True )\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJlGX8VQmiqR"
      },
      "source": [
        "preprocessed_data_service['evaluation'] = data['evaluation'].copy()\n",
        "preprocessed_data_service = preprocessed_data_service[preprocessed_data_service['evaluation']!='괜찮다']\n",
        "\n",
        "\n",
        "preprocessed_data_taste['evaluation'] = data['evaluation'].copy()\n",
        "\n",
        "preprocessed_data_taste = preprocessed_data_taste[preprocessed_data_taste['evaluation']!='괜찮다']\n",
        "\n",
        "preprocessed_data_cost['evaluation'] = data['evaluation'].copy()\n",
        "\n",
        "preprocessed_data_cost = preprocessed_data_cost[preprocessed_data_cost['evaluation']!='괜찮다']\n",
        "preprocessed_data_atmosphere['evaluation'] = data['evaluation'].copy()\n",
        "\n",
        "preprocessed_data_atmosphere = preprocessed_data_atmosphere[preprocessed_data_atmosphere['evaluation']!='괜찮다']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS7eOG8anpKI"
      },
      "source": [
        "preprocessed_data_atmosphere.iloc[:,:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XkBOV9oQTSq"
      },
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.under_sampling import NearMiss\n",
        "import pandas as pd\n",
        "\n",
        "import math \n",
        "\n",
        "# 정확도 계산을 위한 함수\n",
        "\n",
        "list = []\n",
        "coeff = []\n",
        "\n",
        "df = preprocessed_data_service\n",
        "for i in range(1000):\n",
        "    data_ran = df[df['evaluation'].isin([1])]\n",
        "\n",
        "    data_ran = data_ran.sample(n=390, random_state = 10)\n",
        "\n",
        "    data_zero = df[df['evaluation'].isin([0])]\n",
        "\n",
        "    df  = pd.concat([data_ran, data_zero])\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    x_train, x_test, y_train, y_test = model_selection.train_test_split(df.iloc[:,0:-1].values, df.iloc[:,-1].values.astype(int), test_size=0.3)\n",
        "\n",
        "    mod = BernoulliNB(alpha=1, class_prior=None, fit_prior=True)\n",
        "    mod.fit(x_train, y_train)\n",
        "\n",
        "    predicted = mod.predict(x_test)\n",
        "\n",
        "    coeff.append( mod.feature_log_prob_ )\n",
        "\n",
        "\n",
        " \n",
        "    list.append(accuracy_score(y_test, predicted))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "list_np = np.array(list)\n",
        "a = np.mean(list_np)\n",
        "b = np.mean(coeff,axis = 0)\n",
        "\n",
        "print('score is',a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhd-WpkOqfIa"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import silhouette_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LVqZFJiFzP5"
      },
      "source": [
        "\n",
        "df = df.drop([''],axis = 'columns')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKAvA_IlUvk7"
      },
      "source": [
        "df.shape[1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd3ebzx4U-Gd"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amcg4XkfHFUv"
      },
      "source": [
        "def elbow(X):\n",
        "  sse = []\n",
        "  silhouette = []\n",
        "  for i in range(2,20):\n",
        "    km  = KMeans(n_clusters = i , init = 'k-means++' , random_state =0 )\n",
        "    km.fit(X)\n",
        "    sse.append(km.inertia_)\n",
        "    sil = silhouette_score(X,km.labels_   )\n",
        "    silhouette.append(sil)\n",
        "  plt.figure(1)\n",
        "  plt.plot(range(2,20),sse , marker = 'o')\n",
        "  plt.xlabel('number of cluster')\n",
        "  plt.ylabel('SSE')\n",
        "  plt.figure(2)\n",
        "  plt.plot(range(2,20),silhouette , marker = 'x')\n",
        "  plt.xlabel('number of cluster')\n",
        "  plt.ylabel('silhouette')\n",
        "\n",
        "  plt.show\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlMrVFddQgLd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03IC7lmDWGgu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rVHnPjcTwsj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mUDf2ydT5DL"
      },
      "source": [
        "\n",
        "data_for_kmeans = df.values\n",
        "pca = PCA(n_components=7)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "data_for_kmeans\n",
        "\n",
        "\n",
        "elbow(data_for_kmeans)\n",
        "\n",
        "local_km = KMeans(n_clusters = 12 )\n",
        "local_km.fit(data_for_kmeans)\n",
        "\n",
        "label = local_km.labels_\n",
        "df['label_clustered'] = label.copy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOOurKXpmQHZ"
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(data_for_kmeans[:,0],data_for_kmeans[:,1],c = df['label_clustered'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxpvF47afWuH"
      },
      "source": [
        "\n",
        "service_df = df.copy()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agq_68POgYth"
      },
      "source": [
        "service_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXKl5vph82bs"
      },
      "source": [
        "\n",
        "coeff_service = pd.DataFrame(np.exp(b), columns= preprocessed_data_service.columns[:-1] )\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzKUIhON-Uxb"
      },
      "source": [
        "coeff_service"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dMfIvAh4rme"
      },
      "source": [
        "\n",
        "list = []\n",
        "coeff = []\n",
        "\n",
        "df = preprocessed_data_taste\n",
        "for i in range(1000):\n",
        "    data_ran = df[df['evaluation'].isin([1])]\n",
        "\n",
        "    data_ran = data_ran.sample(n=390, random_state = 10)\n",
        "\n",
        "    data_zero = df[df['evaluation'].isin([0])]\n",
        "\n",
        "    df  = pd.concat([data_ran, data_zero])\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    x_train, x_test, y_train, y_test = model_selection.train_test_split(df.iloc[:,0:-1].values, df.iloc[:,-1].values.astype(int), test_size=0.3)\n",
        "\n",
        "    mod = BernoulliNB(alpha=1, class_prior=None, fit_prior=True)\n",
        "    mod.fit(x_train, y_train)\n",
        "\n",
        "    predicted = mod.predict(x_test)\n",
        "\n",
        "    coeff.append( mod.feature_log_prob_ )\n",
        "\n",
        "\n",
        " \n",
        "    list.append(accuracy_score(y_test, predicted))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "list_np = np.array(list)\n",
        "a = np.mean(list_np)\n",
        "b = np.mean(coeff,axis = 0)\n",
        "\n",
        "print('score is',a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC9LFbSugmQZ"
      },
      "source": [
        "data_for_kmeans = df.values\n",
        "pca = PCA(n_components=7)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "data_for_kmeans\n",
        "\n",
        "\n",
        "elbow(data_for_kmeans)\n",
        "\n",
        "local_km = KMeans(n_clusters = 12)\n",
        "local_km.fit(data_for_kmeans)\n",
        "\n",
        "label = local_km.labels_\n",
        "df['label_clustered'] = label\n",
        "\n",
        "taste_df = df.copy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgiHyNyhiil2"
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(data_for_kmeans[:,0],data_for_kmeans[:,1],c = df['label_clustered'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVjDIWIdq6Wa"
      },
      "source": [
        "\n",
        "taste_df = df.copy()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JxKOeYS9ftu"
      },
      "source": [
        "\n",
        "coeff_taste = pd.DataFrame(np.exp(b), columns= preprocessed_data_taste.columns[:-1] )\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dLyAMIHB6Yw"
      },
      "source": [
        "coeff_taste"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdIQ40Ry4ruW"
      },
      "source": [
        "\n",
        "list = []\n",
        "coeff = []\n",
        "\n",
        "df = preprocessed_data_cost\n",
        "for i in range(1000):\n",
        "    data_ran = df[df['evaluation'].isin([1])]\n",
        "\n",
        "    data_ran = data_ran.sample(n=390, random_state = 10)\n",
        "\n",
        "    data_zero = df[df['evaluation'].isin([0])]\n",
        "\n",
        "    df  = pd.concat([data_ran, data_zero])\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    x_train, x_test, y_train, y_test = model_selection.train_test_split(df.iloc[:,0:-1].values, df.iloc[:,-1].values.astype(int), test_size=0.3)\n",
        "\n",
        "    mod = BernoulliNB(alpha=1, class_prior=None, fit_prior=True)\n",
        "    mod.fit(x_train, y_train)\n",
        "\n",
        "    predicted = mod.predict(x_test)\n",
        "\n",
        "    coeff.append( mod.feature_log_prob_ )\n",
        "\n",
        "\n",
        " \n",
        "    list.append(accuracy_score(y_test, predicted))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "list_np = np.array(list)\n",
        "a = np.mean(list_np)\n",
        "b = np.mean(coeff,axis = 0)\n",
        "\n",
        "print('score is',a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuX4Q9rfguvt"
      },
      "source": [
        "data_for_kmeans = df.values\n",
        "pca = PCA(n_components=7)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "data_for_kmeans\n",
        "\n",
        "\n",
        "elbow(data_for_kmeans)\n",
        "\n",
        "local_km = KMeans(n_clusters = 11 )\n",
        "local_km.fit(data_for_kmeans)\n",
        "\n",
        "label = local_km.labels_\n",
        "df['label_clustered'] = label\n",
        "\n",
        "cost_df = df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9UFeX39mYtB"
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(data_for_kmeans[:,0],data_for_kmeans[:,1],c = df['label_clustered'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di5ezpXYq_SB"
      },
      "source": [
        "cost_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lLAJ7oT9g7R"
      },
      "source": [
        "\n",
        "coeff_cost = pd.DataFrame(np.exp(b), columns= preprocessed_data_cost.columns[:-1] )\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdBmajXEB46Y"
      },
      "source": [
        "coeff_cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-ZbVpNG4r2i"
      },
      "source": [
        "\n",
        "list = []\n",
        "coeff = []\n",
        "\n",
        "df = preprocessed_data_atmosphere\n",
        "for i in range(1000):\n",
        "    data_ran = df[df['evaluation'].isin([1])]\n",
        "\n",
        "    data_ran = data_ran.sample(n=390, random_state = 10)\n",
        "\n",
        "    data_zero = df[df['evaluation'].isin([0])]\n",
        "\n",
        "    df  = pd.concat([data_ran, data_zero])\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    x_train, x_test, y_train, y_test = model_selection.train_test_split(df.iloc[:,0:-1].values, df.iloc[:,-1].values.astype(int), test_size=0.3)\n",
        "\n",
        "    mod = BernoulliNB(alpha=1, class_prior=None, fit_prior=True)\n",
        "    mod.fit(x_train, y_train)\n",
        "\n",
        "    predicted = mod.predict(x_test)\n",
        "\n",
        "    coeff.append( mod.feature_log_prob_ )\n",
        "\n",
        "\n",
        " \n",
        "    list.append(accuracy_score(y_test, predicted))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "list_np = np.array(list)\n",
        "a = np.mean(list_np)\n",
        "b = np.mean(coeff,axis = 0)\n",
        "\n",
        "print('score is',a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFilwSM1gwpV"
      },
      "source": [
        "data_for_kmeans = df.values\n",
        "pca = PCA(n_components=7)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "data_for_kmeans\n",
        "\n",
        "\n",
        "elbow(data_for_kmeans)\n",
        "\n",
        "local_km = KMeans(n_clusters = 12 )\n",
        "local_km.fit(data_for_kmeans)\n",
        "\n",
        "label = local_km.labels_\n",
        "df['label_clustered'] = label\n",
        "\n",
        "atmosphere_df = df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eNrDUoSrILk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-GC_Ao1man9"
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(data_for_kmeans[:,0],data_for_kmeans[:,1],c = df['label_clustered'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svw0_hJL9iJH"
      },
      "source": [
        "\n",
        "coeff_atmosphere = pd.DataFrame(np.exp(b), columns= preprocessed_data_atmosphere.columns[:-1] )\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yJTB5KnrXru"
      },
      "source": [
        "atmosphere_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9mjG9iGB2sd"
      },
      "source": [
        "coeff_atmosphere"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1qr2FmpwhEl"
      },
      "source": [
        "coeff_mean = np.mean(coeff,axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bLVGa1pwtSQ"
      },
      "source": [
        "coeff_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3NeRMGXxeJ2"
      },
      "source": [
        "df = preprocessed_data_service\n",
        "\n",
        "\n",
        "data_ran = df[df['evaluation'].isin([1])]\n",
        "\n",
        "data_ran = data_ran.sample(n=390, random_state = 10)\n",
        "\n",
        "data_zero = df[df['evaluation'].isin([0])]\n",
        "\n",
        "df  = pd.concat([data_ran, data_zero])\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(df.iloc[:,0:-1].values, df.iloc[:,-1].values.astype(int), test_size=0.3)\n",
        "\n",
        "mod = BernoulliNB(alpha=1, class_prior=None, fit_prior=True)\n",
        "mod.fit(x_train, y_train)\n",
        "\n",
        "predicted = mod.predict(x_test)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3AQihuuZel"
      },
      "source": [
        "service_df.to_csv('service_df.csv',index = False)\n",
        "taste_df.to_csv('taste_df.csv',index = False)\n",
        "cost_df.to_csv('cost_df.csv',index = False)\n",
        "atmosphere_df.to_csv('atmosphere_df.csv',index = False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2-y8apNOsI9"
      },
      "source": [
        "lst =service_df['label_clustered'].unique().tolist()\n",
        "lst = sorted(lst)\n",
        "\n",
        "for i in lst: \n",
        "\n",
        "  find = service_df[service_df['label_clustered']==i].iloc[:,:-2]\n",
        "  txt = find.sum().sort_values(ascending = False)[0:4]\n",
        "  \n",
        "  print(\"-------------------------\")\n",
        "\n",
        "  print(\"class  : {}\".format(i))\n",
        "\n",
        "  print(txt)\n",
        "\n",
        "  print(\"-------------------------\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pX-tQLYZE3e"
      },
      "source": [
        "lst =taste_df['label_clustered'].unique().tolist()\n",
        "lst = sorted(lst)\n",
        "\n",
        "for i in lst: \n",
        "\n",
        "  find = taste_df[taste_df['label_clustered']==i].iloc[:,:-2]\n",
        "  txt = find.sum().sort_values(ascending = False)[0:4]\n",
        "  \n",
        "  print(\"-------------------------\")\n",
        "\n",
        "  print(\"class  : {}\".format(i))\n",
        "\n",
        "  print(txt)\n",
        "\n",
        "  print(\"-------------------------\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmcfpnXtZ3_R"
      },
      "source": [
        "lst =cost_df['label_clustered'].unique().tolist()\n",
        "lst = sorted(lst)\n",
        "\n",
        "for i in lst: \n",
        "\n",
        "  find = cost_df[cost_df['label_clustered']==i].iloc[:,:-2]\n",
        "  txt = find.sum().sort_values(ascending = False)[0:4]\n",
        "  \n",
        "  print(\"-------------------------\")\n",
        "\n",
        "  print(\"class  : {}\".format(i))\n",
        "\n",
        "  print(txt)\n",
        "\n",
        "  print(\"-------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbSiJbJ8anut"
      },
      "source": [
        "lst =atmosphere_df['label_clustered'].unique().tolist()\n",
        "lst = sorted(lst)\n",
        "\n",
        "for i in lst: \n",
        "\n",
        "  find = atmosphere_df[atmosphere_df['label_clustered']==i].iloc[:,:-2]\n",
        "  txt = find.sum().sort_values(ascending = False)[0:4]\n",
        "  \n",
        "  print(\"-------------------------\")\n",
        "\n",
        "  print(\"class  : {}\".format(i))\n",
        "\n",
        "  print(txt)\n",
        "\n",
        "  print(\"-------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7taJE8rRTWd"
      },
      "source": [
        "find = service_df[service_df['label_clustered']==1].iloc[:,:-2]\n",
        "find"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOTml-TrQEe-"
      },
      "source": [
        "service_df.sum().sort_values(ascending = False)[2:6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaNHUIeLQzdG"
      },
      "source": [
        "ccc = '아 테스트 123123 keyword 상황 파악 어렵  '\n",
        "re.findall('keyword +[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s', ccc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAdvohBtqGHA"
      },
      "source": [
        "\n",
        "def turn_feature_df(dic):\n",
        "  dic2 = {}\n",
        "  for i in dic.keys():\n",
        "    for j in dic[i].keys():\n",
        "      dic2[i+\"_\"+j] = dic[i][j]\n",
        "\n",
        "  return dic2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-3SjL_LPzbt"
      },
      "source": [
        "def ngram2_count(review,dic):\n",
        "\n",
        "'''\n",
        "string = 'test test test'\n",
        "\n",
        "if 'test in string:\n",
        "\n",
        "print string\n",
        "\n",
        "\n",
        "\n",
        "출처: https://metagenomics.tistory.com/entry/여러번-찾기 [메타지노믹스]\n",
        "'''\n",
        "\n",
        "  for i in dic.keys():\n",
        "\n",
        "    a = re.findall(i +'+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s',review)\n",
        "   \n",
        "    b = re.findall('[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+'+i,review)\n",
        "      \n",
        "    if len(a)>0:\n",
        "      print(a,\"a 입니다.\")\n",
        "      splited_a = a[0].split()\n",
        "      print(splited_a)\n",
        "      try:\n",
        "        dic[i][splited_a[1]] += 1\n",
        "\n",
        "      except :\n",
        "\n",
        "        pass\n",
        "      try:\n",
        "        dic[i][splited_a[2]] += 1\n",
        "\n",
        "      except :\n",
        "        pass\n",
        "      try:\n",
        "        dic[i][splited_a[3]] += 1\n",
        "\n",
        "      except :\n",
        "        pass\n",
        "      \n",
        "      try:\n",
        "        dic[i][splited_a[4]] += 1\n",
        "\n",
        "      except :\n",
        "        pass\n",
        "          \n",
        "    elif len(b)>0:\n",
        "        \n",
        "        \n",
        "      print(b,'B 입니다.')\n",
        "      splited_b = b[0].split()\n",
        "      print(splited_b)\n",
        "      try:\n",
        "        dic[i][splited_b[0]] += 1\n",
        "\n",
        "      except :\n",
        "        pass\n",
        "      try:\n",
        "        dic[i][splited_b[1]] += 1\n",
        "      except :\n",
        "        pass\n",
        "      try:\n",
        "        dic[i][splited_b[2]] += 1\n",
        "\n",
        "      except :\n",
        "        pass\n",
        "      try:\n",
        "        dic[i][splited_b[3]] += 1\n",
        "\n",
        "      except :\n",
        "        pass\n",
        "        \n",
        "          \n",
        "  return dic\n",
        "\n",
        "      \n",
        "    \n",
        "    \n",
        "\n",
        "      \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80maAw4gWv2p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
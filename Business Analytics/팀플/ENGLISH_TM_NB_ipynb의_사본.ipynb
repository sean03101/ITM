{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ENGLISH_TM_NB.ipynb의 사본",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwr0218/analysis_reviews_restaurant_with_Korean_English/blob/main/ENGLISH_TM_NB_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYpSmlN9maGg"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt06YjbRlrJD"
      },
      "source": [
        "!pip install lda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6TvSS-mlv24"
      },
      "source": [
        "\n",
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9Vj3PxJlUgI"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import collections\n",
        "import itertools\n",
        "import lda\n",
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import math\n",
        "import operator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from pandas import read_table\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from konlpy  import tag \n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQdOtMHwmAiQ"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Tripadvisor_final (1).csv', encoding = 'utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdQilo85KtxA"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h9YtujcGSnw"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Xgy3HgmWYi"
      },
      "source": [
        "data\n",
        "\n",
        "data['evaluation'] = data.apply(lambda x : 0 if x['evaluation'] <25  else x['evaluation'],axis = 1)\n",
        "\n",
        "data['evaluation'] = data.apply(lambda x : 1 if x['evaluation'] >= 40 else x['evaluation'],axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data = data.dropna(subset=['evaluation'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbCvUiFEGew5"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uNhS8rwm1rw"
      },
      "source": [
        "total_review = data['review']\n",
        "positive_review =  data[data['evaluation']==1]['review']\n",
        "\n",
        "\n",
        "negative_review = data[data['evaluation']==0]['review']\n",
        "\n",
        "\n",
        "tot_review = total_review.values.tolist()\n",
        "pos_review = positive_review.values.tolist()\n",
        "neg_review = negative_review.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGJBzic5KWMt"
      },
      "source": [
        "total_review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP6NHNB_nab4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZe-q4hf7qzg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uniiv_WfJ_u3"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSMBpJXC2zjn"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "\n",
        "class SentenceTokenizer(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.n = WordNetLemmatizer\n",
        "\n",
        "        self.s = PorterStemmer()\n",
        "        self.stopword = ['the','and', 'to', 'of', 'in', 'it', 'for', 'with', 'is', 'wa', 'that', 'but', 'on', 'my', 'this', 'you', 'at', 'not', 'are', 'have', 'food', 'restaurant', 'they',\n",
        " 'so', 'from', 'be', 'place', 'good', 'we', 'had', 'were' 'seoul', 'if','all','which','or','there','like','just','were','the','it','very','about','some','do','we','when','있는','would','menu'\n",
        " ,'make','your','너무','other','here','than','no','we','out','by','can','an','정말','korean'\n",
        " ,'will','다른','korean','서울시','맛은','합니다','강남구','their','위치한','the','하지만','any','after'\n",
        " ,'this','there','they']\n",
        "        \n",
        "    \n",
        "    def text2sentences(self, text):\n",
        "        sentences = self.kkma.sentences(text)\n",
        "        for idx in range(0, len(sentences)):\n",
        "            if len(sentences[idx]) <= 10:\n",
        "                sentences[idx-1] += (' ' + sentences[idx])\n",
        "                sentences[idx] = ''\n",
        "        return sentences\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    def get_find_stem(self, sentences):\n",
        "        adj = []\n",
        "        stopword_1 = ['the','and', 'to', 'of', 'in', 'it', 'for', 'with', 'is', 'wa', 'that', 'but', 'on', 'my', 'this', 'you', 'at', 'not', 'are', 'have', 'food', 'restaurant', 'they',\n",
        " 'so', 'from', 'be', 'place', 'good', 'we', 'had', 'were' 'seoul', 'if','all','which','or','there','like','just','were','the','it','very','about','some','do','we','when','있는','would','menu'\n",
        " ,'make','your','너무','other','here','than','no','we','out','by','can','an','정말','korean'\n",
        " ,'will','다른','korean','서울시','맛은','합니다','강남구','their','위치한','the','하지만','any','after'\n",
        " ,'this','there','they'\n",
        " ]\n",
        "        count = dict()\n",
        "        find = []\n",
        "        for sentence in sentences:\n",
        "            \n",
        "            if sentence is not '':\n",
        "                \n",
        "                \n",
        "                find = [self.s.stem(a) for a in word_tokenize(sentence)\n",
        "                                           if  self.s.stem( a ) not in stopword_1 ]\n",
        "                find = [w.lower() for w in find if not w in stopwords.words('english')]\n",
        "                adj.append(' '.join(find))\n",
        "                \n",
        "                \n",
        "                for i in find:\n",
        "                    try:\n",
        "                        count[i] = count[i] + 1\n",
        "                    except :\n",
        "                        count[i] = 1       \n",
        "        print(count)\n",
        "        return adj ,count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf_tWiwnfgaK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAk8a5h9nkVT"
      },
      "source": [
        "tokenizer = SentenceTokenizer()\n",
        "\n",
        "\n",
        "total_tokenized , total_count = tokenizer.get_find_stem(total_review)\n",
        "\n",
        "positive_tokenized ,positive_count = tokenizer.get_find_stem(positive_review)\n",
        "\n",
        "negative_tokenized, negative_count = tokenizer.get_find_stem(negative_review)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_HrIk0H7CM4"
      },
      "source": [
        "total_tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDEO798xD9dZ"
      },
      "source": [
        "service_list = ['servic', 'attit', 'staff', 'waiter', 'serv', 'english']\n",
        "taste_list = ['dish','pizza','pasta','food','meal','jungsik','bottarga','drink','taste','dessertch','grill','appetis','plate','soup','lunch','crab','stale','fish','tast'\n",
        "'dinner','set','bibimbap','gimbap','lobster','seafood','bbq','branch']\n",
        "cost_list = []\n",
        "\n",
        "atmosphere_list = [\"music\", \"atmosphere\", \"conversation\", \"deco\", \"girlfriend\", \"musician\", \"jazz\", \"piano\", \"vocalist\", \"family\",'transport']\n",
        "\n",
        "\n",
        "service_good_feature = {'better':0,'excellent':0,'kind':0,'authentic':0,'consistent':0,'spectacular':0,'excellent':0,'outstanding':0,'great':0,'impeccable':0,'fine':0,'nice':0,'impressive':0,'perfect':0,'good':0,'incredible':0,'best':0,'outstanding':0,'timely':0,'friendly':0,\n",
        "                        'awesome':0,'attent':0 ,'profes':0, 'understan':0, 'pleasant':0, 'on point':0, 'superb':0, 'efficient':0,'thoughtful':0, 'fantastic':0, 'lovely':0, 'spot on':0,'help':0, 'prompt':0, 'enjoyed':0, 'top':0, 'wonderful':0, 'detail':0, 'flawless':0, 'quick':0, 'courteous':0,'fast':0,'fluent':0, 'ease':0, 'speak':0, 'thoughtful':0}\n",
        "\n",
        "\n",
        "service_bad_feature = {'let down':0,'flaw':0,'notatten':0,'cold':0,'overat':0,'mediocre':0,'quic':0,'ordi':0,'passable':0,'no help':0,'unimpre':0,'shortcom':0,'slow':0,'terrible':0,'worst':0,'rude':0,'unfriendly':0,'unacceptable':0,'bad':0,'poor':0,'horrible':0,'lifeless':0,\n",
        "                        'brusque':0,'notgood':0,'average':0, 'hard communicate':0, 'limit':0, 'notfluent':0}\n",
        "# 서비스 좋음 / 안좋음 \n",
        "#맛\n",
        "\n",
        "\n",
        "taste_good_feature = { 'delici':0, 'amaz':0,'spectacular':0,'super':0,'love':0,'perfectli':0,'perfect':0,'better':0,'good':0,'wonderful':0,'comfort':0,'crispi':0,\n",
        "                      'great':0,'fast':0,'popular':0,'cool':0}\n",
        "\n",
        "taste_bad_feature = {'overpriced':0,'awful':0,'wrong':0,'overprice':0,'terribl':0,'overr':0,'overcook':0,'disappointed':0,'unrefined':0 ,'regrect':0,'cold':0,'unfortun':0,\n",
        "                     'poor':0, 'worst' : 0 , 'empti':0, 'tiny':0,'horrible':0, 'expens':0,'miss':0,'dri':0, 'bad':0, 'notgood':0\n",
        "                     }\n",
        "#느낌 입감 \n",
        "cost_good_feature = {'괜찮':0,'착하다':0,'저렴':0,'적당':0,'싸다':0,'좋다':0,'합리적':0,'훌륭':0,'최고':0,'만족':0,'마음':0,'든든':0,'알맞다':0,'무난':0,'괜춘':0,'최상':0,'낮':0\n",
        "                     ,'많':0,'적당':0,'푸짐':0,'괜찮다':0,'넉넉':0,'충분':0    \n",
        "                     }\n",
        "\n",
        "cost_bad_feature ={'비싸':0,'있다':0,'나쁘':0,'사악':0,'비효율':0,'높다':0,'부담':0,'아쉽':0,'별로':0,'그닥':0,'그다지':0,'쎄':0,'높':0,'거품':0,'눈물':0,\n",
        "                   '부족':0, '별로':0,'적다':0,'작다':0,'아쉽':0,'적다':0,'다소':0,'별로':0  \n",
        "                   }\n",
        "# 특징 \n",
        "\n",
        "atmosphere_good_feature ={'wonder':0,'luv':0, 'amaz':0, 'great':0,'awesome':0, 'outstand':0,  'relaxed':0, 'calm':0, 'cozy':0, 'fancy':0, 'funny':0, \n",
        "                          'ok':0, 'pleasant':0, 'relax':0,'excellent':0,'cool':0, 'perfect':0,  'couple':0, 'roman':0, \n",
        "                          'cute':0,'comfort':0,'enjoy':0,'fine':0, 'propos':0,  'quiet':0,  'classy':0, 'snooty':0 , 'private':0, 'trendy':0, 'celebrating':0, 'appriciated':0,\n",
        "                          'peaceful':0, 'tranquil':0, 'brilliant':0, 'warm':0, 'basic':0, 'happy':0, 'stylish':0}\n",
        "\n",
        "\n",
        "atmosphere_bad_feature ={'sadly':0, 'disappoined':0, 'Not':0, 'stuffy':0, 'wide':0, 'rare':0,'regret':0,'wrong':0}\n",
        "#분위기\n",
        "#transport poor / manag aploget\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsNUwnDt3jyi"
      },
      "source": [
        "def keyword_count(review,dic):\n",
        "  \n",
        "  for i in dic.keys():\n",
        "\n",
        "\n",
        "   \n",
        "    \n",
        "      \n",
        "    if i in review:\n",
        "\n",
        "\n",
        "      dic[i] +=1\n",
        "  return dic \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDd7WyPWl7Zy"
      },
      "source": [
        "def condition_keyword_count(review,dic,lst):\n",
        "\n",
        "\n",
        "  find = word_tokenize(review)\n",
        "  \n",
        "\n",
        "  for i in find :\n",
        "    if i in lst:\n",
        "      dic =keyword_count(review,dic)\n",
        "  return dic       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKVrqEn-97vK"
      },
      "source": [
        "keyword_count(total_tokenized[0],service_good_feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGBi1HMzq76L"
      },
      "source": [
        "preprocessed_data_service = pd.DataFrame()\n",
        "preprocessed_data_taste = pd.DataFrame()\n",
        "preprocessed_data_cost = pd.DataFrame()\n",
        "preprocessed_data_atmosphere = pd.DataFrame()\n",
        "\n",
        "for token in total_tokenized:\n",
        "  \n",
        "  \n",
        "  service_good_feature = {'better':0,'excellent':0,'kind':0,'authentic':0,'consistent':0,'spectacular':0,'excellent':0,'outstanding':0,'great':0,'impeccable':0,'fine':0,'nice':0,'impressive':0,'perfect':0,'good':0,'incredible':0,'best':0,'outstanding':0,'timely':0,'friendly':0,\n",
        "                          'awesome':0,'attent':0 ,'profes':0, 'understan':0, 'pleasant':0, 'on point':0, 'superb':0, 'efficient':0,'thoughtful':0, 'fantastic':0, 'lovely':0, 'spot on':0,'help':0, 'prompt':0, 'enjoyed':0, 'top':0, 'wonderful':0, 'detail':0, 'flawless':0, 'quick':0, 'courteous':0,'fast':0,'fluent':0, 'ease':0, 'speak':0, 'thoughtful':0}\n",
        "\n",
        "\n",
        "  service_bad_feature = {'let down':0,'flaw':0,'notatten':0,'cold':0,'overat':0,'mediocre':0,'quic':0,'ordi':0,'passable':0,'no help':0,'unimpre':0,'shortcom':0,'slow':0,'terrible':0,'worst':0,'rude':0,'unfriendly':0,'unacceptable':0,'bad':0,'poor':0,'horrible':0,'lifeless':0,\n",
        "                          'brusque':0,'notgood':0,'average':0, 'hard communicate':0, 'limit':0, 'notfluent':0}\n",
        "  # 서비스 좋음 / 안좋음 \n",
        "  #맛\n",
        "\n",
        "\n",
        "  taste_good_feature = { 'delici':0, 'amaz':0,'spectacular':0,'super':0,'love':0,'perfectli':0,'perfect':0,'better':0,'good':0,'wonderful':0,'comfort':0,'crispi':0,\n",
        "                        'great':0,'fast':0,'popular':0,'cool':0}\n",
        "\n",
        "  taste_bad_feature = {'overpriced':0,'awful':0,'wrong':0,'overprice':0,'terribl':0,'overr':0,'overcook':0,'disappointed':0,'unrefined':0 ,'regrect':0,'cold':0,'unfortun':0,\n",
        "                      'poor':0, 'worst' : 0 , 'empti':0, 'tiny':0,'horrible':0, 'expens':0,'miss':0,'dri':0, 'bad':0, 'notgood':0\n",
        "                      }\n",
        "  #느낌 입감 \n",
        "  cost_good_feature = {'nice':0, 'better':0, 'good':0, 'ok':0, 'recommended':0, 'reasonable':0, 'cheap':0, 'half':0, 'affordable':0, 'friendly':0, 'worth':0, 'important':0, 'right':0,\n",
        "                      'excellent':0, 'well':0, 'little':0, 'standard':0, 'extra':0, 'enjoy':0, 'fairly':0, 'amazing':0, 'satisfied':0\n",
        "                      }\n",
        "\n",
        "  cost_bad_feature ={ 'double':0, 'overpriced':0, 'overcharge':0, 'slightly':0, 'expensive':0, 'higher':0, 'pricecy':0, 'ridiculous':0, 'unacceptable':0, 'usual':0, 'steep':0, 'exorbitant':0,\n",
        "                     'outrageous':0, 'surprised':0\n",
        "                    }\n",
        "  # 특징 \n",
        "\n",
        "  atmosphere_good_feature ={'wonder':0,'luv':0, 'amaz':0, 'great':0,'awesome':0, 'outstand':0,  'relaxed':0, 'calm':0, 'cozy':0, 'fancy':0, 'funny':0, \n",
        "                            'ok':0, 'pleasant':0, 'relax':0,'excellent':0,'cool':0, 'perfect':0,  'couple':0, 'roman':0, \n",
        "                            'cute':0,'comfort':0,'enjoy':0,'fine':0, 'propos':0,  'quiet':0,  'classy':0, 'snooty':0 , 'private':0, 'trendy':0, 'celebrating':0, 'appriciated':0,\n",
        "                            'peaceful':0, 'tranquil':0, 'brilliant':0, 'warm':0, 'basic':0, 'happy':0, 'stylish':0}\n",
        "\n",
        "\n",
        "  atmosphere_bad_feature ={'sadly':0, 'disappoined':0, 'Not':0, 'stuffy':0, 'wide':0, 'rare':0,'regret':0,'wrong':0, 'noisy':0}\n",
        "#분위기\n",
        "#transport poor / manag aploget\n",
        "  #분위기\n",
        "  service_good_feature = condition_keyword_count(token,service_good_feature,service_list)\n",
        "  service_bad_feature = condition_keyword_count(token,service_bad_feature,service_list)\n",
        "  service_good_feature_df = pd.DataFrame(service_good_feature,index = [0])\n",
        "  service_bad_feature_df = pd.DataFrame(service_bad_feature,index = [0])\n",
        "  \n",
        "  preprocessed_data_service_concated = pd.concat([service_good_feature_df,service_bad_feature_df],axis = 1)\n",
        "\n",
        "\n",
        "  taste_good_feature = condition_keyword_count(token,taste_good_feature,taste_list)\n",
        "  taste_bad_feature = condition_keyword_count(token,taste_bad_feature,taste_list)\n",
        "  taste_good_feature_df = pd.DataFrame(taste_good_feature,index = [0])\n",
        "  taste_bad_feature_df = pd.DataFrame(taste_bad_feature,index = [0])\n",
        "  \n",
        "  \n",
        "  preprocessed_data_taste_concated = pd.concat([taste_good_feature_df,taste_bad_feature_df],axis = 1)\n",
        "\n",
        "  \n",
        "  \n",
        "  cost_good_feature = condition_keyword_count(token,cost_good_feature ,cost_list)\n",
        "  cost_bad_feature = condition_keyword_count(token,cost_bad_feature ,cost_list)\n",
        "  cost_good_feature_df = pd.DataFrame(cost_good_feature,index = [0])\n",
        "  cost_bad_feature_df = pd.DataFrame(cost_bad_feature,index = [0])\n",
        "  \n",
        "  preprocessed_data_cost_concated = pd.concat([cost_good_feature_df,cost_bad_feature_df],axis = 1)\n",
        "\n",
        "  \n",
        "  atmosphere_good_feature = condition_keyword_count(token,atmosphere_good_feature,atmosphere_list)\n",
        "  atmosphere_bad_feature = condition_keyword_count(token,atmosphere_bad_feature,atmosphere_list)\n",
        "  atmosphere_good_feature_df = pd.DataFrame(atmosphere_good_feature,index = [0])\n",
        "  atmosphere_bad_feature_df = pd.DataFrame(atmosphere_bad_feature,index = [0])\n",
        "\n",
        "  preprocessed_data_atmosphere_concated = pd.concat([atmosphere_good_feature_df,atmosphere_bad_feature_df],axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "  preprocessed_data_service = pd.concat([preprocessed_data_service,preprocessed_data_service_concated],ignore_index=True )\n",
        "  preprocessed_data_taste = pd.concat([preprocessed_data_taste,preprocessed_data_taste_concated],ignore_index=True )\n",
        "  preprocessed_data_cost = pd.concat([preprocessed_data_cost,preprocessed_data_cost_concated],ignore_index=True )\n",
        "  preprocessed_data_atmosphere = pd.concat([preprocessed_data_atmosphere,preprocessed_data_atmosphere_concated],ignore_index=True )\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC7b5l8atr8t"
      },
      "source": [
        "preprocessed_data_service"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJlGX8VQmiqR"
      },
      "source": [
        "preprocessed_data_service['evaluation'] = data['evaluation'].copy()\n",
        "preprocessed_data_service = preprocessed_data_service[preprocessed_data_service['evaluation']!='괜찮다']\n",
        "\n",
        "\n",
        "preprocessed_data_taste['evaluation'] = data['evaluation'].copy()\n",
        "\n",
        "preprocessed_data_taste = preprocessed_data_taste[preprocessed_data_taste['evaluation']!='괜찮다']\n",
        "\n",
        "preprocessed_data_cost['evaluation'] = data['evaluation'].copy()\n",
        "\n",
        "preprocessed_data_cost = preprocessed_data_cost[preprocessed_data_cost['evaluation']!='괜찮다']\n",
        "preprocessed_data_atmosphere['evaluation'] = data['evaluation'].copy()\n",
        "\n",
        "preprocessed_data_atmosphere = preprocessed_data_atmosphere[preprocessed_data_atmosphere['evaluation']!='괜찮다']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS7eOG8anpKI"
      },
      "source": [
        "preprocessed_data_atmosphere.iloc[:,:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjFB5Gds6z9s"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import silhouette_score\n",
        "def elbow(X):\n",
        "  sse = []\n",
        "  silhouette = []\n",
        "  for i in range(2,20):\n",
        "    km  = KMeans(n_clusters = i , init = 'k-means++' , random_state =0 )\n",
        "    km.fit(X)\n",
        "    sse.append(km.inertia_)\n",
        "    sil = silhouette_score(X,km.labels_   )\n",
        "    silhouette.append(sil)\n",
        "  plt.figure(1)\n",
        "  plt.plot(range(2,20),sse , marker = 'o')\n",
        "  plt.xlabel('number of cluster')\n",
        "  plt.ylabel('SSE')\n",
        "  plt.figure(2)\n",
        "  plt.plot(range(2,20),silhouette , marker = 'x')\n",
        "  plt.xlabel('number of cluster')\n",
        "  plt.ylabel('silhouette')\n",
        "\n",
        "  plt.show\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XkBOV9oQTSq"
      },
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.under_sampling import NearMiss\n",
        "import pandas as pd\n",
        "\n",
        "import math \n",
        "\n",
        "# 정확도 계산을 위한 함수\n",
        "\n",
        "list = []\n",
        "coeff = []\n",
        "\n",
        "df = preprocessed_data_service\n",
        "for i in range(1000):\n",
        "    data_ran = df[df['evaluation'].isin([1])]\n",
        "\n",
        "    data_ran = data_ran.sample(n=200, random_state = 10)\n",
        "\n",
        "    data_zero = df[df['evaluation'].isin([0])]\n",
        "\n",
        "    df  = pd.concat([data_ran, data_zero])\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    x_train, x_test, y_train, y_test = model_selection.train_test_split(df.iloc[:,0:-1].values, df.iloc[:,-1].values.astype(int), test_size=0.3)\n",
        "\n",
        "    mod = BernoulliNB(alpha=1, class_prior=None, fit_prior=True)\n",
        "    mod.fit(x_train, y_train)\n",
        "\n",
        "    predicted = mod.predict(x_test)\n",
        "\n",
        "    coeff.append( mod.feature_log_prob_ )\n",
        "\n",
        "\n",
        " \n",
        "    list.append(accuracy_score(y_test, predicted))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "list_np = np.array(list)\n",
        "a = np.mean(list_np)\n",
        "b = np.mean(coeff,axis = 0)\n",
        "\n",
        "print('score is',a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGv37wXy7Bb3"
      },
      "source": [
        "\n",
        "data_for_kmeans = df.values\n",
        "pca = PCA(n_components=7)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "data_for_kmeans\n",
        "\n",
        "\n",
        "elbow(data_for_kmeans)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XCvhmZq7LQd"
      },
      "source": [
        "\n",
        "local_km = KMeans(n_clusters = 6 )\n",
        "local_km.fit(data_for_kmeans)\n",
        "\n",
        "label = local_km.labels_\n",
        "df['label_clustered'] = label.copy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhFUQHFP75nH"
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(data_for_kmeans[:,0],data_for_kmeans[:,1],c = df['label_clustered'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXKl5vph82bs"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "coeff_service = pd.DataFrame(np.exp(b), columns= preprocessed_data_service.columns[:-1] )\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzKUIhON-Uxb"
      },
      "source": [
        "coeff_service"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlWq6eZ__G0x"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "a = coeff_service.iloc[0].to_dict()\n",
        "b = coeff_service.iloc[1].to_dict()\n",
        "\n",
        "word_cloud_dict1 = collections.Counter(a)\n",
        "wc1 = WordCloud(max_font_size = 200, background_color=\"White\", width=800, height=800).generate_from_frequencies(word_cloud_dict1)\n",
        "\n",
        "word_cloud_dict2 = collections.Counter(b)\n",
        "wc2 = WordCloud(max_font_size = 200, background_color=\"White\", width=800, height=800).generate_from_frequencies(word_cloud_dict2)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "#plt.imshow(wc1)\n",
        "plt.imshow(wc2)\n",
        "plt.axis('off')\n",
        "plt.title('Service good')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dMfIvAh4rme"
      },
      "source": [
        "\n",
        "list = []\n",
        "coeff = []\n",
        "\n",
        "df = preprocessed_data_taste\n",
        "for i in range(1000):\n",
        "    data_ran = df[df['evaluation'].isin([1])]\n",
        "\n",
        "    data_ran = data_ran.sample(n=200, random_state = 10)\n",
        "\n",
        "    data_zero = df[df['evaluation'].isin([0])]\n",
        "\n",
        "    df  = pd.concat([data_ran, data_zero])\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    x_train, x_test, y_train, y_test = model_selection.train_test_split(df.iloc[:,0:-1].values, df.iloc[:,-1].values.astype(int), test_size=0.3)\n",
        "\n",
        "    mod = BernoulliNB(alpha=1, class_prior=None, fit_prior=True)\n",
        "    mod.fit(x_train, y_train)\n",
        "\n",
        "    predicted = mod.predict(x_test)\n",
        "\n",
        "    coeff.append( mod.feature_log_prob_ )\n",
        "\n",
        "\n",
        " \n",
        "    list.append(accuracy_score(y_test, predicted))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "list_np = np.array(list)\n",
        "a = np.mean(list_np)\n",
        "b = np.mean(coeff,axis = 0)\n",
        "\n",
        "print('score is',a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTmjC-WA8SyA"
      },
      "source": [
        "\n",
        "data_for_kmeans = df.values\n",
        "pca = PCA(n_components=7)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "data_for_kmeans\n",
        "\n",
        "\n",
        "elbow(data_for_kmeans)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GH4Zlaq8Vuf"
      },
      "source": [
        "\n",
        "local_km = KMeans(n_clusters = 6 )\n",
        "local_km.fit(data_for_kmeans)\n",
        "\n",
        "label = local_km.labels_\n",
        "df['label_clustered'] = label.copy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FJr-uSi8aXC"
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(data_for_kmeans[:,0],data_for_kmeans[:,1],c = df['label_clustered'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JxKOeYS9ftu"
      },
      "source": [
        "\n",
        "coeff_taste = pd.DataFrame(np.exp(b), columns= preprocessed_data_taste.columns[:-1] )\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dLyAMIHB6Yw"
      },
      "source": [
        "coeff_taste"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK9WePbdyAna"
      },
      "source": [
        "a = coeff_taste.iloc[0].to_dict()\n",
        "b = coeff_taste.iloc[1].to_dict()\n",
        "\n",
        "word_cloud_dict1 = collections.Counter(a)\n",
        "wc1 = WordCloud(max_font_size = 200, background_color=\"White\", width=800, height=800).generate_from_frequencies(word_cloud_dict1)\n",
        "\n",
        "word_cloud_dict2 = collections.Counter(b)\n",
        "wc2 = WordCloud(max_font_size = 200, background_color=\"White\", width=800, height=800).generate_from_frequencies(word_cloud_dict2)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "#plt.imshow(wc1)\n",
        "plt.imshow(wc2)\n",
        "plt.axis('off')\n",
        "plt.title('Taste good')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdIQ40Ry4ruW"
      },
      "source": [
        "\n",
        "list = []\n",
        "coeff = []\n",
        "\n",
        "df = preprocessed_data_cost\n",
        "for i in range(1000):\n",
        "    data_ran = df[df['evaluation'].isin([1])]\n",
        "\n",
        "    data_ran = data_ran.sample(n=200, random_state = 10)\n",
        "\n",
        "    data_zero = df[df['evaluation'].isin([0])]\n",
        "\n",
        "    df  = pd.concat([data_ran, data_zero])\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    x_train, x_test, y_train, y_test = model_selection.train_test_split(df.iloc[:,0:-1].values, df.iloc[:,-1].values.astype(int), test_size=0.3)\n",
        "\n",
        "    mod = BernoulliNB(alpha=1, class_prior=None, fit_prior=True)\n",
        "    mod.fit(x_train, y_train)\n",
        "\n",
        "    predicted = mod.predict(x_test)\n",
        "\n",
        "    coeff.append( mod.feature_log_prob_ )\n",
        "\n",
        "\n",
        " \n",
        "    list.append(accuracy_score(y_test, predicted))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "list_np = np.array(list)\n",
        "a = np.mean(list_np)\n",
        "b = np.mean(coeff,axis = 0)\n",
        "\n",
        "print('score is',a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnmBV9x49VFz"
      },
      "source": [
        "\n",
        "data_for_kmeans = df.values\n",
        "pca = PCA(n_components=7)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "data_for_kmeans\n",
        "\n",
        "\n",
        "elbow(data_for_kmeans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9I6LQft9Xbt"
      },
      "source": [
        "\n",
        "local_km = KMeans(n_clusters = 6 )\n",
        "local_km.fit(data_for_kmeans)\n",
        "\n",
        "label = local_km.labels_\n",
        "df['label_clustered'] = label.copy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIThj2ql9hyU"
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(data_for_kmeans[:,0],data_for_kmeans[:,1],c = df['label_clustered'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lLAJ7oT9g7R"
      },
      "source": [
        "\n",
        "coeff_cost = pd.DataFrame(np.exp(b), columns= preprocessed_data_cost.columns[:-1] )\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdBmajXEB46Y"
      },
      "source": [
        "coeff_cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7wg7gU_zpQ5"
      },
      "source": [
        "a = coeff_cost.iloc[0].to_dict()\n",
        "b = coeff_cost.iloc[1].to_dict()\n",
        "\n",
        "word_cloud_dict1 = collections.Counter(a)\n",
        "wc1 = WordCloud(max_font_size = 200, background_color=\"White\", width=800, height=800).generate_from_frequencies(word_cloud_dict1)\n",
        "\n",
        "word_cloud_dict2 = collections.Counter(b)\n",
        "wc2 = WordCloud(max_font_size = 200, background_color=\"White\", width=800, height=800).generate_from_frequencies(word_cloud_dict2)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "#plt.imshow(wc1)\n",
        "plt.imshow(wc2)\n",
        "plt.axis('off')\n",
        "plt.title('Cost good')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-ZbVpNG4r2i"
      },
      "source": [
        "\n",
        "list = []\n",
        "coeff = []\n",
        "\n",
        "df = preprocessed_data_atmosphere\n",
        "for i in range(1000):\n",
        "    data_ran = df[df['evaluation'].isin([1])]\n",
        "\n",
        "    data_ran = data_ran.sample(n=200, random_state = 10)\n",
        "\n",
        "    data_zero = df[df['evaluation'].isin([0])]\n",
        "\n",
        "    df  = pd.concat([data_ran, data_zero])\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    x_train, x_test, y_train, y_test = model_selection.train_test_split(df.iloc[:,0:-1].values, df.iloc[:,-1].values.astype(int), test_size=0.3)\n",
        "\n",
        "    mod = BernoulliNB(alpha=1, class_prior=None, fit_prior=True)\n",
        "    mod.fit(x_train, y_train)\n",
        "\n",
        "    predicted = mod.predict(x_test)\n",
        "\n",
        "    coeff.append( mod.feature_log_prob_ )\n",
        "\n",
        "\n",
        " \n",
        "    list.append(accuracy_score(y_test, predicted))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "list_np = np.array(list)\n",
        "a = np.mean(list_np)\n",
        "b = np.mean(coeff,axis = 0)\n",
        "\n",
        "print('score is',a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4aa9mpP9sKk"
      },
      "source": [
        "\n",
        "data_for_kmeans = df.values\n",
        "pca = PCA(n_components=7)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "data_for_kmeans\n",
        "\n",
        "\n",
        "elbow(data_for_kmeans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpqvyGGy9_p_"
      },
      "source": [
        "\n",
        "local_km = KMeans(n_clusters = 7 )\n",
        "local_km.fit(data_for_kmeans)\n",
        "\n",
        "label = local_km.labels_\n",
        "df['label_clustered'] = label.copy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eChGpyVz9x_h"
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "\n",
        "data_for_kmeans = pca.fit_transform(data_for_kmeans)\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(data_for_kmeans[:,0],data_for_kmeans[:,1],c = df['label_clustered'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMT4_RXs9v-G"
      },
      "source": [
        "\n",
        "local_km = KMeans(n_clusters = 6 )\n",
        "local_km.fit(data_for_kmeans)\n",
        "\n",
        "label = local_km.labels_\n",
        "df['label_clustered'] = label.copy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svw0_hJL9iJH"
      },
      "source": [
        "\n",
        "coeff_atmosphere = pd.DataFrame(np.exp(b), columns= preprocessed_data_atmosphere.columns[:-1] )\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9mjG9iGB2sd"
      },
      "source": [
        "coeff_atmosphere"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azJlUN67z5Jz"
      },
      "source": [
        "a = coeff_atmosphere.iloc[0].to_dict()\n",
        "b = coeff_atmosphere.iloc[1].to_dict()\n",
        "\n",
        "word_cloud_dict1 = collections.Counter(a)\n",
        "wc1 = WordCloud(max_font_size = 200, background_color=\"White\", width=800, height=800).generate_from_frequencies(word_cloud_dict1)\n",
        "\n",
        "word_cloud_dict2 = collections.Counter(b)\n",
        "wc2 = WordCloud(max_font_size = 200, background_color=\"White\", width=800, height=800).generate_from_frequencies(word_cloud_dict2)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "#plt.imshow(wc1)\n",
        "plt.imshow(wc2)\n",
        "plt.axis('off')\n",
        "plt.title('atmostphere bad')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1qr2FmpwhEl"
      },
      "source": [
        "coeff_mean = np.mean(coeff,axis = 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOPU8whCbd6Y"
      },
      "source": [
        "type(coeff_mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bLVGa1pwtSQ"
      },
      "source": [
        "\n",
        "coeff_mean = pd.DataFrame(np.exp(coeff_mean), columns= preprocessed_data_atmosphere.columns[:-1] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDCqRBpxcpB1"
      },
      "source": [
        "coeff_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3NeRMGXxeJ2"
      },
      "source": [
        "df = preprocessed_data_service\n",
        "\n",
        "\n",
        "data_ran = df[df['evaluation'].isin([1])]\n",
        "\n",
        "data_ran = data_ran.sample(n=390, random_state = 10)\n",
        "\n",
        "data_zero = df[df['evaluation'].isin([0])]\n",
        "\n",
        "df  = pd.concat([data_ran, data_zero])\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(df.iloc[:,0:-1].values, df.iloc[:,-1].values.astype(int), test_size=0.3)\n",
        "\n",
        "mod = BernoulliNB(alpha=1, class_prior=None, fit_prior=True)\n",
        "mod.fit(x_train, y_train)\n",
        "\n",
        "predicted = mod.predict(x_test)\n",
        "mod.score(y_test,predicted)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3AQihuuZel"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AzlXtsf0x_r"
      },
      "source": [
        "mod.feature_log_prob_\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaNHUIeLQzdG"
      },
      "source": [
        "ccc = '아 테스트 123123 keyword 상황 파악 어렵  '\n",
        "re.findall('keyword +[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s', ccc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAdvohBtqGHA"
      },
      "source": [
        "\n",
        "def turn_feature_df(dic):\n",
        "  dic2 = {}\n",
        "  for i in dic.keys():\n",
        "    for j in dic[i].keys():\n",
        "      dic2[i+\"_\"+j] = dic[i][j]\n",
        "\n",
        "  return dic2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmvQn3l7qHsR"
      },
      "source": [
        "from google.colab import files\n",
        "preprocessed_data.to_csv('data_for_training.csv')\n",
        "#taste_good_emotion\n",
        "files.download('data_for_training.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugznwFUDqJs9"
      },
      "source": [
        "preprocessed_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "melZ--wMrrqn"
      },
      "source": [
        "taste_good_emotion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF5GtmNMBpNp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-3SjL_LPzbt"
      },
      "source": [
        "def ngram2_count(review,dic):\n",
        "\n",
        "\n",
        "\n",
        "  for i in dic.keys():\n",
        "\n",
        "    a = re.findall(i +'+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s',review)\n",
        "   \n",
        "    b = re.findall('[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+\\s+'+i,review)\n",
        "      \n",
        "    if len(a)>0:\n",
        "      print(a,\"a 입니다.\")\n",
        "      splited_a = a[0].split()\n",
        "      print(splited_a)\n",
        "      try:\n",
        "        dic[i][splited_a[1]] += 1\n",
        "\n",
        "      except :\n",
        "\n",
        "        pass\n",
        "      try:\n",
        "        dic[i][splited_a[2]] += 1\n",
        "\n",
        "      except :\n",
        "        pass\n",
        "      try:\n",
        "        dic[i][splited_a[3]] += 1\n",
        "\n",
        "      except :\n",
        "        pass\n",
        "      \n",
        "      try:\n",
        "        dic[i][splited_a[4]] += 1\n",
        "\n",
        "      except :\n",
        "        pass\n",
        "          \n",
        "    elif len(b)>0:\n",
        "        \n",
        "        \n",
        "      print(b,'B 입니다.')\n",
        "      splited_b = b[0].split()\n",
        "      print(splited_b)\n",
        "      try:\n",
        "        dic[i][splited_b[0]] += 1\n",
        "\n",
        "      except :\n",
        "        pass\n",
        "      try:\n",
        "        dic[i][splited_b[1]] += 1\n",
        "      except :\n",
        "        pass\n",
        "      try:\n",
        "        dic[i][splited_b[2]] += 1\n",
        "\n",
        "      except :\n",
        "        pass\n",
        "      try:\n",
        "        dic[i][splited_b[3]] += 1\n",
        "\n",
        "      except :\n",
        "        pass\n",
        "        \n",
        "          \n",
        "  return dic\n",
        "\n",
        "      \n",
        "    \n",
        "    \n",
        "\n",
        "      \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhd-WpkOqfIa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
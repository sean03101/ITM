{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-ORnGHCPhM1"
      },
      "source": [
        "## Assignment #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbQVdrrapfG_"
      },
      "source": [
        "* Release date: 2021.09.28 Tue\n",
        "* Due date: **2021.10.05 Tue 23:59** (will not accept late submission)\n",
        "* Submission format: notebook file which can be executed in Colab environment\n",
        "* Weighting: 5% (total 50 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeVtMwumpcUu"
      },
      "source": [
        "1. **(10pts)** Calculate `rotation*x` and `x*rotation`. Explain how each computation is performed and why two results are the same.\n",
        "\n",
        "  ```python\n",
        "    import numpy as np\n",
        "\n",
        "    x = np.array([[2, 0]])\n",
        "    rotation = np.array([ [0, -1],\n",
        "                          [1,  0] ])\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98mxuJWoBzL1",
        "outputId": "cbcd6d93-e77d-46de-c634-816797e93319"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[2, 0]])\n",
        "rotation = np.array([ [0, -1],\n",
        "                       [1,  0] ])\n",
        "\n",
        "print('x*rotation is')\n",
        "print( x * rotation)\n",
        "print()\n",
        "print('rotation*x is')\n",
        "print(rotation * x)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x*rotation is\n",
            "[[0 0]\n",
            " [2 0]]\n",
            "\n",
            "rotation*x is\n",
            "[[0 0]\n",
            " [2 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ3svJpbO5r5"
      },
      "source": [
        "Broadcating is a function that automatically supports operations between different shape of arrays.\n",
        "The conditions under which broadcasting can occur are as follows.\n",
        "<br/>\n",
        "1.   It is possible when the dimension size is 1. </br>\n",
        "In an operation between two arrays, if the dimension of at least one array is 1, it is possible (whether it is axis 0 or axis 1; row 1 or column 1).\n",
        "<br/>\n",
        "2.   It's possible when the dimensions match. <br/>\n",
        "Broadcasting is possible if the lengths of the axes are the same for the dimensions.\n",
        "\n",
        "That is, the matrix X automatically broadcasts for the asterisk operation and changes as follows.\n",
        "\n",
        " ```python\n",
        "    x = np.array([[2, 0]]) -> x = np.array([[2,0],\n",
        "                                            [2,0]]) #By Broadcasting\n",
        "  ```\n",
        "Asterisk operation is an operator that multiplies two identical matrices by element-wise.\n",
        "As a result, the operation proceeds as follows.\n",
        " ```python\n",
        "    x * rotation = np.array([2*0, 0*-1],\n",
        "                            [2*1, 0*0]])\n",
        "  ```\n",
        "\n",
        "Since the asterisk operator is element-wise, the law of exchange is established."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARQKCA9sCpN-"
      },
      "source": [
        "2. **(5pts)** Suppose we have the following 2D tensor (i.e., a matrix). How to rearrange its values into 1D tensor (i.e., a vector) in a column major order?\n",
        "```python\n",
        "x = np.array([[1,  2,  3,  4],\n",
        "                 [5,  6,  7,  8],\n",
        "                 [9, 10, 11, 12]])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITEmmbqFBzL3",
        "outputId": "bde12c0b-5c60-44bf-84b4-ca72b39654e7"
      },
      "source": [
        "x = np.array([[1,  2,  3,  4],\n",
        "                 [5,  6,  7,  8],\n",
        "                 [9, 10, 11, 12]])\n",
        "\n",
        "x = x.flatten()\n",
        "print('x\\'s 1D tensor is :', x)\n",
        "print('x\\'s shape is :', x.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x's 1D tensor is : [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
            "x's shape is : (12,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0cUrs6OFJVn"
      },
      "source": [
        "3. **(5pts)** Compute a transpose of the matrix `x` in Problem 2 by using only `np.reshape` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyKEDQhYBzL4",
        "outputId": "71838dfc-8c43-4f1d-919c-14495cf952c6"
      },
      "source": [
        "x = np.reshape(x,(3,4))\n",
        "print('x\\'s 2D tensor is :')\n",
        "print(x)\n",
        "print('x\\'s shape is :', x.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x's 2D tensor is :\n",
            "[[ 1  2  3  4]\n",
            " [ 5  6  7  8]\n",
            " [ 9 10 11 12]]\n",
            "x's shape is : (3, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVzaCbniHnF6"
      },
      "source": [
        "4. **(5pts)** Perform vector arithmetic to create a `primes_squared_minus_one` vector, where the `i`th element is equal to the `i`th element in `primes` squared minus 1. For example, the second element of `primes_squared_minus_one` would be equal to `3^2 - 1 = 8`. Note that using `for` loop is not allowed.\n",
        "```python\n",
        "import numpy as np\n",
        "primes = np.array([2, 3, 5, 7, 11, 13])\n",
        "primes_squared_minus_one = ?\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPzuepeWBzL5",
        "outputId": "9384e13a-359a-43f4-a714-81882654b8a9"
      },
      "source": [
        "import numpy as np\n",
        "primes = np.array([2, 3, 5, 7, 11, 13])\n",
        "\n",
        "def fun_primes_squared_minus_one(n):\n",
        "    return n**2 -1\n",
        "\n",
        "func = np.vectorize(fun_primes_squared_minus_one)\n",
        "\n",
        "primes_squared_minus_one = func(primes)\n",
        "print('Answer is ' ,primes_squared_minus_one)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer is  [  3   8  24  48 120 168]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGIhrK8hVgjh"
      },
      "source": [
        "5. **(10pts)** Given any random matrices, compute the element-wise multiplication using a naive Python implementation and Numpy built-in function respectively. Compare the wall-clock times of these implementations as the size of matrices increases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBpXWmdCBzL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3518d8-9578-4ec7-d2b9-642804a6466d"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "matrix_shape_list = []\n",
        "time_to_make_naive = []\n",
        "time_to_make_numpy = []\n",
        "\n",
        "def naive_hadamard_product(mat1, mat2):\n",
        "    matrix_size_row = mat1.shape[0]\n",
        "    matrix_size_col = mat1.shape[1]\n",
        "\n",
        "    matrix_temp = np.zeros(shape=(matrix_size_row, matrix_size_col))\n",
        "\n",
        "    for i in range(matrix_size_row):\n",
        "      for j in range(matrix_size_col):\n",
        "        matrix_temp[i][j] = mat1[i][j] * mat2[i][j]\n",
        "\n",
        "    return matrix_temp\n",
        "\n",
        "for i in range(15):\n",
        "    matrix_shape_list.append(2**i)\n",
        "\n",
        "for matrix_shape in matrix_shape_list:\n",
        "    beg_time1 = time.time()\n",
        "    \n",
        "    matrix_temp1 = np.random.rand(matrix_shape, matrix_shape)\n",
        "    matrix_temp2 = np.random.rand(matrix_shape, matrix_shape)\n",
        "    matrix_temp_numpy = np.multiply(matrix_temp1,matrix_temp2)\n",
        "    \n",
        "    end_time1 = time.time()\n",
        "    time_to_make_numpy.append(end_time1 - beg_time1)\n",
        "\n",
        "    beg_time2 = time.time()\n",
        "    \n",
        "    matrix_temp_naive = naive_hadamard_product(matrix_temp1,matrix_temp2)\n",
        "    \n",
        "    end_time2 = time.time()\n",
        "    time_to_make_naive.append(end_time2 - beg_time2)\n",
        "    \n",
        "time_to_make_naive"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1205673217773438e-05,\n",
              " 7.152557373046875e-06,\n",
              " 2.7418136596679688e-05,\n",
              " 6.818771362304688e-05,\n",
              " 0.0002491474151611328,\n",
              " 0.0010037422180175781,\n",
              " 0.005061149597167969,\n",
              " 0.022311925888061523,\n",
              " 0.07186317443847656,\n",
              " 0.26901817321777344,\n",
              " 1.0825231075286865,\n",
              " 4.309640407562256,\n",
              " 17.509987831115723,\n",
              " 67.9693431854248,\n",
              " 276.3217399120331]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epq78DwcDZ-l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "4f22851f-671e-4485-8326-8d54d663ca24"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(matrix_shape_list, time_to_make_naive, marker = 'o')\n",
        "plt.plot(matrix_shape_list, time_to_make_numpy, marker = 'x')\n",
        "\n",
        "plt.title('Time Difference between Numpy built-in function and Naive method')\n",
        "plt.xlabel('Shape of Square Matrix')\n",
        "plt.ylabel('Time to Calculate(secs)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(['Naive', 'Numpy'])\n",
        "\n",
        "ax = plt.subplot()\n",
        "ax.set_xticks(matrix_shape_list[9:])\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAasAAAEqCAYAAACr/X8QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3wUdfrA8c+TQhJ67yV0BaRGCIqIFTvqSVek61lOTs8T9YrlvPNnPctZQBRRqooN9dBTAQuh9ypVEpASSICQnuf3x0xgCUl2E7LZTfK8X6+8svOd9uzsd+fZ78x3ZkRVMcYYY4JZSKADMMYYY7yxZGWMMSboWbIyxhgT9CxZGWOMCXqWrIwxxgQ9S1bGGGOCXokkKxHZICL9SmJZZ0tELhKRLR7D7UVktYgcE5E/iEiUiHwuIski8kEgYy0uEXlMRN4PdBzmTCKyQETGFnPeR0TkLfd1tIioiIQVYf43ROSvxVx3AxFZ5H5Pni/OMopLRI6LSKvSXGdR+fs7F0z7UF+JSD8RiS+hZXmt7z4lK7cy5f7liEiqx/BwVe2oqgtKImgvcTwmIpnuF+qYiGwVkVdFpFHuNKr6g6q295jtz8D3qlpNVV8GbgEaAHVUdaC/Yw42ZSHRichUt+L29ChrIyLl+qJAVf2nquab6HxJgqp6p6o+WczVjwcOAdVV9YFiLsOr/N6HqlZV1R3+Wqe/uTttFZHX8pT/KCIjfVlGae1Dz4b7HtsEav0+JSu3MlVV1arAr8D1HmXT/RviGWarajWgNnAT0BBY4Zmw8mgBbMgzvFVVs4q64qL8yjVn7TDwj0AHUYG0ADaq3SWguFKA20QkOsBxlF+qWqQ/YBdweUFlwGPAB8D7wDFgHdAOeBg4AOwBrvSYtwYwBdgHJODsoEILWPdjwPt5ykKBNcBz7nA/IN59/R2QDaQBx4GZQAaQ6Q6PcacbDWwCjgDzgRYey1fgbuAXYKdbdh2wGkgCfgY659kWfwLWAsnAbCDSY/wAd96jwHbgqmJuhw/dZR8DVgJdPMY3Bj4CDgI7gT+45Vflef9rgEuAdR7zfgMs8xj+AbixsOW640KAie57SgTmALXdcdHudrwd58fOIeDRQurYVOAF4DfgYresDaAF1UPPuuGxvlE49e0IcCdwvvu5JAGvesw7EvgJeNX9zDYDl7njBgIr8sR3P/BpAbEvAP4FLHU/4089tkM/3LpZyHcn73sIA57i9Hr8aiHb7R+e6wIewPne7QNGFTJfpls3jgOXey4rv9gpRj0v6H2477ONx/dgGk4d2w38BQjx+Jx+BJ5zP9OdwNWF1KPc+ngM2AjclOczL3BZQEtgoTvvN27deL+A9eRu61eAdzzKfwRGuq9b4+yPEnHq/3SgZt56gPMdS8WtM+64bu484d72V3niyq1DPn0PCls2sMhdVor72Q3GSx3z8lmGutv+ELADZx+rQFiBn2dBIwqpALvwnqzSgP44X7RpbkV4FAgHxuHu9N3pPwbeBKoA9XG+5HcUsO7H8qswwBPAkgK+VAuAsQUtA+dLtQ041433L8DPHuMVp7LWBqLcinMA6OVu8Nvd9x/hsS2W4lS62u4Hf6c7rifOF/sKnJ17E+CcYm6HTJxDmuE4O42d7usQYAXwN6AS0MqtDP0LeP9R7udV151/P06yrOaOSwXq+LDc+4A4oCkQ4b6XmXm+NJPdZXYB0oFzC9vpAn8AfnTLipOs3gAigSvd9/iJu22buJ/hxR47rizgj+42GOx+TrXd93LYM1ZgFfC7AmJf4G6/Tu5n+ZFHXP0oRrLKrx4Xtt081pWF890IB64BTgC1vM1bwPBpsVP8en7G++D0ZDUNJ8FXc7fBVk79qByJU+/H4Xz3fg/sBaSA9zTQjS/E/UxTgEa+LAtYjPODKQLoi5O0vCWrhjjJub1b7pms2rjbIwKoh7Pz/3cB9eA7YJzHuGeBN3zZX+WJK5qifQ982Re2yfO+C6xjXj7LO3F+FDbDqT/fE6Bk9Y3HuOtxMnGoO1zNDaomzrmjdCDKY/qhOOeYipKs7gR+KeBLtYDCk9VXuRvQHQ5xN3gLjw/oUo/xrwNP5ln/Fo8PfBdwq8e4Zzwq2pvAi/nEX5ztEJcn5n3ARThJ9Nc80z+M+4svv22I03q6GYgFvsZpFV2F0+pa607jbbmbcFsj7nAjnJ1BGKe+NE09xi8FhhTw/qbiJKsInJbY1RQvWTXxGJ8IDPYY/giYoKd2XKft9Nz4bvP4zJ9yX3fE+dUZUUDsC4CnPYY74LRYQin9ZJWKx5cfZ8cU623eAoZPi51i1POC3of7Ptu42ygD6OAx7g5ggcfntM1jXGV33oaFbReP6VcDA7wtC2iOsxOu4jF+Bl6Slcd2mO2+Ppms8pnnRmBVAfVgLPCd+1pwWkV93eFC91d51pFbh3z9HviyL8ybrPKtYz58lt/h/rhxh6/ES7LyV9f1/R6vU4FDqprtMQxQFec4eTiwT0SSRCQJp6LXL+L6muD8+i2OFsBLHus/jFNBmnhMsyfP9A/kTu/O0wznF1yu3zxen8B5r7jTbS8ghqJuh5MxqWoOzi+7xu6yGueJ7xGchFiQhTgVr6/7egFwsfu30CPGwpbbAvjYY9wmnEM+nustaLvkS1XTgSfdv+LIWw/zDnuuP0Hdb41rN6c+03eBYSIiwG3AHDe2gnjWl904n23dIsZeKLfnYG4npzcKmCxRTz8363WbF1FR67k3ua373R5luzn9u3hynap6wn2Z73sSkRFuT+DcOtmJ0z+HgpbVGDiiqil54vDF/wH9RaRLnlgaiMgsEUkQkaM4p0kKqhMfAb3d8/B9gRycH5Tg2/4qL1+/B8VZdkF1zNtn2ZgzvyeFCnSHgT04LYq6WowODwAiEoLTevvfWcTwlBbeUcRzJ5Y7/VPFXFfrAsqLuh2a5b5wt0FTnNZBFs5h1rYFzKf5lC0EnsdpxTyN03KY7Mb0H48YC1vuHmC0qv6Ud8RZnnR+B3gIp+XnKQXn13CuhmexDoAmIiIeCas58BmAqsaJSAZOy3WY+1eYZh6vm+O0MA/ljVlEQnEOCfnitM9NVf8J/NPHeYvjbLZvQfUc8q9/uQ7hbKsWOOeYwNl+CUVYNwAi0gKnDl8GLFbVbBFZjbPz9WYfUEtEqngkrOZeYgdAVRNF5N+c+QPrn+7856nqYRG5Eec8WH7LOCIiX+McujwXmOVRL33ZXxVXSS7b22e5jzO/J4UK6EXBqroP57DT8yJSXURCRKS1iFzsbV4RCRORc3E6TTTEOb5cHG8AD4tIR3e5NUSksC7tk4E7RaSXOKqIyLUiUs2HdU0BRonIZe57bSIi5xRzO/QQkZvdHooTcBJLHM7hq2Mi8pA415SFikgnETnfnW8/EO0muFw/A+1xzjUsVdUNOJWsF86xdXxY7hvAU+5OAhGpJyIDfNgmhXKT999xEpan1cAQEQkXkRic83dnoz7wB3d5A3F2El96jJ+Gs3PJVNUfvSzrVhHpICKVcY7nf+geWdgKRLr1JRznnECEj/HtxzlPWFpWA9eISG0RaYhTx3yVbz13xxX4PtxtNAenHlVz69L9OK2QoqqCkxwOAojIKJyWlVequhtYDjwuIpVEpA/OD2JfvQBcgFOHclXDOR2SLCJNgAe9LGMGMAKnXs/wKC/q/qoovC3b5zrow2c5B+f71lREauF0hilUMNzBYgTOCfuNOL/oP8Q531GQwSJyHOcE7mc4x2B7qOre4qxcVT/GabrPcpvn63HOkRQ0/XKck7KvuvFuwzn+7cu6luL0zHnRjX8hTlKAom+HT3F+eR3BOTR1s6pmupXkOqArTqeLQ8BbOD1zwOmpCZAoIivduFJwehRuUNUMd/xiYLeqHnCn8bbcl3A+j69F5BhO4uzly3bxwUycX2Ke/orz6/0I8Dinf6GLYwnQFud9PQXcoqqJHuPfw9nZ+bLjfA/nnM9vOCe2/wCgqsnAXTjbLQGn9eLrRZUvAbeIyBERednHec7Gezi9RXfh/JCa7euMXuq5t/dxL8522YFzzmcG8HZRg1fVjThHCxbj7GTPw+nx6athOPX3MM6PpWlFWPdRnHNXtT2KHwe642yPL4C5XhbzGU59/E1V13gsu0j7q6LwYdmPAe+6hwkH+bDIwj7LyTi9Ddfg7Hu8bY+TPV+MqbDEuXBzrKr2KWSaKJyTx91V9ZfSis0Y4wiGlpUxZcHvca4/s0RlTAAEuoOFMUFPRHbhnJi/McChGFNh2WFAY4wxQc8OAxpjjAl6lqyMMcYEvTJ9zqpu3boaHR0d6DCMMaZMWbFixSFV9fWC9KBQppNVdHQ0y5cvD3QYxhhTpoiIr7ePChp2GNAYY0zQs2RljDEm6FmyMsYYE/TK9Dmr/GRmZhIfH09aWlqgQwk6kZGRNG3alPDw8ECHYowxRVLuklV8fDzVqlUjOjoa5/FDBpyHbCYmJhIfH0/Lli0DHY4xxhRJuUtWaWlplqjyISLUqVOHgwcPBjoUY8xZ+GRVAs/O38LepFQa14ziwf7tubFbYc9HLB/KXbICLFEVwLaLMWXbJ6sSeHjuOlIznQevJySl8vDcdQDlPmFZBws/EBEeeOCBk8PPPfccjz32WKHzfPbZZzz99NN+jswYU5Y9O3/LyUSVKzUzm2fnbwlQRKXHkpUfREREMHfuXA4dOuTzPDfccAMTJ3p9WKYxpgLbm5RapPLypMInq09WJXDh09/RcuIXXPj0d3yyKuGslxkWFsb48eN58cUXzxj3+eef06tXL7p168bll1/O/v37AZg6dSr33HMPycnJtGjRgpycHABSUlJo1qwZmZmZbN++nauuuooePXpw0UUXsXnz5rOO1RhTNhw4mkZoSP6H8hvXjCrlaEpfhU5Wucd/E5JSUU4d/y2JhHX33Xczffp0kpOTTyvv06cPcXFxrFq1iiFDhvDMM8+cNr5GjRp07dqVhQsXAjBv3jz69+9PeHg448eP55VXXmHFihU899xz3HXXXWcdpzEm+P2WnMaQSXGIQKWw03fbUeGhPNi/fYAiKz3lsoNFrsc/38DGvUcLHL/q1yQysnNOK0vNzObPH65l5tJf852nQ+Pq/P36jl7XXb16dUaMGMHLL79MVNSpXz3x8fEMHjyYffv2kZGRkW838sGDBzN79mwuueQSZs2axV133cXx48f5+eefGThw4Mnp0tPTvcZhjCnb9ialMnRyHInHM5g5Lpb4I6nWG7CiyZuovJUX1YQJE+jevTujRo06WXbvvfdy//33c8MNN7BgwYJ8O17ccMMNPPLIIxw+fJgVK1Zw6aWXkpKSQs2aNVm9enWJxGaMCX57Dp9g2FtxJKVkMm1MT7o3r0VMdPnv+Zefcp2svLWALnz6OxLyOTHZpGYUs+/ofdbrr127NoMGDWLKlCmMHj0agOTkZJo0cSrau+++m+98VatW5fzzz+e+++7juuuuIzQ0lOrVq9OyZUs++OADBg4ciKqydu1aunTpctZxGmOCz+7EFIZNXsKxtEymj+tF56Y1Ax1SQFXoc1YP9m9PVHjoaWUlffz3gQceOK1X4GOPPcbAgQPp0aMHdevWLXC+wYMH8/777zN48OCTZdOnT2fKlCl06dKFjh078umnn5ZYnMaY4LHzUAqD34wjJSOLGeNiK3yiAhBVDXQMxRYTE6N5n2e1adMmzj33XJ+XUdGuBi/q9jHGlK5tB44zbHIcWTnK9LG9OLdR9RJfh4isUNWYEl+wH5Xrw4C+uLFbk3KdnIwxZcfW/ccYNjkOEGaNj6Vdg2qBDiloVOjDgMYYEyw27j3KkElxhIglqvxU+JaVMcYE2vqEZG6dsoSo8FBmjIulZd0qgQ4p6FjLyhhjAmjNniSGTY6jSqUwZo/vbYmqANayMsaYAFmx+wgj315KzSrhzBwXS9NalQMdUtCylpUxxgTAsl2HGTFlCXWqVmL2+N6WqLywZOUHxXlEiDGm4li8PZHb315KgxqRzL6jd4W4Ee3Z8luyEpFmIvK9iGwUkQ0icp9b/piIJIjIavfvGo95HhaRbSKyRUT6+ys2fyvOI0KMMRXDj78cYtTUpTSpGcWs8bE0qB4Z6JDKBH+2rLKAB1S1AxAL3C0iHdxxL6pqV/fvSwB33BCgI3AV8JqIhOa34BLz479h56LTy3YucsrPQmGPCBk5ciQffvjhyeGqVasCsGDBAi6++GIGDBhAq1atmDhxItOnT6dnz56cd955bN++/eT8d955JzExMbRr14558+YB0Ldv39PuG9inTx/WrFlzVu/DGFOyFmw5wJh3lxFdpwozx8dSv5olKl/5LVmp6j5VXem+PgZsAgq7+nYAMEtV01V1J7AN6Omv+ABo0h0+GHkqYe1c5Aw36X7Wiy7oESGFWbNmDW+88QabNm3ivffeY+vWrSxdupSxY8fyyiuvnJxu165dLF26lC+++II777yTtLQ0xowZw9SpUwHYunUraWlpdt9AY4LIt5v2M37aClrXq8qMcbHUrRoR6JDKlFLpDSgi0UA3YAlwIXCPiIwAluO0vo7gJLI4j9niKTy5effVRPhtXeHTVGsE793k/D+2D+qdAwv+z/nLT8Pz4Grvj58v6BEhhTn//PNp1KgRAK1bt+bKK68E4LzzzuP7778/Od2gQYMICQmhbdu2tGrVis2bNzNw4ECefPJJnn32Wd5++21Gjhzp0zqNMf43f8Nv3DNjJec0rM57Y3pSs3KlQIdU5vi9g4WIVAU+Aiao6lHgdaA10BXYBzxfxOWNF5HlIrL84MGDZx9gZE0nUSXvcf5HltwNIydMmMCUKVNISUk5WRYWFnbyKcA5OTlkZGScHBcRceqXVkhIyMnhkJAQsrKyTo4TOf1poSJC5cqVueKKK/j000+ZM2cOw4cPL7H3YYwpvi/X7ePu6Svp2LgG74/tZYmqmPzashKRcJxENV1V5wKo6n6P8ZOBee5gAtDMY/ambtlpVHUSMAmcG9kWGoAPLaCTh/76/hmWT4F+D0HLvt7n80F+jwiJjo5mxYoVDBo0iM8++4zMzMwiL/eDDz7g9ttvZ+fOnezYsYP27Z27xI8dO5brr7+eiy66iFq1apXIezDGFN9na/byx9mr6dasJu+MOp9qkeGBDqnM8mdvQAGmAJtU9QWP8kYek90ErHdffwYMEZEIEWkJtAWW+is+4FSiGjgVLn3U+e95DqsE5H1EyLhx41i4cCFdunRh8eLFVKlS9KvVmzdvTs+ePbn66qt54403iIx0TtL26NGD6tWrn/awR2NMYMxdGc+EWavo0aIW747uaYnqLPntESEi0gf4AVgH5D569xFgKM4hQAV2AXeo6j53nkeB0Tg9CSeo6leFreOsHxHy47+dzhSeLamdiyBhJfSZ4NsyStnIkSO57rrruOWWW84Yt3fvXvr168fmzZsJCcn/d4g9IsQY/5uzbA8PzV1L71Z1eOv2GCpXCq6bBdkjQjyo6o+A5DPqy0LmeQp4yl8xnSG/hNSyb4kdBixN06ZN49FHH+WFF14oMFEZY/xvxpJfeeTjdVzUti6TR8QQGe7fK3AqiuBK98ar3O7peY0YMYIRI0aUbjDGmNNMW7yLv326gUva1+P1W3tYoipBlqyMMaYETPlxJ0/O28gVHRrw6rBuRIRZoipJ5TJZqeoZ3buNs12MMSXvzYXb+ddXm7m6U0NeGtKNSmF2KL6klbstGhkZSWJiou2Y81BVEhMTT/YcNMaUjFe/+4V/fbWZ6zo34uWhlqj8pdy1rJo2bUp8fDwlcsFwORMZGUnTpk0DHYYx5YKq8tK3v/Dv//3CjV0b89zALoSFWqLyl3KXrMLDw2nZsmWgwzDGlGOqyvNfb+XV77dxS4+m/N/vOhMaYqce/KncJStjjPEnVeXprzbz5qIdDO3ZjKduPI8QS1R+Z8nKGGN8pKo8OW8Tb/+0k9tiW/D4DR0tUZUSS1bGGOODnBzlsc83MG3xbkZdGM3frutgvY5LkSUrY4zxIidHefST9cxc+ivj+7bi4avPsURVyixZGWNMIbJzlIkfreWDFfHc1a81D/Zvb4kqACxZGWNMAbJzlAc/WMPcVQncd1lbJlze1hJVgFiyMsaYfGRl5/DHOWv4fM1eHriiHfde1jbQIVVolqyMMSaPzOwc/jBzFV+t/42JV5/DnRe3DnRIFZ4lK2OM8ZCRlcM9M1by9cb9/OXacxl7UatAh2SwZGWMMSelZWZz1/SVfLf5AI/f0JHbL4gOdEjGZcnKGGNwEtX491awaOtBnrqpE8N7tQh0SMaDT8lKREKALkBjIBVYr6oH/BmYMcaUltSMbMZOW8bP2xN55nedGXR+s0CHZPIoNFmJSGvgIeBy4BfgIBAJtBORE8CbwLuqmuPvQI0xxh9S0rMYPXUZy3Yd5rlbuvC7HvZkgmDkrWX1D+B14A7N84AoEakPDANuA971T3jGGOM/x9OzGPXOUlbsPsKLg7syoGuTQIdkClBoslLVoYWMOwD8u8QjMsaYUnA0LZPb317K2vhkXhnanWs7Nwp0SKYQPj0pTEQGikg19/VfRWSuiHT3b2jGGOMfyScyufWtJaxPSOY/wyxRlQW+Ptbyr6p6TET6AJcBU3AODxpjTJlyJCWDYW/FsXnfMV4f3oOrOjUMdEjGB74mq2z3/7XAJFX9Aqjkn5CMMcY/Eo+nM3RyHL8cOM6kET24vEODQIdkfORrskoQkTeBwcCXIhJRhHmNMSbgDh5zEtXOQylMuT2Gfu3rBzokUwS+JpxBwHygv6omAbWBB/0WlTHGlKD9R9MYMmkxew6n8s6o87mobb1Ah2SKyNc7WHQGvlHVY+5wCpDsn5CMMabk7EtOZdjkJRw4msa7o3vSs2XtQIdkisHXltXrwHGP4eN46WAhIs1E5HsR2SgiG0TkPre8toh8IyK/uP9rueUiIi+LyDYRWWu9DY0xZyv+yAkGvxnHoWPpTBtjiaos8zVZiedFwe4dK7y1yrKAB1S1AxAL3C0iHYCJwLeq2hb41h0GuBpo6/6Nx3obGmPOwp7DTqJKOpHBe2N70aOFJaqyzNdktUNE/iAi4e7ffcCOwmZQ1X2qutJ9fQzYBDQBBnDqjhfvAje6rwcA09QRB9QUEbv4wRhTZLsOpTDozcUcT89ixrhYujarGeiQzFnyNVndCVwAJADxQC+c1o9PRCQa6AYsARqo6j531G9Abt/RJsAej9ni3bK8yxovIstFZPnBgwd9DcEYU0FsP3icwZMWk56Vw8xxsXRqUiPQIZkS4FMHC/fWSkOKswIRqQp8BExQ1aMi4rlcFREtcOb8Y5kETAKIiYkp0rzGmPLtl/3HGDp5CaDMHBdL+4bVAh2SKSG+3m6pnYh8KyLr3eHOIvIXH+YLx0lU01V1rlu8P/fwnvs/91EjCYDnffmbumXGGOPV5t+OMmRSHCIwa7wlqvLG18OAk4GHgUwAVV2Ll5aWOE2oKcAmVX3BY9RnwO3u69uBTz3KR7i9AmOBZI/DhcYYU6ANe5MZOimO8NAQZo+PpU19S1Tlja/XWVVW1aWeh/BwevsV5kKcx4esE5HVbtkjwNPAHBEZA+zGueAY4EvgGmAbcAIY5WNsxpgKbF18MrdOWUKVSqHMHB9LizpVAh2S8QNfk9Uh90GMCiAitwCFtnpU9UdAChh9WT7TK3C3j/EYYwyrfj3CiLeXUiMqnJnjYmlWu3KgQzJ+4muyuhunU8M5IpIA7ARu9VtUxhjjxfJdhxn5zjLqVK3EjHGxNKkZFeiQjB/52htwB3C5iFQBQjxuu2SMMaVuyY5ERk1dRoPqkcwcF0vDGpGBDsn4ma+9Ae8Tkeo455JeFJGVInKlf0Mzxpgz/bztECPfWUajGpHMHm+JqqLwtTfgaFU9ClwJ1MHpOPG036Iyxph8LNp6kFFTl9G8dmVmje9N/eqWqCoKn+8N6P6/BueWSBsouPOEMcaUuO83H2DstOW0qleVGeN6Ua9aRKBDMqXI12S1QkS+xklW80WkGpDjv7CMMeaUbzbuZ/x7y2nXoCozx/WiTlVLVBWNr70BxwBdgR2qekJE6mDXQRljSsF/1+/jnhmr6Ni4OtNG96JG5fBAh2QCoNCWlXsDWlQ1R1VXuk8JRlUTVXWte7eJpv4P0xhTEX2+Zi93z1hF56Y1eG+sJaqKzFvL6lkRCcG5JdIK4CAQCbQBLsG5uPfvOHdIN8aYEvPJqgTun7OamBa1eXvU+VSN8PVAkCmPCv30VXWg+8DE4cBooBFO9/VNOLdHekpV0/wepTGmQvlwRTwPfriG2JZ1mDIyhsqVLFFVdF5rgKpuBB4thViMMYZZS3/l4Y/X0adNXSbdFkNUpdBAh2SCgK8XBVcWkb+IyCR3uK2IXOff0IwxFc17cbuZOHcdfdvWY/IIS1TmFF+7rr8DZOA8LRic50z9wy8RGWMqpHd+2slfP1nP5efWZ9KIHkSGW6Iyp/iarFqr6jOcep7VCeyiYGNMCZm8aAePf76R/h0b8NrwHkSEWaIyp/P1rGWGiERx6hEhrYF0v0VljKkwXluwjWf+u4Vrz2vEv4d0JTzU19/QpiLxNVk9BvwXaCYi03EerGgXBRtjzspL//uFF/+3lQFdG/P8wC6EWaIyBfD1ESFfi8gKIBbn8N99qnrIr5EZY8otVeXFb7by8nfbuLl7E569pQuhIXZmwRTM196A37p3rfhCVeep6iER+dbfwRljyh9V5Zn5W3j5u20Mjmlmicr4pNCWlYhEApWBuiJSi1OdKqoDTfwcmzGmnFFVnvpiE2/9uJPhvZrz5IBOhFiiMj7wdhjwDmAC0Bjndku5teoo8Kof4zLGlDOqyuOfb2Tqz7sYeUE0f7++AyKWqIxvvN1u6SXgJRG5V1VfKaWYjDHlTE6O8tdP1zN9ya+M7dOSR6891xKVKRJfO1i8IiKdgA44N7LNLZ/mr8CMMeVDTo7yyMfrmLVsD3de3JqHrmpvicoUmU/JSkT+DvTDSVZfAlcDPwKWrIwxBcrOUf784Vo+WhnPvZe24f4r2lmiMsXi60UNt+A8DuQ3VR0FdAFq+C0qY0yZl5Wdw/1zVvPRynj+eHk7HrjSWlSm+DQm1QEAACAASURBVHy9KDhVVXNEJEtEqgMHgGZ+jMsYU4ZlZucwYfZqvli7jwf7t+fuS9oEOiRTxvmarJaLSE1gMk6vwOPAYr9FZYwpszKycrh35krmb9jPo9ecy7i+rQIdkikHfDoMqKp3qWqSqr4BXAHc7h4OLJCIvC0iB0RkvUfZYyKSICKr3b9rPMY9LCLbRGSLiPQv7hsyxgROelY2d01fwfwN+/nbdR0sUZkS4+2i4O6FjVPVlYXMPhXnWqy8nTBeVNXn8iyrAzAE6IhzTdf/RKSdqmYXFp8xJnikZWZz5/srWLDlIE8O6MhtvaMDHZIpR7wdBny+kHEKXFrgSNVFIhLtYxwDgFmqmg7sFJFtQE/sUKMxZUJqRjbj31vOj9sO8a+bz2Noz+aBDsmUM94uCr7ED+u8R0RGAMuBB1T1CM6tm+I8ponHbudkTJlwIiOLMVOXE7czkWd+15mBMdb3ypQ8X6+zGpFfeTEuCn4deBKnVfYkTsttdFEWICLjgfEAzZvbrzdjAul4ehaj31nG8t2HeXFQV27sZr8xjX/42hvwfI/XkTjXXK2kiBcFq+r+3NciMhmY5w4mcHpX+KZuWX7LmARMAoiJidGirN8YU3KOpWUy8p1lrN6TxEtDunF9l8aBDsmUY77ebulez2G3G/usoq5MRBqp6j538CYgt6fgZ8AMEXkBp4NFW2BpUZdvjCkdyamZjHh7KRsSknl1aDeuPq9RoEMy5ZyvLau8UoCWhU0gIjNxbtFUV0Tigb8D/USkK85hwF04d3VHVTeIyBxgI5AF3G09AY0JTkknMrh1yhK2/HaM14Z358qODQMdkqkAfD1n9TlOggHn2qwOwJzC5lHVofkUTylk+qeAp3yJxxgTGIdTMhj+1hK2HzzOpNtiuOSc+oEOyVQQvrasPK+LygJ2q2q8H+IxxgSpQ8fTGT55CbsSU3hrRAx929ULdEimAvE1Wf0K7FPVNAARiRKRaFXd5bfIjDFB48DRNIa9tYT4Iyd4e+T5XNimbqBDMhWMr3dd/wDI8RjOdsuMMeXcb8lpDJkUx96kVKaO6mmJygSEry2rMFXNyB1Q1QwRqeSnmIwxQSIhKZVhk+NIPJ7BtNE9iYmuHeiQTAXla8vqoIjckDsgIgOAQ/4JyRgTDPYcPsHgNxdzOCWDaWMsUZnA8rVldScwXUReBQTYA+R7VwtjTNm3OzGFoZPiSMnIZvrYXnRuWjPQIZkKzteLgrcDsSJS1R0+7teojDEBs+PgcYZNXkJ6lpOoOjWxh4KbwPP2iJD7CygHQFVf8ENMxpgA2XbgGMMmLyE7R5kxLpZzG1UPdEjGAN5bVtVKJQpjTMBt+e0Yw9+KA4RZ42Np28C+/iZ4eHtEyOOlFYgxJnA27j3KrVOWEBYizBgXS5v6VQMdkjGn8fV2S5HAGJwn+UbmlqtqkR7vYYwJPusTkrl1yhKiwkOZMS6WlnWrBDokY87ga9f194CGQH9gIc4jPI75KyhjTOlYsyeJYZPjqFIpjNnje1uiMkHL12TVRlX/CqSo6rvAtUAv/4VljPG3FbuPcOtbS6hROZzZd8TSvE7lQIdkTIF8TVaZ7v8kEekE1ADsdsvGlFFLdx5mxJQl1KlaiTl39KZpLUtUJrj5elHwJBGpBfwF50GJVYG/+S0qY4zfLN6eyOipy2hUM5KZ42JpUD3S+0zGBJivFwW/5b5cBLTyXzjGGH/68ZdDjJ22jGa1KjN9XC/qV7NEZcqGQg8Disj9IjImn/IxIjLBf2EZY0ragi0HGP3uMqLrVGHW+FhLVKZM8XbOajgwLZ/y9wDrtm5MGfHtpv2Mn7aCtvWrMnNcLHWqRgQ6JGOKxFuyClPVzLyF7uNCxD8hGWNK0n/X/8ad76/gnEbVmDE2llpV7Ok+puzxlqxCRKRB3sL8yowxweeLtfu4Z8ZKOjauwXtjelGjcnigQzKmWLwlq2eBL0TkYhGp5v71A+YBz/k9OmNMsX26OoE/zFpF12Y1eW9MT2pEWaIyZZe3ewNOE5GDwBNAJ0CBDcDfVPWrUojPGFMMc1fG86cP1nB+dG3eHnk+VSJ8vUrFmODktQa7SckSkzFlxJxle3ho7louaF2HySNiqFzJEpUp+6wWG1OOTF+ym0c/Xk/fdvWYdFsPIsNDAx2SMSXC19stGWOC3Ls/7+LRj9dz6Tn1LVGZcsdaVsaUA2/9sIN/fLGJKzo04NVh3YgIs0RlyhefWlYiUkNEXhSR5e7f8yJSw9/BGWO8e2Phdv7xxSau7tSQ14Z3t0RlyiVfDwO+DRwFBrl/R4F3CptBRN4WkQMist6jrLaIfCMiv7j/a7nlIiIvi8g2EVkrIt2L93aMqVhe/e4Xnv5qM9d3acwrQ7sRHmpH9k355GvNbq2qf1fVHe7f43i/oe1U4Ko8ZROBb1W1LfCtOwxwNdDW/RsPvO5jXMZUSKrKi99s5bmvt3JTtya8OKgLYZaoTDnma+1OFZE+uQMiciGQWtgMqroIOJyneADwrvv6XeBGj/Jp6ogDaopIIx9jM6ZCUVWe+3oLL337C7f0aMpzAy1RmfLP1w4WdwLTPM5THQFuL8b6GqjqPvf1b0DubZuaAHs8pot3y/ZhjDlJVfnXV5uZtGgHQ3s246kbzyMkxG7Taco/X5PVUVXtIiLVAVT1qIi0PJsVq6qKiBZ1PhEZj3OokObNm59NCMaUKarKE/M28s5Pu7gttgWP39DREpWpMHw9dvAROElKVY+6ZR8WY337cw/vuf8PuOUJQDOP6Zq6ZWdQ1UmqGqOqMfXq1StGCMaUPTk5yt8+3cA7P+1i1IXRPDHAEpWpWAptWYnIOUBHoIaI3OwxqjpQnCe3fYZz+PBp9/+nHuX3iMgsoBeQ7HG40JgKLSdHefSTdcxcuoc7+rZi4tXnIGKJylQs3g4DtgeuA2oC13uUHwPGFTajiMwE+gF1RSQe+DtOkprjPn14N043eIAvgWuAbcAJYFSR3oUx5VR2jvLQR2v5cEU8d1/Smj9d2d4SlamQRNX7aSMR6a2qi0shniKJiYnR5cuXBzoMY/wiKzuHBz9cy8erErjvsrZMuLytJSpTIkRkharGBDqOovCpg0UwJipjyrOs7Bz+OGcNn6/Zy5+ubMc9l7YNdEjGBJTdG9CYIJOZncMfZq7iq/W/MfHqc7jz4taBDsmYgLNkZUwQSc/K5p4Zq/hm437+cu25jL3I241ijKkYfL2RbQMRmSIiX7nDHdxOEsaYEpKWmc3v31/JNxv38/gNHS1RGePB1+uspgLzgcbu8FZggj8CMqYiSsvMZty05Xy3+QBP3dSJ2y+IDnRIxgQVX5NVXVWdA+QAqGoWkO23qIypQFIzshnz7jJ+3HaIZ37XmeG9WgQ6JGOCjq/nrFJEpA6gACISCyT7LSpjKoiU9CxGT13Gsl2HeX5gF27u3jTQIRkTlHxNVvfj3GWitYj8BNQDbvFbVMZUAMfSMhn1zjJW7UnixcFdGdC1SaBDMiZo+Xqd1UoRuRjnjhYCbFHVTL9GZkw5lpyaych3lrIuPpmXh3Tj2s72RBxjCuNTshKRUJzbIUW781wpIqjqC36MzZhyKelEBiPeXsqmfUd5dVh3rurUMNAhGRP0fD0M+DmQBqzD7WRhjCm6IykZDH9rCdsOHOeNW3tw2bkNvM9kjPE5WTVV1c5+jcSYcu7Q8XRufWsJOw6lMGlED/q1rx/okIwpM3ztuv6ViFzp10iMKccOHEtj6KQ4diWm8Pbt51uiMqaIfG1ZxQEfi0gIkInTyUJVtbrfIjOmnNh/NI2hk+PYl5TGOyN70rt1nUCHZEyZ42uyegHoDaxTX54pYowBYG9SKsMmx3HwWDrvju5Jz5a1Ax2SMWWSr8lqD7DeEpUx3n2yKoFn529hb1IqISFCmMCM8b3p0aJWoEMzpszyNVntABa4N7JNzy20ruvGnO6TVQk8PHcdqZnO3ciyc5SwsBD2HD5hycqYs+BrB4udwLdAJaCax58xxsMz8zefTFS50rNyeHb+lgBFZEz54OsdLB73dyDGlGVpmdnMWb6HvUlp+Y7fm5RayhEZU74UmqxE5FVVvUdEPse9ia0nVb3Bb5EZUwYcTcvkvcW7eeennRw6nkGl0BAyss+8br5xzagARGdM+eGtZTUCuAd4rhRiMabMOHQ8nbd/3Ml7i3dzLD2Lvu3qcXe/1uxNSuWRj9efdigwKjyUB/u3D2C0xpR93pLVdgBVXVgKsRgT9OKPnGDyoh3MWraHjOwcrunUiN/3a02nJjVOTiMiJ3sDNq4ZxYP923NjN7ujujFnw1uyqici9xc00noDmopi24FjvL5gB5+uTgDg5u5NuOPi1rSuV/WMaW/s1sSSkzElzFuyCgWq4tyxwpgKZ218Eq99v535G38jIiyE23q3YNxFrewclDGlzFuy2qeqT5RKJMYECVVl8Y5EXl+wnR9+OUS1yDDuuaQNIy+Ipk7ViECHZ0yF5C1ZWYvKVBg5Ocq3mw/w2oJtrPo1ibpVI5h49TkM79WcapHhgQ7PmArNW7K6rFSiMCaAsrJzmLd2H68v2M6W/cdoWiuKJ2/sxMAeTYkMDw10eMYYvCQrVT3sj5WKyC7gGJANZKlqjIjUBmbjPI14FzBIVY/4Y/3GgHMh74cr4nlz0Xb2HE6lXYOqvDi4C9d3bkxYqK83dzHGlAZf7w3oD5eo6iGP4YnAt6r6tIhMdIcfCkxopjw7np7F9LjdvPXjTg4eS6drs5r89doOXH5uA0JC7Mi3McEokMkqrwFAP/f1u8ACLFmZEnQ4JYOpP+1k6s+7OJqWRZ82dXlpSFd6t6qDiCUpY4JZoJKVAl+LiAJvquokoIGq7nPH/wY0CFBsppzZm5TK5B92MGvpHlIzs+nfsQF39WtDl2Y1Ax2aMcZHgUpWfVQ1QUTqA9+IyGbPkaqqbiI7g4iMB8YDNG/e3P+RmjJrx8HjvLlwB3NXxZOjMKBrY35/cWvaNrAHBhhT1gQkWalqgvv/gIh8DPQE9otII1XdJyKNgAMFzDsJmAQQExNjD4M0Z1ifkMzrC7bz5fp9VAoNYVjP5ozr24qmtSoHOjRjTDGVerISkSpAiKoec19fCTwBfAbcDjzt/v+0tGMzZdvSnYf5z/fbWLj1INUiwvj9xa0ZdWFL6lWzC3mNKesC0bJqAHzsntAOA2ao6n9FZBkwR0TGALuBQQGIzZQxqsqCLQf5z/fbWL77CHWqVOLB/u25rXcLqtuFvMaUG6WerFR1B9Aln/JE7CJk46PsHOWLdc6FvJv2HaVJzSgev6Ejg2KaEVXJLuQ1prwJpq7rxniVnpXN3JUJvLlwO7sST9C6XhWeG9iFAV0bE24X8hpTblmyMmVCSnoWM5f+yuQfdrD/aDqdm9bgjVu7c2WHhnYhrzEVgCUrE9SSTmQw9eddTP15F0knMundqg7PDexCnzZ17UJeYyoQS1YmKO0/msZbP+xg+pJfOZGRzeXnNuCuS1rTvXmtQIdmjAkAS1YmqOxOTOGNhTv4aEU82apc37kRv+/XhvYN7UJeYyoyS1YmKGzad5TXF2xn3tq9hIWGMDCmKXf0bU3zOnYhrzHGkpUJsBW7D/Of77fz3eYDVKkUyriLWjGmT0vqV48MdGjGmCBiycqUOlVl0S+H+M/321i68zC1Kodz/xXtuL13NDUq24W8xpgzWbIypSY7R5m/4TdeW7CN9QlHaVg9kr9d14EhPZtRuZJVRWNMwWwPYfwuIyuHT1Yl8MbC7ew4lELLulV45nedubFbEyqF2YW8xhjvLFkZv0nNyGbWsl+ZtGgH+5LT6NCoOv8Z1p2rOjUk1C7kNcYUgSUrU+KST2QybfEu3vl5F4dTMujZsjb/uvk8Lm5Xzy7kNcYUiyUrU2IOHEtjyo87mR73K8fTs7j0nPrc1a81MdG1Ax2aMaaMs2Rlztqewyd4c9F25iyPJys7h2s7O0/k7dC4eqBDM8aUE5asTLFt3X+M1xds57M1ewkRuKWHcyFvdN0qgQ7NGFPOWLIyhfpkVQLPzt/C3qRUGteM4sH+7WlRpzKvLdjONxv3U7lSKKMuiGbsRa1oWMMu5DXG+IclK1OgT1Yl8PDcdaRmZgOQkJTK/XNWk6NQIyqc+y5ry8gLoqlVpVKAIzXGlHeWrEyBnvnv5pOJKleOQvXIMH6aeClVI6z6GGNKh+1tzEnpWdms/jWJuB2HWbIzkb3JaflOdywtyxKVMaZU2R6nAkvLzGbVr0ks2ZlI3I5EVv2aRHpWDiJwbsPqVIkIJSU9+4z5GteMCkC0xpiKzJJVBZKWmc3K3UeI23mYuB2JrN6TRIabnDo2rs5tsS3o1aoOPaNrU6Ny+BnnrACiwkN5sH/7AL4LY0xFZMmqHEvNyGbF7iMnW05r9iSTkZ1DiECnJjW4vXcLYlvVISa6NjWizrzb+Y3dmgCc0Rswt9wYU8p+/Dc06Q4t+54q27kIElZCnwmBi6sUWLIqR05kZLFi9xHidiSyZMdh1sQnkZmthIYInZrUYNSF0cS2qkOP6FpUj/TtURw3dmtiycmYYNGkO3wwEgZOdRLWzkWnhss5S1ZlWEp6FstPJqdE1sYnk5XjJKfOTWswpk8rYlvVJia6tnWIMKasUIXsTMhKc/4yUyErHbJSITQC+vwRZg2D8wbCxk9PJa5yzvZgQSK/i2/ztmiOpWV6JKfDrEtIJjtHCQsRujSryfi+rZyWU4taVLHkZMzZy8k5lTTyJo7Mwsrd4cxUj2nS8rzOM43nMjTHe2zL34a+f64QiQosWQWF/C6+dYazaFA9kiU7nA4R6/ceJTtHCQ8Vujarye8vbk1sqzp0b1HTHl5oyr/szCIkC8/yQhKEt/Ls9LMIWCA8CsIinb/wSAiLgrAIpzyyZv7lp00feWbZwS2w8P+gyzBYPgVaXlQhEpbt4YLAs/O3nHHxbWpmNg/PXQ9ApdAQujaryd39WtOrVR26N69FVKXQQIRqjHOYqkitDF8TRwFJJHcZeuZlFD4LCc8/EeS+jqxRQLLIk0TC3eGwqDzLyKc8tBKU9CNxdi6CH56Hwe87Ceqca04/h1WOBV2yEpGrgJeAUOAtVX06EHH4cljOl2lyJadmknAklYSkVBKOnCAhKZX43OGk1ALjmDGuF92b1yIyPADJqQL3PCq20t5m2VlFTBa+HKryUp6V/8XiPju5Q8+nNRFZHcIaFJw4vCWIgspDysmPu4SVpyemln2d4YSVlqxKk4iEAv8BrgDigWUi8pmqbiyRFbwSw/HDe9mS1Ygt2pzPc3rzp9DZtJdfSaYqC3K6khTZlPOrJ5F+KIWEjDGAc1ju47kzafPLCToN+hsA6+c8wcfrKpOQee7JaebOnUnlZYnsbD/WTUqpJ/8fS886LZSIsBCa1IqiSc0o7qk0jxVZLVmc0/Hk+N4hG+hTeQ8XtL62RN56sVTgnkcnqTrnD3Kynf+a4/zCP1mmp5fVaAZzRsBV/weNOsPun+F/j0PfB2Dzl8VofXhJIjlZ3t9DQULC82lBeLyuWr/kWhm55f5obVQk+f3gadm33CcqAFHVQMdwkoj0Bh5T1f7u8MMAqvqv/KaPiYnR5cuX+7z8b54ZzuUp8wDIJARBCMM5tJCFkE4E07MuZXjYdwD8I3M4a7U1nWU7D4fP5AUdTlT0+aRkZKPxy3kwdCZPZw5lnbbkPNnBxPBZPJM5mHXaiioRodSvFkH9qhHUrx5B/aqVqFetEvWqRlCvWiVqRIUjzptjzbKFtFz3Ei9k/o6NGk0n2cWE8I/Y3eluzuvRx9khoqf/P6Msp4DpcjzKyKfMyzIOboVV06B5b9i9GLoMhdot8+y08+zEz9i55y3LO1/eBOBlPtV8yjyTR37Lys4zn4/xU0rfjwLPWfhYXtTEERYJoUH1W9WUIhFZoaoxgY6jKIItWd0CXKWqY93h24BeqnpPftMXNVlFT/yCJ8KmcFvot/bjzm/EOeQiISDu/5BQ59f0GWUhp//lnU9CICQkn7LQAubLnS6fGE6LI79l5Z3P1/g9xnmWbZ4HW76EjjdD99sKTyJhEdbaMKWqLCarMvfTSkTGA+MBmjdvXuT5/5Y1hitCV9KII6eVb81pQruQBOZl9+Lz7AsAuD70Z64LXcK87F58ln0BtatE8PTvOgPw0EdrSUzJZEDoT1wfGsdn2bF8mn0htatE8OzArrnR5gZ9avjkPslznPt6zQxYOwc6D4Zut50a5/lfQvKUkU9Z7v+QQsrwsly37NclMO8+6DLciW/AqxB9UcE7e9vpOodL9yxxuhUvnwIxo6BF70BHZUyZFmzJKgFo5jHc1C07SVUnAZPAaVkVdQVPhE2hIUfwbFDmILSRBD7K7kO/kDVMz74cgN4hG3kp6yZuDf0fH3AlsdcMhXOcDhS9r+nKx3NnckHIhpPTfMQVXHjNUGhXjDs+7FwE2749tYPrdquTFAJp5yL44o8waJpzTLx9/wrT86jYPM/rtezrdCu2bWbMWQu2ZLUMaCsiLXGS1BBgWEktfHLtGVye8i0AmYQCQjhZhKBkEcIVISt5Kesm3gx/gfDQEP4c+hDzjrVhW1Q3JsmLRNTsCTiJ6Maa27k66lX+pA8WOI3PgnUHV4F7HhWbbTNj/CKozlkBiMg1wL9xuq6/rapPFTRtUc9Z+dob8JomqbSsUwVueOnUvHm7H5dkF2XrIm6MKUVl8ZxV0CWroihysjLGGFMmk1VIoAMwxhhjvLFkZYwxJuhZsjLGGBP0LFkZY4wJemW6g4WIHAR2+3k1dYFDJTBNSa4vEII1rmBm28z409nUrxaqWq8kg/G3Mp2sSoOILPfWa8aXaUpyfYEQrHEFM9tmxp8qWv2yw4DGGGOCniUrY4wxQc+SlXeTSmiaklxfIARrXMHMtpnxpwpVv+yclTHGmKBnLStjjDFBz5KVMcaYoGfJqgwQsScalgf2ORpTfJasiqgoOxwROavtKyLNRaQWQfbcMRG5VERibefrGxGpKSKV1U4QGz8rz99JS1ZeiEg/ERknIvcCqKoWViFE5CIRGe1Om1PchCUiNwCzgNnAcLcs4BVRRK4CpgE1cne+wRBXsBKR64H3ga9EZJiIVAt0TKZcqwIgIqGBDqSkWbIqhPsgyNeAcOB+EXkdnISVz7QiIpWBV4E/i8gf3GlzRKRILSMR6Qr8E7gDeBkYJiLVAvnL3H1/VYEHgHGqOl9EqohIBBARqLiCmYhcCfwLeAynXtwKdAhkTKb8EpHewA4R6ayq2eUtYVmyKoCINAf+Atyrqq8BXYHzRKR9fi0JdZzAaXVMArqKyJ/dcVlFXH0LYKOqrgMWAdWBl0XkLjeRlTo3UaYC+4D5IlITp+U3HXhCRC4PRFzByt1RXAg8p6rLVfUDnM9yoDveWqOmpDUDonBa8T3dhBUO5aO+WbIqWDrwhKp+KyKVgBM4O+vaBbWs3JdpQGvgXaCziLwiIs+7LZNCW1juegCWA7VFZDawCfgMmItTGa8WkdBAVD5VzQaygTeBfwMfAE/j3Ez4ehGpUR6+FCXB3VYvAZ+JSIi7XfYC9d3x6rZKjTkrHt+5b4EJwIPA1yLSFAiF/I8GlTVBdeI+GIiIuK2k/SISB6CqGe64HUCO+7oXsMw9zCceleFLoJaqLnRbQf8C3nfHF9jCEpFrgQtF5E1V3S0iY4FuQIiq/tOdJhmYCLzktuL8zj20UAkIV9X/4bQ2JwJdgDtUNV1EjgKXu7GW+S/F2RCRi4FaQKiqfpRn3BbgYvf1UCBcRKa7ic2YYvH4zoUBtwPX4vyo3ASEiEgL4GjufqysspaVBzdh/DH3JLiqJrnludupJhDl7mimA4Pc81iTRKSnO80JoLWI/B64B3gKqJfbQaMQFwHXAVeJSEtV3aWqHwMHReRWd5oaOBWyUkELKUki0h94C+gFPCIik1Q1AfgY5xfbK+6kHXB20BX6x497KHQSzqMb7haRKSJS12OSHCBHREYCfwOWWqIyxSUivUVktIhcKCJVVHU/zo/lCGABkAJkAlXLeqKCCr5z8SQi5wNzcM7JHHd/8abkmWwP8ChOsngOeAR4BufQzkwRuVZVN4vITuAh4B5VnSciFwE7vYTwC3AQiAWqi8gUnJbYemCAiAwDmgK35iZRfxKRKOA+4BFV/VREJgEJInJUVf8kIoOAx9xDla2B0ap60N9xBSP3MEwYMAZ4UlXfF5FZwFZAReQhVU3ESVZDgI7ATaq6OWBBmzJNnN7C/8A5ZXAdzn5pE5AMzAdq45wfbQ18LyIdgLSyfOTDktUpUcBNwAHgRZxDNO+q6nFVzXGn2Q+0AwYDdwEPqOo3ACLSBGgIbAb+C3yuqqvc+X7yWMZpPA4hfovzefyAkyT+hdPyfdxdXidgtar+WrJvu8B4MnDORR0Ep5UpIu8AN4sIqvon4DYRqQ1kqepRf8YVzNztlSkiWwHEuabquIjMwWmVPgHcDWwEvgcetURlisv9zv0e54frWveHZA9xHkY7H+gBzFPVH4AfRORTVU0NYMglosInK3Gug2kGvI5zrumwiDyE03FARGSqu+OJAD7Cue5pL87hntUiEuImosrAFcACVV3qLjsMyM4vUYlIM5zkF47TXFdghKq+KSKrgWdxOlUcd1tSO/y4GTzVB/a7PYnWAS+6rbzzcTqd9AbeEJFo91Dl4VKKqyzYCdwCdBSRhoAAVwIfu+cv1wKDVDUtgDGasi8T58d1NxHZg7PfaQBcAyQA/1TVHW6P1BzA70diSkOFTlbiXAfzJPBn99fxYQBVXSoif8FpZh8SkXpADE5Ci8JJaj+6ywjHqRDrgapu2SCcVtDWAtZ7LfB/wM9ATRF5TFU3isg8t2PFfTgtqi7AcBGZXBrHnEXkOuBREdmAU+mfBeKBNjiJ9R+qmiYiaTgtrwpNRK4GqqvqbABVnSoih3DOOmIxlgAAC5JJREFUbR4CXlPVVPewcKr7o8USlSkW99B8iKoeE5EXgb8C44Gpqvq4iPQDRuDUP4CcsnzYL68Km6xE5ALgPeB6NznVwPmQDwPpqvqziIwC4nDOHf3Tnf57oL6IHFbV0aqa6S4yEciW/2/v3IOtqqs4/vnyEsdHKJiSjzAUdKaxEgdTKEEwy3ESDB8I0oXxNSjWKFMaVs5gQWHp9KC0YkQxyFcqYI6FXoeHD3zwuICPZsBHaA5EIhqPi6s/1jpyPHC5F7z3ngNnfWb2nH33/u3fXvueffba67ceP2kQrgDP2cE5hfudJuLBFyuAi4E5kvriiuFP+Nv3g+FHW91Kiqo7noA8Co8k6o8PKQwzs4cltQ1rawSeB1bVgQGS+uApBZvDup4OYGazStqNwqM6S/2fSdJk4kVyJNBF0k/Cj1wLXI8PL2NmtZKuBD4HvLA3KSqoYmWFK5ctQFdJnYH78DyqDbiPaAquWPbBh3LG4XlXd0k6EE+8u8/MhkR/HXFfVx0waEc+icitWQ08hQdUvGNmN0vaivusTgIOMbN34wG4sMWufnvWAE/EDS9gHm49zZB0QYTTDwSuAWoi8qiaORy/LzYBd8f3Vfj8UFJH4AT8pWSkmb1ZTmGTPZcYiRkPjMFHOSZKmh/PiX8AwyUVXmi7A8+XSdQWpWpD183sZTwf4RZgMfBnPKrmUeCM8DkY0Nu8ksSLRceuN7M+wKHh3AR4GViGOz1XlJ5P0jFhKXXCQ9CHFd58zOwW3Ac2CXfUt4lztziS+kZo/Hqgh6TrLMAjHWfj1h+4gv2amS1qDdkqkQgXPg9X7q+a2QLgEmC8pIuL/JMfhu9yoJktLpe8yZ5NvPQMwF0V8/AqKOuB78fL42vAM7jr4HL8RbKxyOM9EzOr6gXPEbqqZFsh+q5H0bbhuNV0VNG2LrhF1gMPsOjawDnOxp3rT+I14r4JrAKuL2rTDa8MoVa67ja4j20ZrmgHAV2BpcB3itqdCfyu3N9TJSzxvS3Bh4PvKbk/zsSDYM7Egywm4P6Fssudy565AIfF54GFT1xZ3YIXt54DnBv79gMOKLfMLblU8zAgAGa2nBjzBZD0LeAQvBbg7ZIeNrMLzXNnegLzJfUxs9fNbI2keqCzeTDFdlUlwjc2CbjIzF4MS6w3cCrwdETszAD64iGnnYB1LXrReIFdPJ9sKu5/Oh9P7D0dWCCp3sx+iyuwHvJE6Q0Wv4xqI4aKr8S/xzpJU4AvyKuKvG9e2HcQboH/G7dAd5iukCSNEcE7oyJHrxAJbMBoM6uLNkSb2bZ9TuheR9UrqwLhpxkJjMUjam7C62ydKmm6mQ01sx/GDTJT0mTcsjoBeLuR7n9m23KuxuHRO6sjeucG3A90Mm7Ct7iiKqEeOAoP7LgU99MtBS6QV+XojQd8vNfKclUa9Xgk6HGSXgf64S81g4GVkn4EHIAH6AyMl6Ak2WXid/d73Nf5UcqKeRTg8qJcyLb4cHRVBDupSl+UtyOU1WnA2+ZVKD6Djw13xG+cLWY2NNoOxhOAewG3Ft50Gui3LbCfma2P9a7ATOAsM3tLXrfrX9Hm3Ra8xIbk6w6cZ2YTJV2LRyreZB4K2wEfgljT2nJVIpKG4NFXW4DZZjZe0ul4Pbbf4EO5Sy0TfpNPQPiQjzezcfEc+hI+enNnUZsa3NIfubPnz95E1QZYlGJObeFBY2arzatXrMEdlx0kTY/mrwCPmNkljd0oZrbVtlV3EJ6g959QVMPxkk3ty6Gogv8BPSVdClyBW5S9JV1hZptTUW3DzO7DC/bOJQJuzOxx3KI6yMzuTUWVNANv4vmXRwKz8LqhV8tLeCHp83hqyahqUVSQw4BNwszWSrocmCSvnN0WHwba1X7qcT/RG5Im4KHPNVbGUigxHPkGnmB4pZnNlNQf+Ge5ZKpkzGydpMfxIsabccu7G/4CkyS7haQetq2IwDq8qs4IfMaGX0abp+QFsm/Df6sbyiNteUhl1UQimGIJ8A3gDNuNvJkYamyPvym1BwaY2avNK+lu8QfgITMr5Gc8mcEBO+Up4Fjc/7gRH4pZVVaJkj2WSPi9R17Db6iZLZb0CPA94BFJncxLrv0V2FwIjiqnzOUgfVZNRNJBeLjytWa25BP2VYPPhbWsOWRrLooct0kTiAhJWRUX8U0+GZL2w2uOPoBHCHcws4ti36V4GsTf8Ejd86niav2prHYBSR2tGYqQplJIkqRAI8FcfXFFdTJwl3kxg6oklVWSJEmFEPl8t+PDfUMlnQCsNZ/0tKrJaMAkSZIKwXySzsuBjRHM9QAeRVz1pLJKkiSpICJdZAleQ3Tw7gRz7Y2kskqSJKkgIpjrLLxk19Jyy1MppM8qSZKkwmiuYK69iVRWSZIkScWTw4BJkiRJxZPKKkmSJKl4UlklSZIkFU8qq6TVkTRO0jJJSyQtknRybF8lqUsFyPeVkG+RpH1L9u1Q9nIjqVbS61F/srDtQUk7rSEnqZOk0Y20WdBccibJ7pKFbJNWRdIpwNnAiWa2KZRThzKLVcowYIKZTSveWA7ZJbU1s6ZOrvdfoA8wT1InfO60xugEjAYm7+Dc7cys3sxObbLASdJCpGWVtDZdgTVmtgk8AdLMVhftHyPpBUlLJR0HPnNqTI/woqQFknrG9hpJD4VV8aqkHxc6kTRc0rNh/dwWE19+DEkDos+lkqZI2kfSJXjB0PGS7m6q7JK+LumlkP1XkmbF9hsljS06Z52kbrH+oKTnw1K7rKjNBkm/kLQYOKUp1xLMAC6M9XPx6geFPveXNKfof3tO7JoIdI++J0nqJ2mupIeB5QV54nNw9CFJXSW9IumwBmRJkubFzHLJpdUWYH9gET7/02TgtKJ9q4AxsT4a+GOsHwi0i/WBwP2xXgO8BXTGp5yvA04CjsdnY24f7SYDI0rk6Ai8AfSIv+8EvhvrdwBDmip7UV/H4qVx7gFmxb4bgbFFfdQB3WL94PgsyN45/jbg/Fhv9Fpiey1e7HQJPt/aY/g8Wxtifzt81meALvh8ZYo2dUX99APeB44u2rahaH0acBU+KeDQct9PuVTPksOASatiZhsk9cLn9OoP/EXSdWZ2RzQpWAPP49YBeNmZqZKOxR/k7Yu6/Lt5PTUkPQD0BeqBXsDCcOHsC7xTIkpPYKVtm/BuKj5N+K27KjuuwFZazE0maRpwWUP9FHG1pMGxfiSu7NYCW/FpIwAGNOFaCmwF5uHW1b5mtqrYhQX8VNJXgQ+Bw4FDG+jnWTNb2cC+MbhifdrMpjfQJkmanVRWSatj7oOpBWolLQW+jVszAJvicyvb7s/xwBNmNjiG0GqLuyvtHn8wTzWz65tZ9IZkX7STQ+r5+HB7RwBJ/XAr8RQz+0BSbWEfsNG2+al29Vpm4JP03ViyfRhwCNDLzLZIWlV0vlLe30n/R+DK7lBJbSwn6UxaifRZJa2KpJ5hIRX4IvBaI4d9CihMkVBTsu8MSQdH1N4gYD4wBxgi6dNxzoMlfbbkuJeBbpKOib8vBp7cTdlfir66x/ahRW1WASfG8ScCRxdd07pQVMcBX27gtE25lmLmAhOAUqvnU8A7oaj6A4U+3gMO2El/HyGpHTAFv74VwDVNOS5JmoO0rJLWZn/g1xGtVo/7ThobMvs5Pgx4AzC7ZN+z+JDZEcA0M3sOINo+JqkNsAUf4vtIKZrZRkkjgXvjIbwQn/hul2WPvi4DZkv6AFcYBQVwPzBC0jLgGdzfBfAocIWkFbjifHpHJzSz5Y1dS0l7A27ewa67gZlhDT6HK1jMbK2k+ZLq8BlpS/+/xfwAmGtm8yL4Y6Gk2Wa2YifHJEmzkLUBkz0WSTXASWZ2VbllKSaG+Maa2dnlliVJ9hZyGDBJkiSpeNKySpIkSSqetKySJEmSiieVVZIkSVLxpLJKkiRJKp5UVkmSJEnFk8oqSZIkqXhSWSVJkiQVz/8BKNve2vDhx9gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDe6_vqqSe7f"
      },
      "source": [
        "6. **(15pts)** Consider MNIST classification problem covered during the class. For the details, please refer to the course material. In this example, we used the multilayer perceptron composed of an input layer with 512 hidden nodes and an output layer that produces predicted probabilities over 10 classes. In the class, we used GPU as a hardware accelerator to train our model.\n",
        "\n",
        "  Here, let's verify the actual benefit of using GPU for training. For this, compare the wall-clock times in the case of 1) using CPU and 2) using GPU to train MNIST classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVve5XSJ-WRm"
      },
      "source": [
        "### **CPU ver**\n",
        "<br/>\n",
        "<br/>\n",
        "## A first look at a neural network\n",
        "  \n",
        "\n",
        "* An example of a neural network to learn to classify handwritten digits\n",
        "* We will use the MNIST dataset.\n",
        "  * 28*28 grayscale images, 10 categories\n",
        "  * 60,000 training images, 10,000 test images\n",
        "  * Refer to http://yann.lecun.com/exdb/mnist/\n",
        "  * Solving MNIST is like the \"hello world\" of deep learning\n",
        "* Note\n",
        "  * In ML, a category in a classification problem is called a *class*.\n",
        "  * Data points are called *samples*.\n",
        "  * The class associated with a specific sample is called a *label*, *target* or *ground truth*.\n",
        "* First, let's look into the MNIST dataset.\n",
        "\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "print(device)\n",
        "print('16102284')\n",
        "```\n",
        "\n",
        "    cpu\n",
        "    16102284\n",
        "    \n",
        "\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.__version__\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    '2.6.0'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "!nvidia-smi\n",
        "```\n",
        "\n",
        "    NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images,test_labels) = mnist.load_data()\n",
        "\n",
        "# API reference https://keras.io/datasets/#mnist-database-of-handwritten-digits\n",
        "```\n",
        "\n",
        "* The images are encoded as Numpy arrays, and the labels are an array of digits (0-9).\n",
        "* The images and labels have a one-to-one correspondence.\n",
        "\n",
        "\n",
        "```python\n",
        "print(type(train_images))\n",
        "```\n",
        "\n",
        "    <class 'numpy.ndarray'>\n",
        "    \n",
        "\n",
        "\n",
        "```python\n",
        "print('Shape of train_images array: ', train_images.shape)\n",
        "print('# of training samples: ', len(train_images))\n",
        "\n",
        "print('Shape of test_images array: ', test_images.shape)\n",
        "print('# of test samples: ', len(test_images))\n",
        "```\n",
        "\n",
        "    Shape of train_images array:  (60000, 28, 28)\n",
        "    # of training samples:  60000\n",
        "    Shape of test_images array:  (10000, 28, 28)\n",
        "    # of test samples:  10000\n",
        "    \n",
        "\n",
        "\n",
        "```python\n",
        "train_images[10]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  42, 118,\n",
        "            219, 166, 118, 118,   6,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 103, 242, 254,\n",
        "            254, 254, 254, 254,  66,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 232, 254,\n",
        "            254, 254, 254, 254, 238,  70,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 104, 244,\n",
        "            254, 224, 254, 254, 254, 141,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 207,\n",
        "            254, 210, 254, 254, 254,  34,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  84,\n",
        "            206, 254, 254, 254, 254,  41,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "             24, 209, 254, 254, 254, 171,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  91,\n",
        "            137, 253, 254, 254, 254, 112,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  40, 214, 250,\n",
        "            254, 254, 254, 254, 254,  34,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81, 247, 254,\n",
        "            254, 254, 254, 254, 254, 146,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 110, 246,\n",
        "            254, 254, 254, 254, 254, 171,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  73,\n",
        "             89,  89,  93, 240, 254, 171,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   1, 128, 254, 219,  31,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   7, 254, 254, 214,  28,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0, 138, 254, 254, 116,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,  19, 177,  90,   0,   0,   0,   0,\n",
        "              0,  25, 240, 254, 254,  34,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0, 164, 254, 215,  63,  36,   0,  51,\n",
        "             89, 206, 254, 254, 139,   8,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,  57, 197, 254, 254, 222, 180, 241,\n",
        "            254, 254, 253, 213,  11,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0, 140, 105, 254, 254, 254, 254,\n",
        "            254, 254, 236,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   7, 117, 117, 165, 254,\n",
        "            254, 239,  50,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0]], dtype=uint8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "train_images[10].shape\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    (28, 28)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "train_labels\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "train_labels.shape\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    (60000,)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "plt.style.use('default') # set the same default style as jupyter notebook\n",
        "\n",
        "def visualize_mnist(image, label):\n",
        "  plt.imshow(image, cm.binary)\n",
        "  plt.title(label)\n",
        "  plt.show()\n",
        "\n",
        "visualize_mnist(train_images[1000], train_labels[1000])\n",
        "```\n",
        "\n",
        "\n",
        "    \n",
        "![png](output_16_0.png)\n",
        "    \n",
        "\n",
        "\n",
        "* We'll feed the neural network the training data, `train_images` and `train_labels`.\n",
        "* After training, we'll ask the network to produce predictions for `test_images`, and we'll verify whether these predictions match the labels from `test_labels`.\n",
        "* Let's build the network.\n",
        "\n",
        "\n",
        "```python\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))\n",
        "network.add(layers.Dense(10, activation='softmax'))\n",
        "```\n",
        "\n",
        "* As we learned, the core building block of neural networks is the *layers*, a data-processing module.\n",
        "* Specifically, layers extract *representations* out of the data fed into them.\n",
        "* We can obtain more useful representations as training goes.\n",
        "\n",
        "\n",
        "* Here, our network consists of a sequence of two `Dense` layers, which are densely connected (also called `fully connected`) layers.\n",
        "* The last layer is a 10-way `softmax` layer, which means it will return an array of 10 probability scores (summing to 1).\n",
        "\n",
        "\n",
        "* To make the network ready for training, we need three more things,\n",
        "  * A loss function\n",
        "  * An optimizer\n",
        "  * Metrics to monitor during training and testing\n",
        "\n",
        "\n",
        "```python\n",
        "network.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "* Before training, we'll preprocess the data: **reshaping, scaling**.\n",
        "  * `(60000, 28, 28) uint8` in the `[0, 255]` --> `(60000, 28*28) float32` in the `[0,1]`\n",
        "\n",
        "\n",
        "```python\n",
        "train_images = train_images.reshape((60000, 28*28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28*28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "```\n",
        "\n",
        "* We also need to categorically encode the labels, which is called *one-hot vector*.\n",
        "\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "```\n",
        "\n",
        "\n",
        "```python\n",
        "train_labels.shape\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    (60000, 10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "train_labels[10]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)\n",
        "\n",
        "\n",
        "\n",
        "* Now, we are ready to train the network by calling *fit* method of the network.\n",
        "\n",
        "\n",
        "```python\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print('Elapsed time: %.4f' % elapsed)\n",
        "```\n",
        "\n",
        "    Epoch 1/5\n",
        "    469/469 [==============================] - 5s 10ms/step - loss: 0.2556 - accuracy: 0.9255\n",
        "    Epoch 2/5\n",
        "    469/469 [==============================] - 5s 10ms/step - loss: 0.1032 - accuracy: 0.9696\n",
        "    Epoch 3/5\n",
        "    469/469 [==============================] - 5s 10ms/step - loss: 0.0680 - accuracy: 0.9794\n",
        "    Epoch 4/5\n",
        "    469/469 [==============================] - 5s 10ms/step - loss: 0.0487 - accuracy: 0.9851\n",
        "    Epoch 5/5\n",
        "    469/469 [==============================] - 5s 10ms/step - loss: 0.0370 - accuracy: 0.9892\n",
        "    Elapsed time: 23.7541\n",
        "    \n",
        "\n",
        "* Two quantities are displayed during training, *loss* and *accuracy*.\n",
        "* Note that these quantities do not guarantee the *test* performance.\n",
        "* Let's check that the model performs well on the test set, too.\n",
        "\n",
        "\n",
        "```python\n",
        "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
        "print('test_acc: ', test_acc)\n",
        "```\n",
        "\n",
        "    313/313 [==============================] - 1s 2ms/step - loss: 0.0689 - accuracy: 0.9799\n",
        "    test_acc:  0.9799000024795532\n",
        "    \n",
        "\n",
        "* The test accuracy is a bit lower than the training accuracy.\n",
        "* This gap between training accuracy and test accuracy is an example of *overfitting*.\n",
        "\n",
        "\n",
        "* We saw how we can build and train a neural network to classify handwritten digits in less than 20 lines of Python code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbqPOFq0B0cz"
      },
      "source": [
        "### **GPU ver**\n",
        "<br/>\n",
        "<br/>\n",
        "## A first look at a neural network\n",
        "  \n",
        "\n",
        "* An example of a neural network to learn to classify handwritten digits\n",
        "* We will use the MNIST dataset.\n",
        "  * 28*28 grayscale images, 10 categories\n",
        "  * 60,000 training images, 10,000 test images\n",
        "  * Refer to http://yann.lecun.com/exdb/mnist/\n",
        "  * Solving MNIST is like the \"hello world\" of deep learning\n",
        "* Note\n",
        "  * In ML, a category in a classification problem is called a *class*.\n",
        "  * Data points are called *samples*.\n",
        "  * The class associated with a specific sample is called a *label*, *target* or *ground truth*.\n",
        "* First, let's look into the MNIST dataset.\n",
        "\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "print(device)\n",
        "print('16102284')\n",
        "```\n",
        "\n",
        "    cuda\n",
        "    16102284\n",
        "    \n",
        "\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.__version__\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    '2.6.0'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "!nvidia-smi\n",
        "```\n",
        "\n",
        "    Tue Sep 28 06:12:05 2021       \n",
        "    +-----------------------------------------------------------------------------+\n",
        "    | NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
        "    |-------------------------------+----------------------+----------------------+\n",
        "    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
        "    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
        "    |                               |                      |               MIG M. |\n",
        "    |===============================+======================+======================|\n",
        "    |   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
        "    | N/A   71C    P8    34W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
        "    |                               |                      |                  N/A |\n",
        "    +-------------------------------+----------------------+----------------------+\n",
        "                                                                                   \n",
        "    +-----------------------------------------------------------------------------+\n",
        "    | Processes:                                                                  |\n",
        "    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
        "    |        ID   ID                                                   Usage      |\n",
        "    |=============================================================================|\n",
        "    |  No running processes found                                                 |\n",
        "    +-----------------------------------------------------------------------------+\n",
        "    \n",
        "\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images,test_labels) = mnist.load_data()\n",
        "\n",
        "# API reference https://keras.io/datasets/#mnist-database-of-handwritten-digits\n",
        "```\n",
        "\n",
        "    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
        "    11493376/11490434 [==============================] - 0s 0us/step\n",
        "    11501568/11490434 [==============================] - 0s 0us/step\n",
        "    \n",
        "\n",
        "* The images are encoded as Numpy arrays, and the labels are an array of digits (0-9).\n",
        "* The images and labels have a one-to-one correspondence.\n",
        "\n",
        "\n",
        "```python\n",
        "print(type(train_images))\n",
        "```\n",
        "\n",
        "    <class 'numpy.ndarray'>\n",
        "    \n",
        "\n",
        "\n",
        "```python\n",
        "print('Shape of train_images array: ', train_images.shape)\n",
        "print('# of training samples: ', len(train_images))\n",
        "\n",
        "print('Shape of test_images array: ', test_images.shape)\n",
        "print('# of test samples: ', len(test_images))\n",
        "```\n",
        "\n",
        "    Shape of train_images array:  (60000, 28, 28)\n",
        "    # of training samples:  60000\n",
        "    Shape of test_images array:  (10000, 28, 28)\n",
        "    # of test samples:  10000\n",
        "    \n",
        "\n",
        "\n",
        "```python\n",
        "train_images[10]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  42, 118,\n",
        "            219, 166, 118, 118,   6,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 103, 242, 254,\n",
        "            254, 254, 254, 254,  66,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 232, 254,\n",
        "            254, 254, 254, 254, 238,  70,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 104, 244,\n",
        "            254, 224, 254, 254, 254, 141,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 207,\n",
        "            254, 210, 254, 254, 254,  34,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  84,\n",
        "            206, 254, 254, 254, 254,  41,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "             24, 209, 254, 254, 254, 171,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  91,\n",
        "            137, 253, 254, 254, 254, 112,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  40, 214, 250,\n",
        "            254, 254, 254, 254, 254,  34,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81, 247, 254,\n",
        "            254, 254, 254, 254, 254, 146,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 110, 246,\n",
        "            254, 254, 254, 254, 254, 171,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  73,\n",
        "             89,  89,  93, 240, 254, 171,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   1, 128, 254, 219,  31,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   7, 254, 254, 214,  28,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0, 138, 254, 254, 116,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,  19, 177,  90,   0,   0,   0,   0,\n",
        "              0,  25, 240, 254, 254,  34,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0, 164, 254, 215,  63,  36,   0,  51,\n",
        "             89, 206, 254, 254, 139,   8,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,  57, 197, 254, 254, 222, 180, 241,\n",
        "            254, 254, 253, 213,  11,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0, 140, 105, 254, 254, 254, 254,\n",
        "            254, 254, 236,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   7, 117, 117, 165, 254,\n",
        "            254, 239,  50,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0],\n",
        "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "              0,   0]], dtype=uint8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "train_images[10].shape\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    (28, 28)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "train_labels\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "train_labels.shape\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    (60000,)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "plt.style.use('default') # set the same default style as jupyter notebook\n",
        "\n",
        "def visualize_mnist(image, label):\n",
        "  plt.imshow(image, cm.binary)\n",
        "  plt.title(label)\n",
        "  plt.show()\n",
        "\n",
        "visualize_mnist(train_images[1000], train_labels[1000])\n",
        "```\n",
        "\n",
        "\n",
        "    \n",
        "![png](output_16_0.png)\n",
        "    \n",
        "\n",
        "\n",
        "* We'll feed the neural network the training data, `train_images` and `train_labels`.\n",
        "* After training, we'll ask the network to produce predictions for `test_images`, and we'll verify whether these predictions match the labels from `test_labels`.\n",
        "* Let's build the network.\n",
        "\n",
        "\n",
        "```python\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))\n",
        "network.add(layers.Dense(10, activation='softmax'))\n",
        "```\n",
        "\n",
        "* As we learned, the core building block of neural networks is the *layers*, a data-processing module.\n",
        "* Specifically, layers extract *representations* out of the data fed into them.\n",
        "* We can obtain more useful representations as training goes.\n",
        "\n",
        "\n",
        "* Here, our network consists of a sequence of two `Dense` layers, which are densely connected (also called `fully connected`) layers.\n",
        "* The last layer is a 10-way `softmax` layer, which means it will return an array of 10 probability scores (summing to 1).\n",
        "\n",
        "\n",
        "* To make the network ready for training, we need three more things,\n",
        "  * A loss function\n",
        "  * An optimizer\n",
        "  * Metrics to monitor during training and testing\n",
        "\n",
        "\n",
        "```python\n",
        "network.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "* Before training, we'll preprocess the data: **reshaping, scaling**.\n",
        "  * `(60000, 28, 28) uint8` in the `[0, 255]` --> `(60000, 28*28) float32` in the `[0,1]`\n",
        "\n",
        "\n",
        "```python\n",
        "train_images = train_images.reshape((60000, 28*28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28*28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "```\n",
        "\n",
        "* We also need to categorically encode the labels, which is called *one-hot vector*.\n",
        "\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "```\n",
        "\n",
        "\n",
        "```python\n",
        "train_labels.shape\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    (60000, 10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "train_labels[10]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)\n",
        "\n",
        "\n",
        "\n",
        "* Now, we are ready to train the network by calling *fit* method of the network.\n",
        "\n",
        "\n",
        "```python\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print('Elapsed time: %.4f' % elapsed)\n",
        "```\n",
        "\n",
        "    Epoch 1/5\n",
        "    469/469 [==============================] - 5s 4ms/step - loss: 0.2572 - accuracy: 0.9260\n",
        "    Epoch 2/5\n",
        "    469/469 [==============================] - 2s 4ms/step - loss: 0.1044 - accuracy: 0.9694\n",
        "    Epoch 3/5\n",
        "    469/469 [==============================] - 2s 4ms/step - loss: 0.0674 - accuracy: 0.9799\n",
        "    Epoch 4/5\n",
        "    469/469 [==============================] - 2s 4ms/step - loss: 0.0499 - accuracy: 0.9851\n",
        "    Epoch 5/5\n",
        "    469/469 [==============================] - 2s 4ms/step - loss: 0.0371 - accuracy: 0.9888\n",
        "    Elapsed time: 12.8957\n",
        "    \n",
        "\n",
        "* Two quantities are displayed during training, *loss* and *accuracy*.\n",
        "* Note that these quantities do not guarantee the *test* performance.\n",
        "* Let's check that the model performs well on the test set, too.\n",
        "\n",
        "\n",
        "```python\n",
        "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
        "print('test_acc: ', test_acc)\n",
        "```\n",
        "\n",
        "    313/313 [==============================] - 1s 3ms/step - loss: 0.0645 - accuracy: 0.9812\n",
        "    test_acc:  0.9811999797821045\n",
        "    \n",
        "\n",
        "* The test accuracy is a bit lower than the training accuracy.\n",
        "* This gap between training accuracy and test accuracy is an example of *overfitting*.\n",
        "\n",
        "\n",
        "* We saw how we can build and train a neural network to classify handwritten digits in less than 20 lines of Python code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQnKtvaJCo-d"
      },
      "source": [
        "As a result of the experiment, the time spent learning through the **CPU was 23.7541 sec**, and the time taken using the **GPU was 12.8957 sec**. Nearly half of the time has been **shortened through GPUs**, and if you learn a more complex deep learning model, these performance differences will be even more significant."
      ]
    }
  ]
}